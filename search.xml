<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[NLP-Transformer-task03]]></title>
      <url>/2021/08/20/NLP-Transformer-task03/</url>
      <content type="html"><![CDATA[<p>author：TongMing<br>Datawhale 开源学习地址：<a href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a></p>
<h2 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h2><p>预训练：根据MASK的方式进行自监督的学习，<br>微调：结合监督信息进行半监督的学习</p>
<p>BERT是transformer的encoder部分<br>ELMo可以表示不同语境下的“多义词”</p>
<h2 id="图解BERT（精华）"><a href="#图解BERT（精华）" class="headerlink" title="图解BERT（精华）"></a>图解BERT（精华）</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>BERT 开发的两个步骤：第 1 步，你可以下载预训练好的模型（这个模型是在无标注的数据上训练的）。然后在第 2 步只需要关心模型微调即可。</p>
<h3 id="句子分类"><a href="#句子分类" class="headerlink" title="句子分类"></a>句子分类</h3><p>使用 BERT 最直接的方法就是对一个句子进行分类。这个模型如下所示：</p>
<p><img src="3-bert-cls.png" alt="BERT句子分类">图：BERT句子分类</p>
<p>为了训练这样一个模型，你主要需要训练分类器（上图中的 Classifier），在训练过程中 几乎不用改动BERT模型。这个训练过程称为微调，它起源于Semi-supervised Sequence Learning 和 ULMFiT。</p>
<p>其他例子还包括：预测语义分析和断言</p>
<h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>论文里介绍了两种不同模型大小的 BERT：</p>
<ul>
<li>BERT BASE - 与 OpenAI 的 Transformer 大小相当，以便比较性能</li>
<li>BERT LARGE - 一个非常巨大的模型，它取得了最先进的结果</li>
</ul>
<p>2 种不同大小规模的 BERT 模型都有大量的 Encoder 层（论文里把这些层称为 Transformer Blocks）- BASE 版本由 12 层 Encoder，Large 版本有 20 层 Encoder。同时，这些 BERT 模型也有更大的前馈神经网络（分别有 768 个和 1024 个隐藏层单元）和更多的 attention heads（分别有 12 个和 16 个），超过了原始 Transformer 论文中的默认配置参数（原论文中有 6 个 Encoder 层， 512 个隐藏层单元和 8 个 attention heads）。</p>
<h3 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h3><p><img src="3-bert-input.png" alt="模型输入">图：模型输入</p>
<p>第一个输入的 token 是特殊的 [CLS]，它 的含义是分类（class的缩写）。</p>
<p>就像 Transformer 中普通的 Encoder 一样，BERT 将一串单词作为输入，这些单词在 Encoder 的栈中不断向上流动。每一层都会经过 Self Attention 层，并通过一个前馈神经网络，然后将结果传给下一个 Encoder。</p>
<p><img src="3-bert-encoder.webp" alt="BERT encoder">图：BERT encoder</p>
<p>在模型架构方面，到目前为止，和 Transformer 是相同的（除了模型大小，因为这是我们可以改变的参数）。我们会在下面看到，BERT 和 Transformer 在模型的输出上有一些不同。</p>
<h3 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h3><p>每个位置输出一个大小为 hidden_size（在 BERT Base 中是 768）的向量。对于上面提到的句子分类的例子，我们只关注第一个位置的输出（输入是 [CLS] 的那个位置）。</p>
<p><img src="3-bert-output.png" alt="BERT output">图：BERT output</p>
<p>这个输出的向量现在可以作为后面分类器的输入。论文里用单层神经网络作为分类器，取得了很好的效果。</p>
<p><img src="3-bert-clss.webp" alt="BERT 接分类器">图：BERT 接分类器</p>
<p>如果你有更多标签（例如你是一个电子邮件服务，需要将邮件标记为 “垃圾邮件”、“非垃圾邮件”、“社交”、“推广”），你只需要调整分类器的神经网络，增加输出的神经元个数，然后经过 softmax 即可。</p>
<h3 id="与卷积神经网络进行对比"><a href="#与卷积神经网络进行对比" class="headerlink" title="与卷积神经网络进行对比"></a>与卷积神经网络进行对比</h3><p>对于那些有计算机视觉背景的人来说，这个向量传递过程，会让人联想到 VGGNet 等网络的卷积部分，和网络最后的全连接分类部分之间的过程。</p>
<p><img src="3-cnn.png" alt="CNN">图：CNN</p>
<h3 id="词嵌入（Embedding）的新时代"><a href="#词嵌入（Embedding）的新时代" class="headerlink" title="词嵌入（Embedding）的新时代"></a>词嵌入（Embedding）的新时代</h3><p>上面提到的这些新发展带来了文本编码方式的新转变。到目前为止，词嵌入一直是 NLP 模型处理语言的主要表示方法。像 Word2Vec 和 Glove 这样的方法已经被广泛应用于此类任务。在我们讨论新的方法之前，让我们回顾一下它们是如何应用的。</p>
<h4 id="回顾词嵌入"><a href="#回顾词嵌入" class="headerlink" title="回顾词嵌入"></a>回顾词嵌入</h4><p>单词不能直接输入机器学习模型，而需要某种数值表示形式，以便模型能够在计算中使用。通过 Word2Vec，我们可以使用一个向量（一组数字）来恰当地表示单词，并捕捉单词的语义以及单词和单词之间的关系（例如，判断单词是否相似或者相反，或者像 “Stockholm” 和 “Sweden” 这样的一对词，与 “Cairo” 和 “Egypt”这一对词，是否有同样的关系）以及句法、语法关系（例如，”had” 和 “has” 之间的关系与 “was” 和 “is” 之间的关系相同）。</p>
<p>人们很快意识到，相比于在小规模数据集上和模型一起训练词嵌入，更好的一种做法是，在大规模文本数据上预训练好词嵌入，然后拿来使用。因此，我们可以下载由 Word2Vec 和 GloVe 预训练好的单词列表，及其词嵌入。下面是单词 “stick” 的 Glove 词嵌入向量的例子（词嵌入向量长度是 200）。</p>
<p><img src="3-wordvector.webp" alt="wrod vector">图： wrod vector</p>
<p>单词 “stick” 的 Glove 词嵌入 - 一个由200个浮点数组成的向量（四舍五入到小数点后两位）。</p>
<h4 id="语境问题"><a href="#语境问题" class="headerlink" title="语境问题"></a>语境问题</h4><p>如果我们使用 Glove 的词嵌入表示方法，那么不管上下文是什么，单词 “stick” 都只表示为同一个向量。一些研究人员指出，像 “stick” 这样的词有多种含义。为什么不能根据它使用的上下文来学习对应的词嵌入呢？这样既能捕捉单词的语义信息，又能捕捉上下文的语义信息。于是，语境化的词嵌入模型应运而生。<br><img src="3-elmo.webp" alt="ELMO">图：ELMO<br>语境化的词嵌入，可以根据单词在句子语境中的含义，赋予不同的词嵌入。你可以查看这个视频 RIP Robin Williams（<a href="https://zhuanlan.zhihu.com/RIP">https://zhuanlan.zhihu.com/RIP</a> Robin Williams）</p>
<p>ELMo 没有对每个单词使用固定的词嵌入，而是在为每个词分配词嵌入之前，查看整个句子，融合上下文信息。它使用在特定任务上经过训练的双向 LSTM 来创建这些词嵌入。</p>
<p>ELMo 在语境化的预训练这条道路上迈出了重要的一步。ELMo LSTM 会在一个大规模的数据集上进行训练，然后我们可以将它作为其他语言处理模型的一个部分，来处理自然语言任务。</p>
<p>那么 ELMo 的秘密是什么呢？</p>
<p>ELMo 通过训练，预测单词序列中的下一个词，从而获得了语言理解能力，这项任务被称为语言建模。要实现 ELMo 很方便，因为我们有大量文本数据，模型可以从这些数据中学习，而不需要额外的标签。</p>
<p><img src="3-elmo-pre.webp" alt="ELMO 训练">图： ELMO 训练</p>
<p>ELMo 预训练过程的其中一个步骤：以 “Let’s stick to” 作为输入，预测下一个最有可能的单词。这是一个语言建模任务。当我们在大规模数据集上训练时，模型开始学习语言的模式。例如，在 “hang” 这样的词之后，模型将会赋予 “out” 更高的概率（因为 “hang out” 是一个词组），而不是 “camera”。</p>
<p>在上图中，我们可以看到 ELMo 头部上方展示了 LSTM 的每一步的隐藏层状态向量。在这个预训练过程完成后，这些隐藏层状态在词嵌入过程中派上用场。</p>
<p><img src="3-elmo-pre1.png" alt="ELMO 训练 stick">图：ELMO 训练</p>
<p>ELMo 通过将隐藏层状态（以及初始化的词嵌入）以某种方式（向量拼接之后加权求和）结合在一起，实现了带有语境化的词嵌入。</p>
<p><img src="3-elmo-pre2.webp" alt="ELMO 训练 stick">图：ELMO 训练</p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP-transformer]]></title>
      <url>/2021/08/18/NLP-transformer/</url>
      <content type="html"><![CDATA[<ul>
<li>DataWhale开源学习资料:<a href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a><h2 id="图解Attention"><a href="#图解Attention" class="headerlink" title="图解Attention"></a>图解Attention</h2></li>
</ul>
<p>Seq2seq：有2篇开创性的论文：<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sutskever等2014年发表的Sequence to Sequence Learning<br>with Neural Networks</a>和<a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf">Cho等2014年发表的Learning Phrase Representations using RNN Encoder–Decoder<br>for Statistical Machine Translation</a>都对这些模型进行了解释。</p>
<p>编码器和解码器在Transformer出现之前一般采用的是循环神经网络。关于循环神经网络，建议阅读 <a href="https://www.youtube.com/watch?v=UNmqTiOnRfg">Luis Serrano写的一篇关于循环神经网络</a>的精彩介绍.</p>
<p>如果你觉得你准备好了学习注意力机制的代码实现，一定要看看基于 TensorFlow 的 神经机器翻译 (seq2seq) <a href="https://github.com/tensorflow/nmt">指南</a>。</p>
<h2 id="图解transformer"><a href="#图解transformer" class="headerlink" title="图解transformer"></a>图解transformer</h2><p>本文翻译自<a href="http://jalammar.github.io/illustrated-transformer">illustrated-transformer</a></p>
<p>添加了一些简单的代码，实现了一个基本的 Self Attention 以及 multi-head attention 的矩阵运算。</p>
<p>2017 年，Google 提出了 Transformer 模型，用 <strong>Self Attention</strong> 的结构，取代了以往 NLP 任务中的 <strong>RNN</strong> 网络结构</p>
<p>用Self Attention机制效果又好，而且还可以并行计算。<br>（这个模型的其中一个优点，就是使得模型训练过程能够并行计算。在 RNN 中，每一个 time step 的计算都依赖于上一个 time step 的输出，这就使得所有的 time step 必须串行化，无法并行计算。）</p>
<p>编码部分是多层的编码器(Encoder)组成（Transformer 的论文中使用了 6 层编码器，这里的层数 6 并不是固定的，你也可以根据实验效果来修改层数）。同理，解码部分也是由多层的解码器(Decoder)组成（论文里也使用了 6 层的解码器）。每层编码器在结构上都是一样的，但不同层编码器的权重参数是不同的。</p>
<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>每层编码器里面，主要由以下两部分组成：</p>
<ul>
<li>Self-Attention Layer</li>
<li>Feed Forward Neural Network（前馈神经网络，缩写为 FFNN）<br><img src="2-encoder.png" alt="encoder"></li>
</ul>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>同理，解码器也具有这两层，<strong>但是这两层中间还插入了一个 Encoder-Decoder Attention 层</strong>，这个层能帮助解码器聚焦于输入句子的相关部分（类似于 seq2seq 模型 中的 Attention）。<br><img src="2-decoder.webp" alt="decoder"></p>
<h3 id="Transformer细节"><a href="#Transformer细节" class="headerlink" title="Transformer细节"></a>Transformer细节</h3><h4 id="Transformer-的输入"><a href="#Transformer-的输入" class="headerlink" title="Transformer 的输入"></a>Transformer 的输入</h4><p>和通常的 NLP 任务一样，我们<strong>首先会使用词嵌入算法（embedding algorithm）</strong>，将每个词转换为一个词向量。实际中向量一般是 256 或者 512 维。</p>
<p>那么整个输入的句子是一个向量列表，其中有 3 个词向量。在实际中，每个句子的长度不一样，我们会取一个适当的值，作为向量列表的长度。<strong>如果一个句子达不到这个长度</strong>，<strong>那么就填充全为 0 的词向量</strong>；<strong>如果句子超出这个长度，则做截断</strong>。句子长度是一个超参数，通常是训练集中的句子的最大长度，你可以尝试不同长度的效果。</p>
<p>编码器（Encoder）接收的输入都是一个向量列表，输出也是大小同样的向量列表。</p>
<p><img src="2-x-encoder.png" alt="输入encoder"><br>图：输入encoder</p>
<p><img src="2-multi-encoder.webp" alt="一层传一层"><br>图：一层传一层</p>
<p>每一个人都经历过类似的无知的狼狈，其实每一个人都经历了自我怀疑之后才能如释重负。<br><strong>别被“Self-Attention”这么高大上的词给唬住了，乍一听好像每个人都应该对这个词熟悉一样</strong>。</p>
<p>当模型处理句子中的每个词时，Self Attention机制使得模型不仅能够关注这个位置的词，而且能够关注句子中其他位置的词，作为辅助线索，进而可以更好地编码当前位置的词。如果你熟悉 RNN，回忆一下：RNN 在处理一个词时，会考虑前面传过来的hidden state，而hidden state就包含了前面的词的信息。而 Transformer 使用Self Attention机制，会把其他单词的理解融入处理当前的单词。</p>
<h3 id="Self-Attention的细节"><a href="#Self-Attention的细节" class="headerlink" title="Self-Attention的细节"></a>Self-Attention的细节</h3><p>计算Query 向量，Key 向量，Value 向量</p>
<p>下面我们先看下如何使用向量来计算 Self Attention，然后再看下如何使用矩阵来实现 Self Attention。（矩阵运算的方式，使得 Self Attention 的计算能够并行化，这也是 Self Attention 最终的实现方式）。</p>
<p>计算 Self Attention 的第 1 步是：对输入编码器的每个词向量，都创建 3 个向量，分别是：Query 向量，Key 向量，Value 向量。这 3 个向量是词向量分别和 3 个矩阵相乘得到的，而这个矩阵是我们要学习的参数。</p>
<p>注意，这 3 个新得到的向量一般比原来的词向量的长度更小。假设这 3 个向量的长度是$d_{key}$，而原始的词向量或者最终输出的向量的长度是 512（<strong>这 3 个向量的长度，和最终输出的向量长度，是有倍数关系的</strong>）。关于 Multi-head Attention，后面会给出实际代码。这里为了简化，假设只有一个 head 的 Self-Attention。</p>
<p><img src="2-qkv.png" alt="Q,K,V">图：Q,K,V</p>
<p>这里可以锻炼一下github的提交，上下角标？？？？？？</p>
<p>上图中，有两个词向量：Thinking 的词向量 x1 和 Machines 的词向量 x2。以 x1 为例，X1 乘以 WQ 得到 q1，q1 就是 X1 对应的 Query 向量。同理，X1 乘以 WK 得到 k1，k1 是 X1 对应的 Key 向量；X1 乘以 WV 得到 v1，v1 是 X1 对应的 Value 向量。</p>
<p><strong>Query 向量，Key 向量，Value 向量是什么含义呢</strong>？</p>
<h4 id="计算-Attention-Score（注意力分数）"><a href="#计算-Attention-Score（注意力分数）" class="headerlink" title="计算 Attention Score（注意力分数）"></a>计算 Attention Score（注意力分数）</h4><p>第 2 步，是计算 Attention Score（注意力分数）。假设我们现在计算第一个词 Thinking 的 Attention Score（注意力分数），<strong>需要根据 Thinking 这个词，对句子中的其他每个词都计算一个分数</strong>。这些分数决定了我们在编码Thinking这个词时，需要对句子中其他位置的每个词放置多少的注意力。</p>
<p>这些分数，是通过计算 “Thinking” 对应的 Query 向量和其他位置的每个词的 Key 向量的点积，而得到的。如果我们计算句子中第一个位置单词的 Attention Score（注意力分数），那么第一个分数就是 q1 和 k1 的内积，第二个分数就是 q1 和 k2 的点积。</p>
<p><img src="2-think.png" alt="Thinking计算"><br>图：Thinking计算</p>
<p>第 3 步就是把每个分数除以 $\sqrt(d_{key})$ （$d_{key}$是 Key 向量的长度）。你也可以除以其他数，除以一个数是为了在反向传播时，求取梯度更加稳定。</p>
<p>第 4 步，接着把这些分数经过一个 Softmax 层，Softmax可以将分数归一化，这样使得分数都是正数并且加起来等于 1。</p>
<p><img src="2-think2.png" alt="Thinking计算"><br>图：Thinking计算</p>
<p>这些分数决定了在编码当前位置（这里的例子是第一个位置）的词时，对所有位置的词分别有多少的注意力。很明显，在上图的例子中，当前位置（这里的例子是第一个位置）的词会有最高的分数，但有时，关注到其他位置上相关的词也很有用。</p>
<p>第 5 步，得到每个位置的分数后，将每个分数分别与每个 Value 向量相乘。这种做法背后的直觉理解就是：对于分数高的位置，相乘后的值就越大，我们把更多的注意力放到了它们身上；对于分数低的位置，相乘后的值就越小，这些位置的词可能是相关性不大的，这样我们就忽略了这些位置的词。</p>
<p>第 6 步是把上一步得到的向量相加，就得到了 Self Attention 层在这个位置（这里的例子是第一个位置）的输出。</p>
<p><img src="2-sum.png" alt="Think计算"><br>图：Think计算</p>
<p>上面这张图，包含了 Self Attention 的全过程，最终得到的当前位置（这里的例子是第一个位置）的向量会输入到前馈神经网络。但这样每次只能计算一个位置的输出向量，在实际的代码实现中，Self Attention 的计算过程是使用矩阵来实现的，这样可以加速计算，一次就得到所有位置的输出向量。下面让我们来看，如何使用矩阵来计算所有位置的输出向量。</p>
<h4 id="使用矩阵计算Self-Attention"><a href="#使用矩阵计算Self-Attention" class="headerlink" title="使用矩阵计算Self-Attention"></a>使用矩阵计算Self-Attention</h4><p>第一步是计算 Query，Key，Value 的矩阵。首先，我们把所有词向量放到一个矩阵 X 中，然后分别和3 个权重矩阵$W^Q, W^K W^V$ 相乘，得到 Q，K，V 矩阵。</p>
<p><img src="2-qkv-multi.png">图：QKV矩阵乘法</p>
<p>?????? WQ\WK矩阵都是怎么来的</p>
<p>矩阵 X 中的每一行，表示句子中的每一个词的词向量，长度是 512。Q，K，V 矩阵中的每一行表示 Query 向量，Key 向量，Value 向量，向量长度是 64。</p>
<p>接着，由于我们使用了矩阵来计算，我们可以把上面的第 2 步到第 6 步压缩为一步，直接得到 Self Attention 的输出。</p>
<p><img src="2-attention-output.webp" alt="输出"><br>图：输出</p>
<h4 id="多头注意力机制（multi-head-attention）"><a href="#多头注意力机制（multi-head-attention）" class="headerlink" title="多头注意力机制（multi-head attention）"></a>多头注意力机制（multi-head attention）</h4><p>Transformer 的论文通过增加多头注意力机制（一组注意力称为一个 attention head），进一步完善了 Self Attention 层。这种机制从如下两个方面增强了 attention 层的能力：</p>
<ul>
<li>它扩展了模型关注不同位置的能力。</li>
<li>多头注意力机制赋予 attention 层多个“子表示空间”。</li>
</ul>
<p>接下来就有点麻烦了，因为前馈神经网络层接收的是 1 个矩阵（其中每行的向量表示一个词），而不是 8 个矩阵。所以我们需要一种方法，把 8 个矩阵整合为一个矩阵。</p>
<p>怎么才能做到呢？我们把矩阵拼接起来，然后和另一个权重矩阵$W^Q$相乘。</p>
<p><img src="2-to1.webp" alt="整合矩阵"><br>图：整合矩阵</p>
<ol>
<li>把 8 个矩阵 {Z0,Z1…,Z7} 拼接起来</li>
<li>把拼接后的矩阵和 WO 权重矩阵相乘</li>
<li>得到最终的矩阵 Z，这个矩阵包含了所有 attention heads（注意力头） 的信息。这个矩阵会输入到 FFNN (Feed Forward Neural Network)层。</li>
</ol>
<p>这就是多头注意力的全部内容。我知道，在上面的讲解中，出现了相当多的矩阵。下面我把所有的内容都放到一张图中，这样你可以总揽全局，在这张图中看到所有的内容。</p>
<p><img src="2-put-together.webp" alt="放在一起"><br>图：放在一起</p>
<p>既然我们已经谈到了多头注意力，现在让我们重新回顾之前的翻译例子，看下当我们编码单词it时，不同的 attention heads （注意力头）关注的是什么部分。</p>
<p><img src="2-it-attention.webp" alt="`it`的attention"><br>图：<code>it</code>的attention</p>
<p>当我们编码单词”it”时，其中一个 attention head （注意力头）最关注的是”the animal”，另外一个 attention head 关注的是”tired”。因此在某种意义上，”it”在模型中的表示，融合了”animal”和”word”的部分表达。</p>
<p>然而，当我们把所有 attention heads（注意力头） 都在图上画出来时，多头注意力又变得难以解释了。</p>
<p><img src="2-all-att.png" alt="所有注意力heads"><br>图：所有注意力heads</p>
<h2 id="代码实现矩阵计算-Attention"><a href="#代码实现矩阵计算-Attention" class="headerlink" title="代码实现矩阵计算 Attention"></a>代码实现矩阵计算 Attention</h2><p>需要注意的是：在前面的讲解中，我们的 K、Q、V 矩阵的序列长度都是一样的。但是在实际中，K、V 矩阵的序列长度是一样的，而 Q 矩阵的序列长度可以不一样。</p>
<p>这种情况发生在：在解码器部分的Encoder-Decoder Attention层中，Q 矩阵是来自解码器下层，而 K、V 矩阵则是来自编码器的输出。</p>
<p><img src="2-encoder-decoder.gif" alt="encoder-decoder动态图"><br>动态图：encoder-decoder动态图</p>
<p>在完成了编码（encoding）阶段之后，我们开始解码（decoding）阶段。解码（decoding ）阶段的每一个时间步都输出一个翻译后的单词（这里的例子是英语翻译）。</p>
<p>输出是：</p>
<ul>
<li>attn_output：形状是 (L,N,E)</li>
<li>attn_output_weights：形状是 (N,L,S)<br>代码示例如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">## nn.MultiheadAttention 输入第0维为length</span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br><span class="line">query &#x3D; torch.rand(12,64,300)</span><br><span class="line"># batch_size 为 64，有 10 个词，每个词的 Key 向量是 300 维</span><br><span class="line">key &#x3D; torch.rand(10,64,300)</span><br><span class="line"># batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维</span><br><span class="line">value&#x3D; torch.rand(10,64,300)</span><br><span class="line"></span><br><span class="line">embed_dim &#x3D; 299</span><br><span class="line">num_heads &#x3D; 1</span><br><span class="line"># 输出是 (attn_output, attn_output_weights)</span><br><span class="line">multihead_attn &#x3D; nn.MultiheadAttention(embed_dim, num_heads)</span><br><span class="line">attn_output &#x3D; multihead_attn(query, key, value)[0]</span><br><span class="line"># output: torch.Size([12, 64, 300])</span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的向量是 300 维</span><br><span class="line">print(attn_output.shape)</span><br></pre></td></tr></table></figure>

<h3 id="手动实现计算-Attention"><a href="#手动实现计算-Attention" class="headerlink" title="手动实现计算 Attention"></a>手动实现计算 Attention</h3><p>在 PyTorch 提供的 MultiheadAttention  中，第 1 维是句子长度，第 2 维是 batch size。这里我们的代码实现中，第 1 维是 batch size，第 2 维是句子长度。代码里也包括：如何用矩阵实现多组注意力的并行计算。代码中已经有详细注释和说明。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">class MultiheadAttention(nn.Module):</span><br><span class="line">    # n_heads：多头注意力的数量</span><br><span class="line">    # hid_dim：每个词输出的向量维度</span><br><span class="line">    def __init__(self, hid_dim, n_heads, dropout):</span><br><span class="line">        super(MultiheadAttention, self).__init__()</span><br><span class="line">        self.hid_dim &#x3D; hid_dim</span><br><span class="line">        self.n_heads &#x3D; n_heads</span><br><span class="line"></span><br><span class="line">        # 强制 hid_dim 必须整除 h</span><br><span class="line">        assert hid_dim % n_heads &#x3D;&#x3D; 0</span><br><span class="line">        # 定义 W_q 矩阵</span><br><span class="line">        self.w_q &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        # 定义 W_k 矩阵</span><br><span class="line">        self.w_k &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        # 定义 W_v 矩阵</span><br><span class="line">        self.w_v &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.do &#x3D; nn.Dropout(dropout)</span><br><span class="line">        # 缩放</span><br><span class="line">        self.scale &#x3D; torch.sqrt(torch.FloatTensor([hid_dim &#x2F;&#x2F; n_heads]))</span><br><span class="line"></span><br><span class="line">    def forward(self, query, key, value, mask&#x3D;None):</span><br><span class="line">        # K: [64,10,300], batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br><span class="line">        # V: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span><br><span class="line">        # Q: [64,12,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span><br><span class="line">        # bsz： batch size</span><br><span class="line">        bsz &#x3D; query.shape[0]</span><br><span class="line">        Q &#x3D; self.w_q(query)</span><br><span class="line">        K &#x3D; self.w_k(key)</span><br><span class="line">        V &#x3D; self.w_v(value)</span><br><span class="line">        # 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵</span><br><span class="line">        # 最后一维就是是用 self.hid_dim &#x2F;&#x2F; self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300&#x2F;6&#x3D;50</span><br><span class="line">        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度</span><br><span class="line">        # K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]</span><br><span class="line">        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算</span><br><span class="line">        Q &#x3D; Q.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        K &#x3D; K.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        V &#x3D; V.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line"></span><br><span class="line">        # 第 1 步：Q 乘以 K的转置，除以scale</span><br><span class="line">        # [64,6,12,50] * [64,6,50,10] &#x3D; [64,6,12,10]</span><br><span class="line">        # attention：[64,6,12,10]</span><br><span class="line">        attention &#x3D; torch.matmul(Q, K.permute(0, 1, 3, 2)) &#x2F; self.scale</span><br><span class="line"></span><br><span class="line">        # 把 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10</span><br><span class="line">        if mask isnotNone:</span><br><span class="line">            attention &#x3D; attention.masked_fill(mask &#x3D;&#x3D; 0, -1e10)</span><br><span class="line"></span><br><span class="line">        # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。</span><br><span class="line">        # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax</span><br><span class="line">        # attention: [64,6,12,10]</span><br><span class="line">        attention &#x3D; self.do(torch.softmax(attention, dim&#x3D;-1))</span><br><span class="line"></span><br><span class="line">        # 第三步，attention结果与V相乘，得到多头注意力的结果</span><br><span class="line">        # [64,6,12,10] * [64,6,10,50] &#x3D; [64,6,12,50]</span><br><span class="line">        # x: [64,6,12,50]</span><br><span class="line">        x &#x3D; torch.matmul(attention, V)</span><br><span class="line"></span><br><span class="line">        # 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果</span><br><span class="line">        # x: [64,6,12,50] 转置-&gt; [64,12,6,50]</span><br><span class="line">        x &#x3D; x.permute(0, 2, 1, 3).contiguous()</span><br><span class="line">        # 这里的矩阵转换就是：把多组注意力的结果拼接起来</span><br><span class="line">        # 最终结果就是 [64,12,300]</span><br><span class="line">        # x: [64,12,6,50] -&gt; [64,12,300]</span><br><span class="line">        x &#x3D; x.view(bsz, -1, self.n_heads * (self.hid_dim &#x2F;&#x2F; self.n_heads))</span><br><span class="line">        x &#x3D; self.fc(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br><span class="line">query &#x3D; torch.rand(64, 12, 300)</span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维</span><br><span class="line">key &#x3D; torch.rand(64, 10, 300)</span><br><span class="line"># batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维</span><br><span class="line">value &#x3D; torch.rand(64, 10, 300)</span><br><span class="line">attention &#x3D; MultiheadAttention(hid_dim&#x3D;300, n_heads&#x3D;6, dropout&#x3D;0.1)</span><br><span class="line">output &#x3D; attention(query, key, value)</span><br><span class="line">## output: torch.Size([64, 12, 300])</span><br><span class="line">print(output.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="关键代码"><a href="#关键代码" class="headerlink" title="关键代码"></a>关键代码</h3><p>其中用矩阵实现多头注意力的关键代码如下所示， K、Q、V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵</span><br><span class="line">        # 最后一维就是是用 self.hid_dim &#x2F;&#x2F; self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300&#x2F;6&#x3D;50</span><br><span class="line">        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 个词，50 表示每组注意力的词的向量长度</span><br><span class="line">        # K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]</span><br><span class="line">        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算</span><br><span class="line">        Q &#x3D; Q.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        K &#x3D; K.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        V &#x3D; V.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">经过 attention 计算得到 x 的形状是 &#96;[64,12,6,50]&#96;，64 表示 batch size，6 表示有 6组注意力，10 表示有 10 个词，50 表示每组注意力的词的向量长度。把这个矩阵转换为 &#96;[64,12,300]&#96;的矩阵，就是相当于把多组注意力的结果拼接起来。</span><br><span class="line">e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e ee</span><br><span class="line"></span><br><span class="line">这里的矩阵转换就是：把多组注意力的结果拼接起来，最终结果就是 [64,12,300]，x: [64,12,6,50] -&gt; [64,12,300]</span><br><span class="line">x &#x3D; x.view(bsz, -1, self.n_heads * (self.hid_dim &#x2F;&#x2F; self.n_heads))</span><br></pre></td></tr></table></figure>

<h2 id="使用位置编码来表示序列的顺序"><a href="#使用位置编码来表示序列的顺序" class="headerlink" title="使用位置编码来表示序列的顺序"></a>使用位置编码来表示序列的顺序</h2><p>到目前为止，我们阐述的模型中缺失了一个东西，那就是表示序列中单词顺序的方法。</p>
<p>为了解决这个问题，Transformer 模型对每个输入的向量都添加了一个向量。这些向量遵循模型学习到的特定模式，有助于确定每个单词的位置，或者句子中不同单词之间的距离。这种做法背后的直觉是：将这些表示位置的向量添加到词向量中，得到了新的向量，这些新向量映射到 Q/K/V，然后计算点积得到 attention 时，可以提供有意义的信息。</p>
<p><img src="2-position.png" alt="位置编码"><br>图：位置编码</p>
<p>为了让模型了解单词的顺序，我们添加了带有位置编码的向量–这些向量的值遵循特定的模式。 如果我们假设词向量的维度是 4，那么带有位置编码的向量可能如下所示：</p>
<p><img src="2-position2.png" alt="位置编码"><br>图：位置编码</p>
<p>位置编码的可视化分析：</p>
<p>在下图中，每一行表示一个带有位置编码的向量。所以，第一行对应于序列中第一个单词的位置编码向量。每一行都包含 512 个值，每个值的范围在 -1 和 1 之间。我对这些向量进行了涂色可视化，你可以从中看到向量遵循的模式。</p>
<p><img src="2-position3.png" alt="位置编码图示"><br>图：位置编码图示</p>
<p>这是一个真实的例子，包含了 20 个词，每个词向量的维度是 512。你可以看到，它看起来像从中间一分为二。这是因为左半部分的值是由<strong>sine</strong>函数产生的，而右半部分的值是由<strong>cosine</strong>函数产生的，然后将他们拼接起来，得到每个位置编码向量。</p>
<p>你可以在get_timing_signal_1d()上查看生成位置编码的代码。这种方法来自于<code>Tranformer2Transformer</code> 的实现。</p>
<p>而论文中的方法和上面图中的稍有不同，它不是直接拼接两个向量，而是将两个向量交织在一起。如下图所示。</p>
<p><img src="2-positin4.png" alt="位置编码交织"><br>图：位置编码交织</p>
<p>此为生成位置编码的公式，在 Transformer 论文的 3.5 节中有详细说明。</p>
<p>这不是唯一一种生成位置编码的方法。但这种方法的优点是：可以扩展到未知的序列长度。例如：当我们的模型需要翻译一个句子，而这个句子的长度大于训练集中所有句子的长度，这时，这种位置编码的方法也可以生成一样长的位置编码向量。</p>
<h3 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h3><p>在我们继续讲解之前，编码器结构中有一个需要注意的细节是：编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization）。</p>
<p><img src="2-resnet.png" alt="残差连接"><br>图：残差连接</p>
<p>将 Self-Attention 层的层标准化（layer-normalization）和向量都进行可视化，如下所示：</p>
<p><img src="2-lyn.png" alt="标准化"><br>图：标准化</p>
<p>在解码器的子层里面也有层标准化（layer-normalization）。假设一个 Transformer 是由 2 层编码器和两层解码器组成的，如下图所示。</p>
<p><img src="2-2layer.png" alt="2层示意图"><br>图：2层示意图</p>
<h3 id="Decoder（解码器）"><a href="#Decoder（解码器）" class="headerlink" title="Decoder（解码器）"></a>Decoder（解码器）</h3><p>现在我们已经介绍了解码器中的大部分概念，我们也基本知道了解码器的原理。现在让我们来看下， 编码器和解码器是如何协同工作的。</p>
<p>上面说了，编码器一般有多层，第一个编码器的输入是一个序列，最后一个编码器输出是一组注意力向量 K 和 V。这些注意力向量将会输入到每个解码器的Encoder-Decoder Attention层，这有助于解码器把注意力集中中输入序列的合适位置。</p>
<p>在完成了编码（encoding）阶段之后，我们开始解码（decoding）阶段。解码（decoding ）阶段的每一个时间步都输出一个翻译后的单词（这里的例子是英语翻译）。</p>
<p>接下来会重复这个过程，直到输出一个结束符，Transformer 就完成了所有的输出。每一步的输出都会在下一个时间步输入到下面的第一个解码器。Decoder 就像 Encoder 那样，从下往上一层一层地输出结果。正对如编码器的输入所做的处理，我们把解码器的输入向量，也加上位置编码向量，来指示每个词的位置。</p>
<p><img src="2-decoder.gif" alt="decoder动态图"><br>动态图：decoder动态图</p>
<p>解码器中的 Self Attention 层，和编码器中的 Self Attention 层不太一样：<strong>在解码器里，Self Attention 层只允许关注到输出序列中早于当前位置之前的单词</strong>。具体做法是：在 Self Attention 分数经过 Softmax 层之前，屏蔽当前位置之后的那些位置。</p>
<p>Encoder-Decoder Attention层的原理和多头注意力（multiheaded Self Attention）机制类似，<strong>不同之处</strong>是：Encoder-Decoder Attention层是使用前一层的输出来构造 Query 矩阵，而 Key 矩阵和 Value 矩阵来自于解码器最终的输出。</p>
<h3 id="最后的线性层和-Softmax-层"><a href="#最后的线性层和-Softmax-层" class="headerlink" title="最后的线性层和 Softmax 层"></a>最后的线性层和 Softmax 层</h3><p>Decoder 最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是由 Softmax 层后面的线性层来完成的。</p>
<p><strong>线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更长的向量，这个向量称为 logits 向量</strong>。</p>
<p><strong>Softmax 层会把这些分数转换为概率（把所有的分数转换为正数，并且加起来等于 1）。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词</strong>。</p>
<p><img src="2-linear.png" alt="线性层"><br>图：线性层</p>
<p>在上图中，最下面的向量，就是编码器的输出，这个向量输入到线性层和 Softmax 层，最终得到输出的词。</p>
<h3 id="Transformer-的训练过程"><a href="#Transformer-的训练过程" class="headerlink" title="Transformer 的训练过程"></a>Transformer 的训练过程</h3><p>现在我们已经了解了 Transformer 的前向传播过程，下面讲讲 Transformer 的训练过程，这也是非常有用的知识。</p>
<p>在训练过程中，模型会经过上面讲的所有前向传播的步骤。但是，当我们在一个标注好的数据集上训练这个模型的时候，我们可以对比模型的输出和真实的标签。</p>
<p>为了可视化这个对比，让我们假设输出词汇表只包含 6 个单词（“a”, “am”, “i”, “thanks”, “student”, and “<eos>”（“<eos>”表示句子末尾））。</p>
<p><img src="2-6words.webp" alt="6个词"><br>图：6个词</p>
<p>我们模型的输出词汇表，是在训练之前的数据预处理阶段构造的。当我们确定了输出词汇表，我们可以用向量来表示词汇表中的每个单词。这个表示方法也称为  one-hot encoding。例如，我们可以把单词 “am” 用下面的向量来表示：</p>
<p><img src="2-am.webp" alt="am向量"><br>图：am向量</p>
<p>介绍了训练过程，我们接着讨论模型的损失函数，这我们在训练时需要优化的目标，通过优化这个目标来得到一个训练好的、非常精确的模型。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>用一个简单的例子来说明训练过程，比如：把“merci”翻译为“thanks”。</p>
<p>这意味着我们希望模型最终输出的概率分布，会指向单词 ”thanks“（在“thanks”这个词的概率最高）。但模型还没训练好，它输出的概率分布可能和我们希望的概率分布相差甚远。</p>
<p><img src="2-loss.webp" alt="概率分布"><br>图：概率分布</p>
<p>由于模型的参数都是随机初始化的。模型在每个词输出的概率都是随机的。我们可以把这个概率和正确的输出概率做对比，然后使用反向传播来调整模型的权重，使得输出的概率分布更加接近正确(pq)输出。</p>
<p>那我们要怎么比较两个概率分布呢？我们可以简单地用一个概率分布减去另一个概率分布。关于更多细节，你可以查看<strong>交叉熵</strong>(cross-entropy)]和<strong>KL 散度</strong>(Kullback–Leibler divergence)的相关概念。</p>
<p>但上面的例子是经过简化的，因为我们的句子只有一个单词。在实际中，我们使用的句子不只有一个单词。例如–输入是：“je suis étudiant” ，输出是：“i am a student”。这意味着，我们的模型需要输出多个概率分布，满足如下条件：</p>
<ul>
<li>每个概率分布都是一个向量，长度是 vocab_size（我们的例子中，向量长度是 6，但实际中更可能是 30000 或者 50000）</li>
<li>第一个概率分布中，最高概率对应的单词是 “i”</li>
<li>第二个概率分布中，最高概率对应的单词是 “am”</li>
<li>以此类推，直到第 5 个概率分布中，最高概率对应的单词是 “&lt;eos&gt;(pq)”，表示没有下一个单词了</li>
</ul>
<p><img src="2-target.png" alt="概率分布"><br>图：概率分布</p>
<p>我们用例子中的句子训练模型，希望产生图中所示的概率分布<br>我们的模型在一个足够大的数据集上，经过足够长时间的训练后，希望输出的概率分布如下图所示：</p>
<p><img src="2-trained.webp" alt="训练后概率分布"><br>图：训练后概率分布</p>
<p>希望经过训练，模型会输出我们希望的正确翻译。当然，如果你要翻译的句子是训练集中的一部分，那输出的结果并不能说明什么。我们希望的是模型在没见过的句子上也能够准确翻译。需要注意的是：概率分布向量中，每个位置都会有一点概率，即使这个位置不是输出对应的单词–这是 Softmax 中一个很有用的特性，有助于帮助训练过程。</p>
<p>现在，由于模型每个时间步只产生一个输出，我们可以认为：模型是从概率分布中选择概率最大的词，并且丢弃其他词。这种方法叫做<strong>贪婪解码</strong>（greedy decoding）。</p>
<p>另一种方法是每个时间步保留两个最高概率的输出词，然后在下一个时间步，重复执行这个过程：假设第一个位置概率最高的两个输出的词是”I“和”a“，这两个词都保留，然后根据第一个词计算第二个位置的词的概率分布，再取出 2 个概率最高的词，对于第二个位置和第三个位置，我们也重复这个过程。这种方法称为<strong>集束搜索</strong>(beam search)，在我们的例子中，beam_size 的值是 2（含义是：在所有时间步，我们保留两个最高概率），top_beams 的值也是 2（表示我们最终会返回两个翻译的结果）。beam_size 和 top_beams 都是你可以在实验中尝试的超参数。</p>
<h3 id="更进一步理解"><a href="#更进一步理解" class="headerlink" title="更进一步理解"></a>更进一步理解</h3><p>我希望上面讲的内容，可以帮助你理解 Transformer 中的主要概念。如果你想更深一步地理解，我建议你可以参考下面这些：</p>
<ul>
<li>阅读 Transformer 的论文：<br>《Attention Is All You Need》<br>链接地址：<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
<li>阅读Transformer 的博客文章：<br>《Transformer: A Novel Neural Network Architecture for Language Understanding》<br>链接地址：<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a><br>阅读《Tensor2Tensor announcement》</li>
<li>链接地址：<a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html">https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html</a></li>
<li>观看视频 【Łukasz Kaiser’s talk】来理解模型和其中的细节<br>链接地址：<a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">https://www.youtube.com/watch?v=rBCqOTEfxvg</a><br>运行这份代码：【Jupyter Notebook provided as part of the Tensor2Tensor repo】</li>
<li>链接地址：<a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb%E3%80%82">https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb。</a></li>
<li>查看这个项目：【Tensor2Tensor repo】<br>链接地址：<a href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a></li>
</ul>
<h3 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h3><p>主要由哈尔滨工业大学张贤同学翻译撰写，由本项目同学组织和整理。最后，期待您的阅读反馈和star哦，谢谢。</p>
<h3 id="转载并编辑"><a href="#转载并编辑" class="headerlink" title="转载并编辑"></a>转载并编辑</h3><p>童茗 整理笔记</p>
<p>编码器是并行的，但是解码器和RNN一样不能并行。</p>
]]></content>
      
        
        <tags>
            
            <tag> DataWhale-NLP-Transformer </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title></title>
      <url>/2021/08/18/Task02%20%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<h1 id="Task02-消息传递图神经网络"><a href="#Task02-消息传递图神经网络" class="headerlink" title="Task02 消息传递图神经网络"></a>Task02 消息传递图神经网络</h1><h2 id="笔记部分"><a href="#笔记部分" class="headerlink" title="笔记部分"></a>笔记部分</h2><ul>
<li><p>消息传递范式</p>
<ul>
<li><p>用$\mathbf{x}^{(k-1)}<em>i\in\mathbb{R}^F$表示$(k-1)$层中节点$i$的节点特征，$\mathbf{e}</em>{j,i} \in \mathbb{R}^D$ 表示从节点$j$到节点$i$的边的特征，消息传递图神经网络可以描述为</p>
</li>
<li><p>$$<br>\mathbf{x}_i^{(k)} = \gamma^{(k)} \left( \mathbf{x}<em>i^{(k-1)}, \square</em>{j \in \mathcal{N}(i)} , \phi^{(k)}\left(\mathbf{x}_i^{(k-1)}, \mathbf{x}<em>j^{(k-1)},\mathbf{e}</em>{j,i}\right) \right),<br>$$</p>
<p><img src="file://C:\Users\qinan\Desktop\team-study\team-learning-nlp\GNN\Markdown%E7%89%88%E6%9C%AC\images\image-20210516110407207.png?lastModify=1624111214" alt="节点嵌入（Node Embedding）"></p>
<p>图片展示了<strong>基于消息传递范式的生成节点表征的过程</strong>：</p>
<ol>
<li>在图的最右侧，B节点的邻接节点（A,C）的信息传递给了B，经过信息变换得到了B的嵌入，C、D节点同。</li>
<li>在图的中右侧，A节点的邻接节点（B,C,D）的之前得到的节点嵌入传递给了节点A；在图的中左侧，聚合得到的信息经过信息变换得到了A节点新的嵌入。</li>
<li>重复多次，我们可以得到每一个节点的经过多次信息变换的嵌入。这样的经过多次信息聚合与变换的节点嵌入就可以作为节点的表征，可以用于节点的分类。</li>
</ol>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>继承<code>MessagePassing</code>类的<code>GCNConv</code></p>
<ul>
<li>GCNConv的数学定义为</li>
</ul>
<p>$$<br>\mathbf{x}<em>i^{(k)} = \sum</em>{j \in \mathcal{N}(i) \cup { i }} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right),<br>$$</p>
<ul>
<li><p>其中，相邻节点的特征首先通过权重矩阵$\mathbf{\Theta}$进行转换，然后按端点的度进行归一化处理，最后进行加总。</p>
</li>
<li><p>步骤细分：</p>
</li>
</ul>
<ol>
<li>向邻接矩阵添加自环边。</li>
<li>线性转换节点特征矩阵。</li>
<li>计算归一化系数。</li>
<li>归一化$j$中的节点特征。</li>
<li>将相邻节点特征相加（”求和 “聚合）。</li>
</ol>
</li>
</ul>
<ul>
<li><p>作业</p>
<ul>
<li><p>MessagePassing的运行流程：</p>
<ul>
<li>通过线性变换以及利用归一化系数将源节点往目标节点传递特征</li>
<li>通过多种方式（max、average、sum）对特征进行聚合</li>
<li>将聚合后的信息再次进行转换</li>
</ul>
</li>
<li><p>继承<code>MessagePassing</code>类的规范,并请继承<code>MessagePassing</code>类来自定义几个的图神经网络类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> MessagePassing</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyGNN</span>(<span class="params">MessagePassing</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \mathbf&#123;x&#125;^&#123;\prime&#125;_i = \mathbf&#123;x&#125;_i \cdot \mathbf&#123;\Theta&#125;_1 +</span></span><br><span class="line"><span class="string">        \sum_&#123;j \in \mathcal&#123;N&#125;(i)&#125; e_&#123;j,i&#125; \cdot</span></span><br><span class="line"><span class="string">        (\mathbf&#123;\Theta&#125;_2 \mathbf&#123;x&#125;_i - \mathbf&#123;\Theta&#125;_3 \mathbf&#123;x&#125;_j)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, device</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyGNN, self).__init__(aggr=<span class="string">&#x27;add&#x27;</span>)</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line"></span><br><span class="line">        self.lin1 = torch.nn.Linear(in_channels, out_channels).to(device)</span><br><span class="line">        self.lin2 = torch.nn.Linear(in_channels, out_channels).to(device)</span><br><span class="line">        self.lin3 = torch.nn.Linear(in_channels, out_channels).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        a = self.lin1(x)</span><br><span class="line">        b = self.lin2(x)</span><br><span class="line">        out = self.propagate(edge_index, a=a, b=b)</span><br><span class="line">        <span class="keyword">return</span> self.lin3(x) + out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">message</span>(<span class="params">self, a_i, b_j</span>):</span></span><br><span class="line">        out = a_i - b_j</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#123;&#125;(&#123;&#125;, &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(self.__class__.__name__, self.in_channels,</span><br><span class="line">                                   self.out_channels)</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dataset = Planetoid(root=<span class="string">&#x27;dataset/Cora&#x27;</span>, name=<span class="string">&#x27;Cora&#x27;</span>)</span><br><span class="line">model = MyGNN(in_channels=dataset.num_features, out_channels=dataset.num_classes, device=device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line">data = dataset[<span class="number">0</span>].to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out = model(data.x, data.edge_index).to(device)</span><br><span class="line">    pred = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">    accuracy = <span class="built_in">int</span>((pred[data.test_mask] == data.y[data.test_mask]).<span class="built_in">sum</span>()) / data.test_mask.<span class="built_in">sum</span>()</span><br><span class="line">    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Train Epoch: &#123;:3&#125; Accuracy: &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(epoch, accuracy.item() * <span class="number">100.0</span>))</span><br><span class="line">        </span><br><span class="line">参考自@天国之影 http://relph.gitee.io/my-team-learning/<span class="comment">#/gnn_learning26/task02</span></span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>/2021/08/18/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title></title>
      <url>/2021/08/18/AI%E4%BA%A4%E5%8F%89/</url>
      <content type="html"></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[知识图谱笔记]]></title>
      <url>/2021/04/16/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h1><ul>
<li><a href="https://github.com/km1994/NLP-Interview-Notes/tree/main/NLPinterview/KG">https://github.com/km1994/NLP-Interview-Notes/tree/main/NLPinterview/KG</a></li>
</ul>
<ol>
<li>什么是schema？<br> 即是给出特定的数据格式，如实体的属性只有年龄 姓名，那么加入此知识图谱的实体就必须符合这个要求，加入如“性别”就不行。</li>
<li>信息抽取的难点在哪里？<br> 处理非结构化数据，抽取出结构化数据（实体和关系）</li>
<li>构建KG所涉及到的技术<ul>
<li>NER（实体命名识别）<ul>
<li>目标：从文本中提取出实体并对每个实体做“分类/打标签”<br>如“Virgil’s BBQ”提取出并标记上“Restaurant”</li>
<li>目前技术已经比较成熟</li>
</ul>
</li>
<li>关系抽取（Relation Extaction）</li>
<li>实体对齐（Entity Resolution **）<ul>
<li>如NYC和new york city指得同一个东西需要对齐</li>
</ul>
</li>
<li>指代消解（Coreference Resolution）<br>“it”指代的哪个名词</li>
</ul>
</li>
<li>KG的存储和查询<ul>
<li>知识图谱并不一定用图数据库（PDB），数据库只是一种存储形式，特定场景可以用不同数据库（如不用扩展的可以用关系型数据库）</li>
<li>eg： 查 Bob’s friend’s friend’s friend<ul>
<li>若使用关系型数据库，就要一直做joint，查询时间指数级增长</li>
<li>但是若使用图数据库，查询时间只会线性增长</li>
</ul>
</li>
<li>PDB图数据库的好处：<ul>
<li>表达更接近于自然语言</li>
<li>易扩展</li>
<li>性能好</li>
</ul>
</li>
</ul>
</li>
</ol>
]]></content>
      
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[hexo调试日志]]></title>
      <url>/2021/03/24/hexo%E8%B0%83%E8%AF%95%E6%97%A5%E5%BF%97/</url>
      <content type="html"><![CDATA[<ol>
<li>hexo d出错 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Error: Spawn failed at ChildProcess.&lt;anonymous&gt; (E:\blog\node_modules\hexo-</span><br><span class="line">util\lib\spawn.js:51:21) at ChildProcess.emit (events.js:210:5) at ChildProcess.cp.emit (E:\blog\node_modules\cross-</span><br><span class="line">spawn\lib\enoent.js:34:29) at Process.ChildProcess._handle.onexit (internal&#x2F;child_process.js:272:12)</span><br></pre></td></tr></table></figure>
<ul>
<li>解决办法：</li>
</ul>
<ol>
<li>删除.deploy_git文件</li>
<li>依次执行<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dexo clean</span><br><span class="line">dexo g</span><br><span class="line">dexo d</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
]]></content>
      
        
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[调试日志]]></title>
      <url>/2021/03/24/python%E8%B0%83%E8%AF%95%E6%97%A5%E5%BF%97/</url>
      <content type="html"><![CDATA[<hr>
<ol>
<li><p>如果函数def中没有return，只有print，如果再在引用时使用print，那么最后会有none输出。</p>
</li>
<li><p>print不换行加  end=“”</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;....&quot;, end&#x3D;&quot;&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>计算字符串中各字母出现的次数：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def get_frequency_dict(sequence):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Returns a dictionary where the keys are elements of the sequence</span><br><span class="line">    and the values are integer counts, for the number of times that</span><br><span class="line">    an element is repeated in the sequence.</span><br><span class="line"></span><br><span class="line">    sequence: string or list</span><br><span class="line">    return: dictionary</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # freqs: dictionary (element_type -&gt; int)</span><br><span class="line">    freq &#x3D; &#123;&#125;</span><br><span class="line">    for x in sequence:</span><br><span class="line">        freq[x] &#x3D; freq.get(x,0) + 1</span><br><span class="line">    return freq</span><br></pre></td></tr></table></figure></li>
<li><p><strong>dict.get(‘a’,0) and dict[‘a’]</strong>:usual way to access a value is hand[‘a’], where ‘a’<br>​is the key we want to find. However,this only works if the key is in the dictionary; otherwise, we get a KeyError ​. To avoid this, we can instead use the function call hand.get(‘a’,0)</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get(key, [default])</span><br><span class="line">Return the value for key if key is in the dictionary, else default. If default is not given, it defaults to None, so that this method never raises a KeyError.</span><br></pre></td></tr></table></figure></li>
<li><p>大小写转换</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">str &#x3D; &quot;www.runoob.com&quot;</span><br><span class="line">print(str.upper())          # 把所有字符中的小写字母转换成大写字母</span><br><span class="line">print(str.lower())          # 把所有字符中的大写字母转换成小写字母</span><br></pre></td></tr></table></figure></li>
<li><p>在字典遍历过程中修改字典元素，报错  <strong>得知遍历时不能修改字典元素</strong></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for k in func_dict.keys():</span><br><span class="line">    if func_dict[k] is np.nan:</span><br><span class="line">        del func_dict[k]</span><br><span class="line">        continue</span><br><span class="line">RuntimeError: dictionary changed size during iteration</span><br></pre></td></tr></table></figure>
<p> <strong>解决办法：将遍历条件改为列表</strong></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for k in list(func_dict.keys()):</span><br><span class="line">if func_dict[k] is np.nan:</span><br><span class="line">    del func_dict[k]</span><br><span class="line">    continue</span><br></pre></td></tr></table></figure></li>
<li><p>dict.copy()才是复制，而等于只是引用而已,字符串复制直接可以用等号</p>
</li>
<li><p>How can I add new keys to a dictionary?</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d &#x3D; &#123;&#39;key&#39;: &#39;value&#39;&#125;</span><br><span class="line">print(d)  # &#123;&#39;key&#39;: &#39;value&#39;&#125;</span><br><span class="line"></span><br><span class="line">d[&#39;mynewkey&#39;] &#x3D; &#39;mynewvalue&#39;</span><br><span class="line"></span><br><span class="line">print(d)  # &#123;&#39;key&#39;: &#39;value&#39;, &#39;mynewkey&#39;: &#39;mynewvalue&#39;&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>input() can only has one parameter</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nilai_tinggi &#x3D; int(input(&quot;Enter the height of children number &quot;, str(A)))</span><br><span class="line">TypeError: raw_input() takes from 1 to 2 positional arguments but 3 were given</span><br></pre></td></tr></table></figure>
<p> 解决方法：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nilai_tinggi &#x3D; int(input(&quot;Enter the height of children number %d&quot; %A))</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hello, World Article!]]></title>
      <url>/2021/03/24/Hello-World-Article/</url>
      <content type="html"><![CDATA[<h1 id="hello-word"><a href="#hello-word" class="headerlink" title="hello word!"></a>hello word!</h1>]]></content>
      
        
    </entry>
    
  
  
</search>
