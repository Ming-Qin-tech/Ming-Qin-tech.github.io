<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Task07-Transformer解决序列标注问题]]></title>
      <url>/2021/08/27/Task07-Transformer%E8%A7%A3%E5%86%B3%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h1 id="0-总结"><a href="#0-总结" class="headerlink" title="0 总结"></a>0 总结</h1><p>版本问题导致evaluate步骤出错<br>序列标注任务和文本分类任务相比，只是在最后一层有所改动</p>
<h1 id="1-序列标注任务简介"><a href="#1-序列标注任务简介" class="headerlink" title="1 序列标注任务简介"></a>1 序列标注任务简介</h1><p>常见的三种token级别的分类任务：</p>
<ul>
<li>NER，标注 person、organization\location 等</li>
<li>POS(Part-of-speech tagging，词性标注) ： 名词、动词和形容词等</li>
<li>Chunk（Chunking短语组块）：将同一短语的token组块放在一起。</li>
</ul>
<p>注意根据GPU显存调整batch size 的大小</p>
<h1 id="2-选择一种分类任务"><a href="#2-选择一种分类任务" class="headerlink" title="2 选择一种分类任务"></a>2 选择一种分类任务</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">task = <span class="string">&quot;ner&quot;</span> <span class="comment">#需要是&quot;ner&quot;, &quot;pos&quot; 或者 &quot;chunk&quot;</span></span><br><span class="line">model_checkpoint = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br></pre></td></tr></table></figure>

<h1 id="3-加载数据及数据分析"><a href="#3-加载数据及数据分析" class="headerlink" title="3 加载数据及数据分析"></a>3 加载数据及数据分析</h1><h2 id="3-1-加载数据"><a href="#3-1-加载数据" class="headerlink" title="3.1 加载数据"></a>3.1 加载数据</h2><p>同Task06</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, load_metric</span><br></pre></td></tr></table></figure>
<p>加载数据官方指南：<a href="https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files">数据集文档</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">datasets = load_dataset(<span class="string">&quot;conll2003&quot;</span>)</span><br><span class="line"><span class="comment"># 此处加载[CONLL 2003 dataset](https://www.aclweb.org/anthology/W03-0419.pdf)数据集。</span></span><br></pre></td></tr></table></figure>

<h2 id="3-2-数据分析"><a href="#3-2-数据分析" class="headerlink" title="3.2 数据分析"></a>3.2 数据分析</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">datasets</span><br></pre></td></tr></table></figure>




<pre><code>DatasetDict(&#123;
    train: Dataset(&#123;
        features: [&#39;id&#39;, &#39;tokens&#39;, &#39;pos_tags&#39;, &#39;chunk_tags&#39;, &#39;ner_tags&#39;],
        num_rows: 14041
    &#125;)
    validation: Dataset(&#123;
        features: [&#39;id&#39;, &#39;tokens&#39;, &#39;pos_tags&#39;, &#39;chunk_tags&#39;, &#39;ner_tags&#39;],
        num_rows: 3250
    &#125;)
    test: Dataset(&#123;
        features: [&#39;id&#39;, &#39;tokens&#39;, &#39;pos_tags&#39;, &#39;chunk_tags&#39;, &#39;ner_tags&#39;],
        num_rows: 3453
    &#125;)
&#125;)
</code></pre>
<p>‘datasets’本身是一种<a href="https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict"><code>DatasetDict</code></a>数据结构.拥有：</p>
<ul>
<li>训练集</li>
<li>验证集</li>
<li>测试集<br>只需要使用对应的key（train，validation，set）即可得到对应数据。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 给定一个数据切分的key（train、validation或者test）和下标即可查看数据。</span></span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;chunk_tags&#39;: [11, 21, 11, 12, 21, 22, 11, 12, 0],
 &#39;id&#39;: &#39;0&#39;,
 &#39;ner_tags&#39;: [3, 0, 7, 0, 0, 0, 7, 0, 0],
 &#39;pos_tags&#39;: [22, 42, 16, 21, 35, 37, 16, 21, 7],
 &#39;tokens&#39;: [&#39;EU&#39;,
  &#39;rejects&#39;,
  &#39;German&#39;,
  &#39;call&#39;,
  &#39;to&#39;,
  &#39;boycott&#39;,
  &#39;British&#39;,
  &#39;lamb&#39;,
  &#39;.&#39;]&#125;
</code></pre>
<p>所有的数据标签labels都已经被编码成了整数，可以直接被预训练transformer模型使用。这些整数的编码所对应的实际类别储存在<code>features</code>中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">datasets[<span class="string">&quot;train&quot;</span>].features[<span class="string">f&quot;ner_tags&quot;</span>]</span><br></pre></td></tr></table></figure>
<pre><code>Sequence(feature=ClassLabel(num_classes=9, names=[&#39;O&#39;, &#39;B-PER&#39;, &#39;I-PER&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;], names_file=None, id=None), length=-1, id=None)
- &#39;PER&#39; for person
- &#39;ORG&#39; for organization
- &#39;LOC&#39; for location
- &#39;MISC&#39; for miscellaneous
‘0’的意思是没有实体。 ‘B-’前缀代表实体开始的token，‘I-’前缀代表实体中间的token。
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">label_list = datasets[<span class="string">&quot;train&quot;</span>].features[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>].feature.names</span><br><span class="line">label_list</span><br></pre></td></tr></table></figure>




<pre><code>[&#39;O&#39;, &#39;B-PER&#39;, &#39;I-PER&#39;, &#39;B-ORG&#39;, &#39;I-ORG&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;B-MISC&#39;, &#39;I-MISC&#39;]
</code></pre>
<p>同Task06， 为了能够进一步理解数据长什么样子，下面的函数将从数据集里随机选择几个例子进行展示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> ClassLabel, <span class="type">Sequence</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_random_elements</span>(<span class="params">dataset, num_examples=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="keyword">assert</span> num_examples &lt;= <span class="built_in">len</span>(dataset), <span class="string">&quot;Can&#x27;t pick more elements than there are in the dataset.&quot;</span></span><br><span class="line">    picks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> pick <span class="keyword">in</span> picks:</span><br><span class="line">            pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        picks.append(pick)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(dataset[picks])</span><br><span class="line">    <span class="keyword">for</span> column, typ <span class="keyword">in</span> dataset.features.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(typ, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> i: typ.names[i])</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(typ, <span class="type">Sequence</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(typ.feature, ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> x: [typ.feature.names[i] <span class="keyword">for</span> i <span class="keyword">in</span> x])</span><br><span class="line">    display(HTML(df.to_html()))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_random_elements(datasets[<span class="string">&quot;train&quot;</span>])</span><br></pre></td></tr></table></figure>


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>tokens</th>
      <th>pos_tags</th>
      <th>chunk_tags</th>
      <th>ner_tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2227</td>
      <td>[Result, of, a, French, first, division, match, on, Friday, .]</td>
      <td>[NN, IN, DT, JJ, JJ, NN, NN, IN, NNP, .]</td>
      <td>[B-NP, B-PP, B-NP, I-NP, I-NP, I-NP, I-NP, B-PP, B-NP, O]</td>
      <td>[O, O, O, B-MISC, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2615</td>
      <td>[Mid-tier, golds, up, in, heavy, trading, .]</td>
      <td>[NN, NNS, IN, IN, JJ, NN, .]</td>
      <td>[B-NP, I-NP, B-PP, B-PP, B-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10256</td>
      <td>[Neagle, (, 14-6, ), beat, the, Braves, for, the, third, time, this, season, ,, allowing, two, runs, and, six, hits, in, eight, innings, .]</td>
      <td>[NNP, (, CD, ), VB, DT, NNPS, IN, DT, JJ, NN, DT, NN, ,, VBG, CD, NNS, CC, CD, NNS, IN, CD, NN, .]</td>
      <td>[B-NP, O, B-NP, O, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, I-NP, B-NP, I-NP, O, B-VP, B-NP, I-NP, O, B-NP, I-NP, B-PP, B-NP, I-NP, O]</td>
      <td>[B-PER, O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10720</td>
      <td>[Hansa, Rostock, 4, 1, 2, 1, 5, 4, 5]</td>
      <td>[NNP, NNP, CD, CD, CD, CD, CD, CD, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7125</td>
      <td>[MONTREAL, 70, 59, .543, 11]</td>
      <td>[NNP, CD, CD, CD, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, O, O, O, O]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3316</td>
      <td>[Softbank, Corp, said, on, Friday, that, it, would, procure, $, 900, million, through, the, foreign, exchange, market, by, September, 5, as, part, of, its, acquisition, of, U.S., firm, ,, Kingston, Technology, Co, .]</td>
      <td>[NNP, NNP, VBD, IN, NNP, IN, PRP, MD, NN, $, CD, CD, IN, DT, JJ, NN, NN, IN, NNP, CD, IN, NN, IN, PRP$, NN, IN, NNP, NN, ,, NNP, NNP, NNP, .]</td>
      <td>[B-NP, I-NP, B-VP, B-PP, B-NP, B-SBAR, B-NP, B-VP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, O, B-NP, I-NP, I-NP, O]</td>
      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, B-ORG, I-ORG, I-ORG, O]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3923</td>
      <td>[Ghent, 3, Aalst, 2]</td>
      <td>[NN, CD, NNP, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, O, B-ORG, O]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2776</td>
      <td>[The, separatists, ,, who, swept, into, Grozny, on, August, 6, ,, still, control, large, areas, of, the, centre, of, town, ,, and, Russian, soldiers, are, based, at, checkpoints, on, the, approach, roads, .]</td>
      <td>[DT, NNS, ,, WP, VBD, IN, NNP, IN, NNP, CD, ,, RB, VBP, JJ, NNS, IN, DT, NN, IN, NN, ,, CC, JJ, NNS, VBP, VBN, IN, NNS, IN, DT, NN, NNS, .]</td>
      <td>[B-NP, I-NP, O, B-NP, B-VP, B-PP, B-NP, B-PP, B-NP, I-NP, O, B-ADVP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, O, O, B-NP, I-NP, B-VP, I-VP, B-PP, B-NP, B-PP, B-NP, I-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1178</td>
      <td>[Doctor, Masserigne, Ndiaye, said, medical, staff, were, overwhelmed, with, work, ., "]</td>
      <td>[NNP, NNP, NNP, VBD, JJ, NN, VBD, VBN, IN, NN, ., "]</td>
      <td>[B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-VP, I-VP, B-PP, B-NP, O, O]</td>
      <td>[O, B-PER, I-PER, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10988</td>
      <td>[Reuters, historical, calendar, -, September, 4, .]</td>
      <td>[NNP, JJ, NN, :, NNP, CD, .]</td>
      <td>[B-NP, I-NP, I-NP, O, B-NP, I-NP, O]</td>
      <td>[B-ORG, O, O, O, O, O, O]</td>
    </tr>
  </tbody>
</table>

<h1 id="4-预处理数据"><a href="#4-预处理数据" class="headerlink" title="4 预处理数据"></a>4 预处理数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">    </span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</span><br></pre></td></tr></table></figure>
<p>注意：以下代码要求tokenizer必须是transformers.PreTrainedTokenizerFast类型，因为我们在预处理的时候需要用到fast tokenizer的一些特殊特性（比如多线程快速tokenizer）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">isinstance</span>(tokenizer, transformers.PreTrainedTokenizerFast)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer(<span class="string">&quot;Hello, this is one sentence!&quot;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;input_ids&#39;: [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 102], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer([<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;,&quot;</span>, <span class="string">&quot;this&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;one&quot;</span>, <span class="string">&quot;sentence&quot;</span>, <span class="string">&quot;split&quot;</span>, <span class="string">&quot;into&quot;</span>, <span class="string">&quot;words&quot;</span>, <span class="string">&quot;.&quot;</span>], is_split_into_words=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;input_ids&#39;: [101, 7592, 1010, 2023, 2003, 2028, 6251, 3975, 2046, 2616, 1012, 102], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;
</code></pre>
<h2 id="4-1-tokenizer会将word继续切分"><a href="#4-1-tokenizer会将word继续切分" class="headerlink" title="4.1 tokenizer会将word继续切分"></a>4.1 tokenizer会将word继续切分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">example = datasets[<span class="string">&quot;train&quot;</span>][<span class="number">4</span>]</span><br><span class="line"><span class="built_in">print</span>(example[<span class="string">&quot;tokens&quot;</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;Germany&#39;, &quot;&#39;s&quot;, &#39;representative&#39;, &#39;to&#39;, &#39;the&#39;, &#39;European&#39;, &#39;Union&#39;, &quot;&#39;s&quot;, &#39;veterinary&#39;, &#39;committee&#39;, &#39;Werner&#39;, &#39;Zwingmann&#39;, &#39;said&#39;, &#39;on&#39;, &#39;Wednesday&#39;, &#39;consumers&#39;, &#39;should&#39;, &#39;buy&#39;, &#39;sheepmeat&#39;, &#39;from&#39;, &#39;countries&#39;, &#39;other&#39;, &#39;than&#39;, &#39;Britain&#39;, &#39;until&#39;, &#39;the&#39;, &#39;scientific&#39;, &#39;advice&#39;, &#39;was&#39;, &#39;clearer&#39;, &#39;.&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenized_input = tokenizer(example[<span class="string">&quot;tokens&quot;</span>], is_split_into_words=<span class="literal">True</span>)</span><br><span class="line">tokens = tokenizer.convert_ids_to_tokens(tokenized_input[<span class="string">&quot;input_ids&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(tokens)</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;[CLS]&#39;, &#39;germany&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;representative&#39;, &#39;to&#39;, &#39;the&#39;, &#39;european&#39;, &#39;union&#39;, &quot;&#39;&quot;, &#39;s&#39;, &#39;veterinary&#39;, &#39;committee&#39;, &#39;werner&#39;, &#39;z&#39;, &#39;##wing&#39;, &#39;##mann&#39;, &#39;said&#39;, &#39;on&#39;, &#39;wednesday&#39;, &#39;consumers&#39;, &#39;should&#39;, &#39;buy&#39;, &#39;sheep&#39;, &#39;##me&#39;, &#39;##at&#39;, &#39;from&#39;, &#39;countries&#39;, &#39;other&#39;, &#39;than&#39;, &#39;britain&#39;, &#39;until&#39;, &#39;the&#39;, &#39;scientific&#39;, &#39;advice&#39;, &#39;was&#39;, &#39;clearer&#39;, &#39;.&#39;, &#39;[SEP]&#39;]
</code></pre>
<h2 id="4-2-对切分后的token与原word对齐"><a href="#4-2-对切分后的token与原word对齐" class="headerlink" title="4.2 对切分后的token与原word对齐"></a>4.2 对切分后的token与原word对齐</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tokenized_input.word_ids())</span><br><span class="line"><span class="comment"># word_ids方法能将subtokens和words还有标注的labels对齐。</span></span><br></pre></td></tr></table></figure>

<pre><code>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word_ids = tokenized_input.word_ids()</span><br><span class="line">aligned_labels = [-<span class="number">100</span> <span class="keyword">if</span> i <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> example[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> word_ids]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(aligned_labels), <span class="built_in">len</span>(tokenized_input[<span class="string">&quot;input_ids&quot;</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>39 39
</code></pre>
<p>两种对齐label的方式：</p>
<ul>
<li>多个subtokens对齐一个word，对齐一个label</li>
<li>多个subtokens的第一个subtoken对齐word，对齐一个label，其他subtokens直接赋予-100.</li>
</ul>
<h2 id="4-3-整合预处理函数"><a href="#4-3-整合预处理函数" class="headerlink" title="4.3 整合预处理函数"></a>4.3 整合预处理函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##？？？？</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_and_align_labels</span>(<span class="params">examples</span>):</span></span><br><span class="line">    tokenized_inputs = tokenizer(examples[<span class="string">&quot;tokens&quot;</span>], truncation=<span class="literal">True</span>, is_split_into_words=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>]):</span><br><span class="line">        word_ids = tokenized_inputs.word_ids(batch_index=i)</span><br><span class="line">        previous_word_idx = <span class="literal">None</span></span><br><span class="line">        label_ids = []</span><br><span class="line">        <span class="keyword">for</span> word_idx <span class="keyword">in</span> word_ids:</span><br><span class="line">            <span class="comment"># Special tokens have a word id that is None. We set the label to -100 so they are automatically</span></span><br><span class="line">            <span class="comment"># ignored in the loss function.</span></span><br><span class="line">            <span class="keyword">if</span> word_idx <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                label_ids.append(-<span class="number">100</span>)</span><br><span class="line">            <span class="comment"># We set the label for the first token of each word.</span></span><br><span class="line">            <span class="keyword">elif</span> word_idx != previous_word_idx:</span><br><span class="line">                label_ids.append(label[word_idx])</span><br><span class="line">            <span class="comment"># For the other tokens in a word, we set the label to either the current label or -100, depending on</span></span><br><span class="line">            <span class="comment"># the label_all_tokens flag.</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_ids.append(label[word_idx] <span class="keyword">if</span> label_all_tokens <span class="keyword">else</span> -<span class="number">100</span>)</span><br><span class="line">            previous_word_idx = word_idx</span><br><span class="line"></span><br><span class="line">        labels.append(label_ids)</span><br><span class="line"></span><br><span class="line">    tokenized_inputs[<span class="string">&quot;labels&quot;</span>] = labels</span><br><span class="line">    <span class="keyword">return</span> tokenized_inputs</span><br></pre></td></tr></table></figure>

<p>以上的预处理函数可以处理一个样本，也可以处理多个样本exapmles。如果是处理多个样本，则返回的是多个样本被预处理之后的结果list。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenize_and_align_labels(datasets[<span class="string">&#x27;train&#x27;</span>][:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>


<pre><code>&#123;&#39;input_ids&#39;: [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], &#39;labels&#39;: [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 0, 0, 0, 0, 0, -100], [-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]&#125;
</code></pre>
<p>接下来对数据集datasets里面的所有样本进行预处理，处理的方式是使用map函数，将预处理函数prepare_train_features应用到（map)所有样本上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenized_datasets = datasets.<span class="built_in">map</span>(tokenize_and_align_labels, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h1 id="5-微调预训练模型"><a href="#5-微调预训练模型" class="headerlink" title="5 微调预训练模型"></a>5 微调预训练模型</h1><h2 id="5-1-设置Trainer"><a href="#5-1-设置Trainer" class="headerlink" title="5.1 设置Trainer"></a>5.1 设置Trainer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForTokenClassification, TrainingArguments, Trainer</span><br><span class="line"></span><br><span class="line">model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=<span class="built_in">len</span>(label_list))</span><br><span class="line"><span class="comment"># 使用`AutoModelForTokenClassification` 这个类做seq2seq任务</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同Task06</span></span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">f&quot;test-<span class="subst">&#123;task&#125;</span>&quot;</span>,</span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=batch_size,</span><br><span class="line">    per_device_eval_batch_size=batch_size,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForTokenClassification</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorForTokenClassification(tokenizer)</span><br><span class="line"><span class="comment"># 需要一个数据收集器data collator，将处理好的输入喂给模型</span></span><br></pre></td></tr></table></figure>

<h2 id="5-2-设置评估方法"><a href="#5-2-设置评估方法" class="headerlink" title="5.2 设置评估方法"></a>5.2 设置评估方法</h2><p>使用<a href="https://github.com/chakki-works/seqeval"><code>seqeval</code></a> metric来完成评估。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">metric = load_metric(<span class="string">&quot;seqeval&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>评估的输入是预测和label的list</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">labels = [label_list[i] <span class="keyword">for</span> i <span class="keyword">in</span> example[<span class="string">f&quot;<span class="subst">&#123;task&#125;</span>_tags&quot;</span>]]</span><br><span class="line">metric.compute(predictions=[labels], references=[labels])</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;LOC&#39;: &#123;&#39;f1&#39;: 1.0, &#39;number&#39;: 2, &#39;precision&#39;: 1.0, &#39;recall&#39;: 1.0&#125;,
 &#39;ORG&#39;: &#123;&#39;f1&#39;: 1.0, &#39;number&#39;: 1, &#39;precision&#39;: 1.0, &#39;recall&#39;: 1.0&#125;,
 &#39;PER&#39;: &#123;&#39;f1&#39;: 1.0, &#39;number&#39;: 1, &#39;precision&#39;: 1.0, &#39;recall&#39;: 1.0&#125;,
 &#39;overall_accuracy&#39;: 1.0,
 &#39;overall_f1&#39;: 1.0,
 &#39;overall_precision&#39;: 1.0,
 &#39;overall_recall&#39;: 1.0&#125;
</code></pre>
<p>对模型预测结果做一些后处理：</p>
<ul>
<li>选择预测分类最大概率的下标</li>
<li>将下标转化为label</li>
<li>忽略-100所在地方</li>
</ul>
<h2 id="5-3-将参数设置和评估方法两步整合起来"><a href="#5-3-将参数设置和评估方法两步整合起来" class="headerlink" title="5.3 将参数设置和评估方法两步整合起来"></a>5.3 将参数设置和评估方法两步整合起来</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">p</span>):</span></span><br><span class="line">    predictions, labels = p</span><br><span class="line">    predictions = np.argmax(predictions, axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove ignored index (special tokens)</span></span><br><span class="line">    true_predictions = [</span><br><span class="line">        [label_list[p] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">        <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">    ]</span><br><span class="line">    true_labels = [</span><br><span class="line">        [label_list[l] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">        <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    results = metric.compute(predictions=true_predictions, references=true_labels)</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;precision&quot;</span>: results[<span class="string">&quot;overall_precision&quot;</span>],</span><br><span class="line">        <span class="string">&quot;recall&quot;</span>: results[<span class="string">&quot;overall_recall&quot;</span>],</span><br><span class="line">        <span class="string">&quot;f1&quot;</span>: results[<span class="string">&quot;overall_f1&quot;</span>],</span><br><span class="line">        <span class="string">&quot;accuracy&quot;</span>: results[<span class="string">&quot;overall_accuracy&quot;</span>],</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>


<p>我们计算所有类别总的precision/recall/f1，所以会扔掉单个类别的precision/recall/f1 </p>
<p>将数据/模型/参数传入<code>Trainer</code>即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>调用<code>train</code>方法开始训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>



  <div>
      <style>
          /* Turns off some styling */
          progress {
              /* gets rid of default border in Firefox and Opera. */
              border: none;
              /* Needs to be in here for Safari polyfill so background images work as expected. */
              background-size: auto;
          }
      </style>

<p>  <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress><br>  [2634/2634 01:45, Epoch 3/3]<br>  </div><br>  <table border="1" class="dataframe"></p>
<thead>
  <tr style="text-align: left;">
    <th>Epoch</th>
    <th>Training Loss</th>
    <th>Validation Loss</th>
    <th>Precision</th>
    <th>Recall</th>
    <th>F1</th>
    <th>Accuracy</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>1</td>
    <td>0.237721</td>
    <td>0.068198</td>
    <td>0.903148</td>
    <td>0.921132</td>
    <td>0.912051</td>
    <td>0.979713</td>
  </tr>
  <tr>
    <td>2</td>
    <td>0.053160</td>
    <td>0.059337</td>
    <td>0.927697</td>
    <td>0.932990</td>
    <td>0.930336</td>
    <td>0.983113</td>
  </tr>
  <tr>
    <td>3</td>
    <td>0.029850</td>
    <td>0.059346</td>
    <td>0.929267</td>
    <td>0.939143</td>
    <td>0.934179</td>
    <td>0.984257</td>
  </tr>
</tbody>
</table><p>





<pre><code>TrainOutput(global_step=2634, training_loss=0.08569671253227518)
</code></pre>
<h1 id="6-对模型进行评估"><a href="#6-对模型进行评估" class="headerlink" title="6 对模型进行评估"></a>6 对模型进行评估</h1><h2 id="6-1-对总类别进行评估"><a href="#6-1-对总类别进行评估" class="headerlink" title="6.1 对总类别进行评估"></a>6.1 对总类别进行评估</h2><p>我们可以再次使用<code>evaluate</code>方法评估，可以评估其他数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer.evaluate()</span><br></pre></td></tr></table></figure>



<div>
    <style>
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
    </style>

<p>  <progress value='408' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress><br>  [204/204 00:05]</p>
</div>






<pre><code>&#123;&#39;eval_loss&#39;: 0.05934586375951767,
 &#39;eval_precision&#39;: 0.9292672127518264,
 &#39;eval_recall&#39;: 0.9391430808815304,
 &#39;eval_f1&#39;: 0.9341790463472988,
 &#39;eval_accuracy&#39;: 0.9842565968195466,
 &#39;epoch&#39;: 3.0&#125;
</code></pre>
<h2 id="6-2-对单个类别进行评估"><a href="#6-2-对单个类别进行评估" class="headerlink" title="6.2 对单个类别进行评估"></a>6.2 对单个类别进行评估</h2><p>如果想要得到单个类别的precision/recall/f1，我们直接将结果输入相同的评估函数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">predictions, labels, _ = trainer.predict(tokenized_datasets[<span class="string">&quot;validation&quot;</span>])</span><br><span class="line">predictions = np.argmax(predictions, axis=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove ignored index (special tokens)</span></span><br><span class="line">true_predictions = [</span><br><span class="line">    [label_list[p] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">    <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">]</span><br><span class="line">true_labels = [</span><br><span class="line">    [label_list[l] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">    <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">results = metric.compute(predictions=true_predictions, references=true_labels)</span><br><span class="line">results</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;LOC&#39;: &#123;&#39;precision&#39;: 0.949718574108818,
  &#39;recall&#39;: 0.966768525592055,
  &#39;f1&#39;: 0.9581677077418134,
  &#39;number&#39;: 2618&#125;,
 &#39;MISC&#39;: &#123;&#39;precision&#39;: 0.8132387706855791,
  &#39;recall&#39;: 0.8383428107229894,
  &#39;f1&#39;: 0.8255999999999999,
  &#39;number&#39;: 1231&#125;,
 &#39;ORG&#39;: &#123;&#39;precision&#39;: 0.9055232558139535,
  &#39;recall&#39;: 0.9090466926070039,
  &#39;f1&#39;: 0.9072815533980583,
  &#39;number&#39;: 2056&#125;,
 &#39;PER&#39;: &#123;&#39;precision&#39;: 0.9759552042160737,
  &#39;recall&#39;: 0.9765985497692815,
  &#39;f1&#39;: 0.9762767710049424,
  &#39;number&#39;: 3034&#125;,
 &#39;overall_precision&#39;: 0.9292672127518264,
 &#39;overall_recall&#39;: 0.9391430808815304,
 &#39;overall_f1&#39;: 0.9341790463472988,
 &#39;overall_accuracy&#39;: 0.9842565968195466&#125;
</code></pre>
<h1 id="7-将训练好的自己的模型上传到Model-Hub"><a href="#7-将训练好的自己的模型上传到Model-Hub" class="headerlink" title="7 将训练好的自己的模型上传到Model-Hub"></a>7 将训练好的自己的模型上传到Model-Hub</h1><p>最后别忘了，查看如何上传模型 ，上传模型到](<a href="https://huggingface.co/transformers/model_sharing.html">https://huggingface.co/transformers/model_sharing.html</a>) 到<a href="https://huggingface.co/models">🤗 Model Hub</a>。随后您就可以像这个notebook一开始一样，直接用模型名字就能使用您自己上传的模型啦。</p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[Transformers解决文本分类任务、超参搜索]]></title>
      <url>/2021/08/27/Task06-Transformers%E8%A7%A3%E5%86%B3%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E3%80%81%E8%B6%85%E5%8F%82%E6%90%9C%E7%B4%A2/</url>
      <content type="html"><![CDATA[<h1 id="Task06-Transformers解决文本分类任务、超参搜索"><a href="#Task06-Transformers解决文本分类任务、超参搜索" class="headerlink" title="Task06-Transformers解决文本分类任务、超参搜索"></a>Task06-Transformers解决文本分类任务、超参搜索</h1><ul>
<li>Datawhale 开源学习地址：<a href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a></li>
</ul>
<h2 id="0-总结"><a href="#0-总结" class="headerlink" title="0 总结"></a>0 总结</h2><ul>
<li>将BERT应用于文本分类</li>
<li>数据预处理（tokenize）</li>
<li>训练参数设定/模型评估</li>
<li>超参数自动调优的两个库</li>
<li>上传自己训练好的模型到ModelHub，多终端共享使用</li>
<li>如果使用conda安装ray[tune]包时，请下载ray-tune依赖包<br>命令：conda install ray-tune -c conda-forge</li>
</ul>
<h2 id="1-文本分类任务简介"><a href="#1-文本分类任务简介" class="headerlink" title="1 文本分类任务简介"></a>1 文本分类任务简介</h2><ul>
<li>使用Transformers代码库中的模型来解决文本分类任务，任务来源于<a href="https://gluebenchmark.com/">GLUE Benchmark</a></li>
<li>GLUE榜单的9个级别的分类任务：<ol>
<li>CoLA (Corpus of Linguistic Acceptability)：鉴别一个句子是否语法正确.</li>
<li>MNLI (Multi-Genre Natural Language Inference)：给定一个假设，判断另一个句子与该假设的关系：entails、contradicts、unrelated。</li>
<li>MRPC (Microsoft Research Paraphrase Corpus)：判断两个句子是否互为paraphrases</li>
<li>QNLI (Question-answering Natural Language Inference)：判断第2句是否包含第1句问题的答案</li>
<li>QQP (Quora Question Pairs2)：判断两个问句是否语义相同</li>
<li>RTE (Recognizing Textual Entailment)：判断一个句子是否与假设成entail关系</li>
<li>SST-2 (Stanford Sentiment Treebank)：判断一个句子的情感正负向</li>
<li>STS-B (Semantic Textual Similarity Benchmark)：判断两个句子的相似性（分数为1-5分）</li>
<li>WNLI (Winograd Natural Language Inference)：判断带有匿名代词的句子中，是否存在能够替换该代词的子句</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GLUE_TASKS = [<span class="string">&quot;cola&quot;</span>, <span class="string">&quot;mnli&quot;</span>, <span class="string">&quot;mnli-mm&quot;</span>, <span class="string">&quot;mrpc&quot;</span>,</span><br><span class="line">              <span class="string">&quot;qnli&quot;</span>, <span class="string">&quot;qqp&quot;</span>, <span class="string">&quot;rte&quot;</span>, <span class="string">&quot;sst2&quot;</span>, <span class="string">&quot;stsb&quot;</span>, <span class="string">&quot;wnli&quot;</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">task = <span class="string">&quot;cola&quot;</span></span><br><span class="line"><span class="comment"># 任务为CoLA任务</span></span><br><span class="line">model_checkpoint = <span class="string">&quot;distilbert-base-uncased&quot;</span></span><br><span class="line"><span class="comment"># 使用BERT蒸馏模型</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 根据GPU调整batch_size大小，避免显存溢出</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="2-加载数据"><a href="#2-加载数据" class="headerlink" title="2 加载数据"></a>2 加载数据</h2><h3 id="2-1-加载数据和对应的评测方式"><a href="#2-1-加载数据和对应的评测方式" class="headerlink" title="2.1 加载数据和对应的评测方式"></a>2.1 加载数据和对应的评测方式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, load_metric</span><br><span class="line"></span><br><span class="line">actual_task = <span class="string">&quot;mnli&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;mnli-mm&quot;</span> <span class="keyword">else</span> task</span><br><span class="line">dataset = load_dataset(<span class="string">&quot;glue&quot;</span>, actual_task)</span><br><span class="line">metric = load_metric(<span class="string">&#x27;glue&#x27;</span>, actual_task</span><br><span class="line"><span class="comment"># MNLI(The Multi-Genre Natural Language Inference Corpus, 多类型自然语言推理数据库)，自然语言推断任务，是通过众包方式对句子对进行文本蕴含标注的集合。</span></span><br><span class="line"><span class="comment"># 任务：句子对，一个前提，一个是假设。前提和假设的关系有三种情况：蕴含（entailment），矛盾（contradiction），中立（neutral）。句子对三分类问题。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-2-查看数据"><a href="#2-2-查看数据" class="headerlink" title="2.2 查看数据"></a>2.2 查看数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset</span><br></pre></td></tr></table></figure>

<pre><code>DatasetDict(&#123;
    train: Dataset(&#123;
        features: [&#39;sentence&#39;, &#39;label&#39;, &#39;idx&#39;],
        num_rows: 8551
    &#125;)
    validation: Dataset(&#123;
        features: [&#39;sentence&#39;, &#39;label&#39;, &#39;idx&#39;],
        num_rows: 1043
    &#125;)
    test: Dataset(&#123;
        features: [&#39;sentence&#39;, &#39;label&#39;, &#39;idx&#39;],
        num_rows: 1063
    &#125;)
&#125;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 给定一个数据切分的key（train、validation或者test）和下标即可查看数据。</span></span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;sentence&#39;: &quot;Our friends won&#39;t buy this analysis, let alone the next one we propose.&quot;,
    &#39;label&#39;: 1,
    &#39;idx&#39;: 0&#125;
</code></pre>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import datasets</span><br><span class="line">import random</span><br><span class="line">import pandas as pd</span><br><span class="line">from IPython.display import display, HTML</span><br><span class="line"></span><br><span class="line">def show_random_elements(dataset, num_examples&#x3D;10):</span><br><span class="line">    assert num_examples &lt;&#x3D; len(dataset), &quot;Can&#39;t pick more elements than there are in the dataset.&quot;</span><br><span class="line">    picks &#x3D; []</span><br><span class="line">    for _ in range(num_examples):</span><br><span class="line">        pick &#x3D; random.randint(0, len(dataset)-1)</span><br><span class="line">        while pick in picks:</span><br><span class="line">            pick &#x3D; random.randint(0, len(dataset)-1)</span><br><span class="line">        picks.append(pick)</span><br><span class="line">    </span><br><span class="line">    df &#x3D; pd.DataFrame(dataset[picks])</span><br><span class="line">    for column, typ in dataset.features.items():</span><br><span class="line">        if isinstance(typ, datasets.ClassLabel):</span><br><span class="line">            df[column] &#x3D; df[column].transform(lambda i: typ.names[i])</span><br><span class="line">    display(HTML(df.to_html()))</span><br></pre></td></tr></table></figure>


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentence</th>
      <th>label</th>
      <th>idx</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The more I talk to Joe, the less about linguistics I am inclined to think Sally has taught him to appreciate.</td>
      <td>acceptable</td>
      <td>196</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Have in our class the kids arrived safely?</td>
      <td>unacceptable</td>
      <td>3748</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I gave Mary a book.</td>
      <td>acceptable</td>
      <td>5302</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Every student, who attended the party, had a good time.</td>
      <td>unacceptable</td>
      <td>4944</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Bill pounded the metal fiat.</td>
      <td>acceptable</td>
      <td>2178</td>
    </tr>
    <tr>
      <th>5</th>
      <td>It bit me on the leg.</td>
      <td>acceptable</td>
      <td>5908</td>
    </tr>
    <tr>
      <th>6</th>
      <td>The boys were made a good mother by Aunt Mary.</td>
      <td>unacceptable</td>
      <td>736</td>
    </tr>
    <tr>
      <th>7</th>
      <td>More of a man is here.</td>
      <td>unacceptable</td>
      <td>5403</td>
    </tr>
    <tr>
      <th>8</th>
      <td>My mother baked me a birthday cake.</td>
      <td>acceptable</td>
      <td>3761</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Gregory appears to have wanted to be loyal to the company.</td>
      <td>acceptable</td>
      <td>4334</td>
    </tr>
  </tbody>
</table>

<h3 id="2-3-查看评测方法及常见分类"><a href="#2-3-查看评测方法及常见分类" class="headerlink" title="2.3 查看评测方法及常见分类"></a>2.3 查看评测方法及常见分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">metric</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>metic是<a href="https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric"><code>datasets.Metric</code></a>的一个实例:</li>
</ul>
<pre><code>Metric(name: &quot;glue&quot;, features: &#123;&#39;predictions&#39;: Value(dtype=&#39;int64&#39;, id=None), &#39;references&#39;: Value(dtype=&#39;int64&#39;, id=None)&#125;, usage: &quot;&quot;&quot;
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    &quot;accuracy&quot;: Accuracy
    &quot;f1&quot;: F1 score
    &quot;pearson&quot;: Pearson Correlation
    &quot;spearmanr&quot;: Spearman Correlation
    &quot;matthews_correlation&quot;: Matthew Correlation
Examples:

    &gt;&gt;&gt; glue_metric = datasets.load_metric(&#39;glue&#39;, &#39;sst2&#39;)  # &#39;sst2&#39; or any of [&quot;mnli&quot;, &quot;mnli_mismatched&quot;, &quot;mnli_matched&quot;, &quot;qnli&quot;, &quot;rte&quot;, &quot;wnli&quot;, &quot;hans&quot;]
    &gt;&gt;&gt; references = [0, 1]
    &gt;&gt;&gt; predictions = [0, 1]
    &gt;&gt;&gt; results = glue_metric.compute(predictions=predictions, references=references)
    &gt;&gt;&gt; print(results)
    &#123;&#39;accuracy&#39;: 1.0&#125;

    &gt;&gt;&gt; glue_metric = datasets.load_metric(&#39;glue&#39;, &#39;mrpc&#39;)  # &#39;mrpc&#39; or &#39;qqp&#39;
    &gt;&gt;&gt; references = [0, 1]
    &gt;&gt;&gt; predictions = [0, 1]
    &gt;&gt;&gt; results = glue_metric.compute(predictions=predictions, references=references)
    &gt;&gt;&gt; print(results)
    &#123;&#39;accuracy&#39;: 1.0, &#39;f1&#39;: 1.0&#125;

    &gt;&gt;&gt; glue_metric = datasets.load_metric(&#39;glue&#39;, &#39;stsb&#39;)
    &gt;&gt;&gt; references = [0., 1., 2., 3., 4., 5.]
    &gt;&gt;&gt; predictions = [0., 1., 2., 3., 4., 5.]
    &gt;&gt;&gt; results = glue_metric.compute(predictions=predictions, references=references)
    &gt;&gt;&gt; print(&#123;&quot;pearson&quot;: round(results[&quot;pearson&quot;], 2), &quot;spearmanr&quot;: round(results[&quot;spearmanr&quot;], 2)&#125;)
    &#123;&#39;pearson&#39;: 1.0, &#39;spearmanr&#39;: 1.0&#125;

    &gt;&gt;&gt; glue_metric = datasets.load_metric(&#39;glue&#39;, &#39;cola&#39;)
    &gt;&gt;&gt; references = [0, 1]
    &gt;&gt;&gt; predictions = [0, 1]
    &gt;&gt;&gt; results = glue_metric.compute(predictions=predictions, references=references)
    &gt;&gt;&gt; print(results)
    &#123;&#39;matthews_correlation&#39;: 1.0&#125;
&quot;&quot;&quot;, stored examples: 0)
</code></pre>
<p>直接调用metric的<code>compute</code>方法，传入<code>labels</code>和<code>predictions</code>即可得到metric的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">fake_preds = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">64</span>,))</span><br><span class="line">fake_labels = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, size=(<span class="number">64</span>,))</span><br><span class="line">metric.compute(predictions=fake_preds, references=fake_labels)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;matthews_correlation&#39;: 0.1513518081969605&#125;
</code></pre>
<p>每一个文本分类任务所对应的metic有所不同，具体如下:</p>
<table>
<thead>
<tr>
<th align="center">任务</th>
<th align="center">评测方法</th>
</tr>
</thead>
<tbody><tr>
<td align="center">CoLA</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">Matthews Correlation Coefficient</a></td>
</tr>
<tr>
<td align="center">MNLI</td>
<td align="center">Accuracy</td>
</tr>
<tr>
<td align="center">MRPC</td>
<td align="center">Accuracy and <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a></td>
</tr>
<tr>
<td align="center">QNLI</td>
<td align="center">Accuracy</td>
</tr>
<tr>
<td align="center">QQP</td>
<td align="center">Accuracy and F1 score</td>
</tr>
<tr>
<td align="center">RTE</td>
<td align="center">Accuracy</td>
</tr>
<tr>
<td align="center">SST-2</td>
<td align="center">Accuracy</td>
</tr>
<tr>
<td align="center">STS-B</td>
<td align="center"><a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson Correlation Coefficient</a> and <a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient">Spearman’s_Rank_Correlation_Coefficient</a></td>
</tr>
<tr>
<td align="center">WNLI</td>
<td align="center">Accuracy</td>
</tr>
</tbody></table>
<p>所以一定要将metric和任务对齐</p>
<h2 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3 数据预处理"></a>3 数据预处理</h2><h3 id="3-1-数据预处理流程"><a href="#3-1-数据预处理流程" class="headerlink" title="3.1 数据预处理流程"></a>3.1 数据预处理流程</h3><p>预处理目的：数据只有经过预处理后，才能作为向量形式输入model。</p>
<ul>
<li>流程：<ol>
<li>对输入数据进行tokenize，得到tokens</li>
<li>将tokens转化为预训练模型中需要对应的token ID</li>
<li>将token ID转化为模型需要的输入格式</li>
</ol>
</li>
</ul>
<p>为了达到数据预处理的目的，我们使用<code>AutoTokenizer.from_pretrained</code>方法实例化我们的tokenizer，这样可以确保：（？？？？）</p>
<ul>
<li>我们得到一个与预训练模型一一对应的tokenizer。</li>
<li>使用指定的模型checkpoint对应的tokenizer的时候，我们也下载了模型需要的词表库vocabulary，准确来说是tokens vocabulary。</li>
</ul>
<p>这个被下载的tokens vocabulary会被缓存起来，从而再次使用的时候不会重新下载。</p>
<h3 id="3-2-构建模型对应的tokenizer"><a href="#3-2-构建模型对应的tokenizer" class="headerlink" title="3.2 构建模型对应的tokenizer"></a>3.2 构建模型对应的tokenizer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">    </span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>注意：</p>
<ul>
<li><code>use_fast=True</code>要求tokenizer必须是transformers.PreTrainedTokenizerFast类型，</li>
<li>因为我们在预处理的时候需要用到fast tokenizer的一些特殊特性（比如多线程快速tokenizer）。</li>
<li>如果对应的模型没有fast tokenizer，去掉这个选项即可。</li>
<li>几乎所有模型对应的tokenizer都有对应的fast tokenizer。我们可以在<a href="https://huggingface.co/transformers/index.html#bigtable">模型tokenizer对应表</a>里查看所有预训练模型对应的tokenizer所拥有的特点。</li>
</ul>
<p>tokenizer既可以对单个文本进行预处理，也可以对一对文本进行预处理，tokenizer预处理后得到的数据满足预训练模型输入格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer(<span class="string">&quot;Hello, this one sentence!&quot;</span>, <span class="string">&quot;And this sentence goes with it.&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;input_ids&#39;: [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;
</code></pre>
<p>取决于我们选择的预训练模型，我们将会看到tokenizer有不同的返回，tokenizer和预训练模型是一一对应的，更多信息可以在<a href="https://huggingface.co/transformers/preprocessing.html">这里</a>进行学习。</p>
<h3 id="3-3-对数据集datasets所有样本进行预处理"><a href="#3-3-对数据集datasets所有样本进行预处理" class="headerlink" title="3.3  对数据集datasets所有样本进行预处理"></a>3.3  对数据集datasets所有样本进行预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义如下dict，用于对数据格式进行检查</span></span><br><span class="line">task_to_keys = &#123;</span><br><span class="line">    <span class="string">&quot;cola&quot;</span>: (<span class="string">&quot;sentence&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    <span class="string">&quot;mnli&quot;</span>: (<span class="string">&quot;premise&quot;</span>, <span class="string">&quot;hypothesis&quot;</span>),</span><br><span class="line">    <span class="string">&quot;mnli-mm&quot;</span>: (<span class="string">&quot;premise&quot;</span>, <span class="string">&quot;hypothesis&quot;</span>),</span><br><span class="line">    <span class="string">&quot;mrpc&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;qnli&quot;</span>: (<span class="string">&quot;question&quot;</span>, <span class="string">&quot;sentence&quot;</span>),</span><br><span class="line">    <span class="string">&quot;qqp&quot;</span>: (<span class="string">&quot;question1&quot;</span>, <span class="string">&quot;question2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;rte&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;sst2&quot;</span>: (<span class="string">&quot;sentence&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">    <span class="string">&quot;stsb&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">    <span class="string">&quot;wnli&quot;</span>: (<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对数据格式进行检查</span></span><br><span class="line">sentence1_key, sentence2_key = task_to_keys[task]</span><br><span class="line"><span class="keyword">if</span> sentence2_key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence: <span class="subst">&#123;dataset[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>][sentence1_key]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence 1: <span class="subst">&#123;dataset[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>][sentence1_key]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sentence 2: <span class="subst">&#123;dataset[<span class="string">&#x27;train&#x27;</span>][<span class="number">0</span>][sentence2_key]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Sentence: Our friends won&#39;t buy this analysis, let alone the next one we propose.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由上面思路构造预处理函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">if</span> sentence2_key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> tokenizer(examples[sentence1_key], truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 举例</span></span><br><span class="line"><span class="comment"># 预处理函数可以处理单个样本，也可以对多个样本进行处理。如果输入是多个样本，那么返回的是一个list：</span></span><br><span class="line">preprocess_function(dataset[<span class="string">&#x27;train&#x27;</span>][:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;input_ids&#39;: [[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 1998, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 2030, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 1996, 2062, 2057, 2817, 16025, 1010, 1996, 13675, 16103, 2121, 2027, 2131, 1012, 102], [101, 2154, 2011, 2154, 1996, 8866, 2024, 2893, 14163, 8024, 3771, 1012, 102]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对datasets里所有样本进行预处理</span></span><br><span class="line">encoded_dataset = dataset.<span class="built_in">map</span>(preprocess_function, batched=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 使用map：将预处理函数prepare_train_features应用到（map)所有样本上。</span></span><br></pre></td></tr></table></figure>

<p>另外，返回的结果会自动被缓存，避免下次处理的时候重新计算（但是也要注意，如果输入有改动，可能会被缓存影响！）。<br>datasets库函数会对输入的参数进行检测，判断是否有变化，如果没有变化就使用缓存数据，如果有变化就重新处理。但如果输入参数不变，只是想改变输入数据的时候，最好清理调这个缓存。清理的方式是使用<code>load_from_cache_file=False</code>参数。<br>另外，上面使用到的<code>batched=True</code>这个参数是tokenizer的特点，这会使用多线程同时并行对输入进行处理。</p>
<h2 id="4-微调预训练模型"><a href="#4-微调预训练模型" class="headerlink" title="4 微调预训练模型"></a>4 微调预训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer</span><br><span class="line"><span class="comment"># 做seq2seq任务，需要一个能解决这个任务的模型类。这里使用`AutoModelForSequenceClassification`类 。</span></span><br><span class="line">num_labels = <span class="number">3</span> <span class="keyword">if</span> task.startswith(<span class="string">&quot;mnli&quot;</span>) <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">if</span> task==<span class="string">&quot;stsb&quot;</span> <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)</span><br><span class="line"><span class="comment"># `from_pretrained`方法可以下载并加载模型，同时也会对模型进行缓存。</span></span><br></pre></td></tr></table></figure>
<p>需要注意的是：STS-B是一个回归问题，MNLI是一个3分类问题。</p>
<p>由于我们微调的任务是文本分类任务，而我们加载的是预训练的语言模型，所以会提示我们加载模型的时候扔掉了一些不匹配的神经网络参数（比如：预训练语言模型的神经网络head被扔掉了，同时随机初始化了文本分类的神经网络head）。？？？？</p>
<h3 id="4-1-训练参数"><a href="#4-1-训练参数" class="headerlink" title="4.1 训练参数"></a>4.1 训练参数</h3><p>训练的设定/参数 <a href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments"><code>TrainingArguments</code></a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">metric_name = <span class="string">&quot;pearson&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;stsb&quot;</span> <span class="keyword">else</span> <span class="string">&quot;matthews_correlation&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;cola&quot;</span> <span class="keyword">else</span> <span class="string">&quot;accuracy&quot;</span></span><br><span class="line"></span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;test-glue&quot;</span>,</span><br><span class="line">    evaluation_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    <span class="comment"># 每训练一个epoch做一次验证评估</span></span><br><span class="line">    save_strategy = <span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=batch_size,</span><br><span class="line">    per_device_eval_batch_size=batch_size,</span><br><span class="line">    num_train_epochs=<span class="number">5</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    load_best_model_at_end=<span class="literal">True</span>,</span><br><span class="line">    metric_for_best_model=metric_name,</span><br><span class="line">    <span class="comment"># 包含了能够定义训练过程的所有属性</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="4-2-评估方法"><a href="#4-2-评估方法" class="headerlink" title="4.2 评估方法"></a>4.2 评估方法</h3><p>不同的任务需要不同的评价指标，定义一个函数根据名字匹配评价方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_pred</span>):</span></span><br><span class="line">    predictions, labels = eval_pred</span><br><span class="line">    <span class="keyword">if</span> task != <span class="string">&quot;stsb&quot;</span>:</span><br><span class="line">        predictions = np.argmax(predictions, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        predictions = predictions[:, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure>

<h3 id="4-3-将上述定义好的参数及方法全部传入Trainer"><a href="#4-3-将上述定义好的参数及方法全部传入Trainer" class="headerlink" title="4.3 将上述定义好的参数及方法全部传入Trainer"></a>4.3 将上述定义好的参数及方法全部传入Trainer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">validation_key = <span class="string">&quot;validation_mismatched&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;mnli-mm&quot;</span> <span class="keyword">else</span> <span class="string">&quot;validation_matched&quot;</span> <span class="keyword">if</span> task == <span class="string">&quot;mnli&quot;</span> <span class="keyword">else</span> <span class="string">&quot;validation&quot;</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    args,</span><br><span class="line">    train_dataset=encoded_dataset[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=encoded_dataset[validation_key],</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="4-4-训练及评估"><a href="#4-4-训练及评估" class="headerlink" title="4.4 训练及评估"></a>4.4 训练及评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>

<pre><code>TrainOutput(global_step=2675, training_loss=0.2717150308484229, metrics=&#123;&#39;train_runtime&#39;: 100.5668, &#39;train_samples_per_second&#39;: 425.14, &#39;train_steps_per_second&#39;: 26.599, &#39;total_flos&#39;: 229537542078168.0, &#39;train_loss&#39;: 0.2717150308484229, &#39;epoch&#39;: 5.0&#125;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer.evaluate()</span><br></pre></td></tr></table></figure>




<div>

<p>  <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress><br>  [66/66 00:00]</p>
</div>






<pre><code>&#123;&#39;eval_loss&#39;: 0.8624260425567627,
 &#39;eval_matthews_correlation&#39;: 0.519563286537562,
 &#39;eval_runtime&#39;: 0.6501,
 &#39;eval_samples_per_second&#39;: 1604.31,
 &#39;eval_steps_per_second&#39;: 101.519,
 &#39;epoch&#39;: 5.0&#125;
</code></pre>
<h2 id="5-超参数搜索"><a href="#5-超参数搜索" class="headerlink" title="5. 超参数搜索"></a>5. 超参数搜索</h2><p><code>Trainer</code>同样支持超参搜索，使用<a href="https://optuna.org/">optuna</a> or <a href="https://docs.ray.io/en/latest/tune/">Ray Tune</a>代码库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">! pip install optuna</span><br><span class="line">! pip install ray[tune]</span><br><span class="line"><span class="comment"># 安装两个超参数自动调优库</span></span><br></pre></td></tr></table></figure>

<h3 id="5-1-设置初始化模型"><a href="#5-1-设置初始化模型" class="headerlink" title="5.1 设置初始化模型"></a>5.1 设置初始化模型</h3><p>超参搜索时，<code>Trainer</code>将会返回多个训练好的模型，所以需要传入一个定义好的模型从而让<code>Trainer</code>可以不断重新初始化该传入的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_init</span>():</span></span><br><span class="line">    <span class="keyword">return</span> AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model_init=model_init,</span><br><span class="line">    args=args,</span><br><span class="line">    train_dataset=encoded_dataset[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=encoded_dataset[validation_key],</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    compute_metrics=compute_metrics</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="5-2-使用超参调优库"><a href="#5-2-使用超参调优库" class="headerlink" title="5.2 使用超参调优库"></a>5.2 使用超参调优库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_run = trainer.hyperparameter_search(n_trials=<span class="number">10</span>, direction=<span class="string">&quot;maximize&quot;</span>)</span><br><span class="line"><span class="comment"># `hyperparameter_search`会返回效果最好的模型相关的参数</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_run</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n, v <span class="keyword">in</span> best_run.hyperparameters.items():</span><br><span class="line">    <span class="built_in">setattr</span>(trainer.args, n, v)</span><br><span class="line"><span class="comment"># 将最好的超参数应用于`Trainner`</span></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>

<h2 id="6-上传模型到model-hub"><a href="#6-上传模型到model-hub" class="headerlink" title="6 上传模型到model-hub"></a>6 上传模型到model-hub</h2><p>类似于docker-hub，可以上传自己配置好的模型</p>
<p><a href="https://huggingface.co/transformers/model_sharing.html">上传模型到</a> 到<a href="https://huggingface.co/models"> Model Hub</a></p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title></title>
      <url>/2021/08/24/NLP-Transformer-task03/</url>
      <content type="html"><![CDATA[<p>author：TongMing<br>Datawhale 开源学习地址：<a href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a></p>
<h2 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h2><p>预训练：根据MASK的方式进行自监督的学习，<br>微调：结合监督信息进行半监督的学习</p>
<p>BERT是transformer的encoder部分<br>ELMo可以表示不同语境下的“多义词”</p>
<h2 id="图解BERT（精华）"><a href="#图解BERT（精华）" class="headerlink" title="图解BERT（精华）"></a>图解BERT（精华）</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>BERT 开发的两个步骤：第 1 步，你可以下载预训练好的模型（这个模型是在无标注的数据上训练的）。然后在第 2 步只需要关心模型微调即可。</p>
<h3 id="句子分类"><a href="#句子分类" class="headerlink" title="句子分类"></a>句子分类</h3><p>使用 BERT 最直接的方法就是对一个句子进行分类。这个模型如下所示：</p>
<p><img src="3-bert-cls.png" alt="BERT句子分类">图：BERT句子分类</p>
<p>为了训练这样一个模型，你主要需要训练分类器（上图中的 Classifier），在训练过程中 几乎不用改动BERT模型。这个训练过程称为微调，它起源于Semi-supervised Sequence Learning 和 ULMFiT。</p>
<p>其他例子还包括：预测语义分析和断言</p>
<h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p>论文里介绍了两种不同模型大小的 BERT：</p>
<ul>
<li>BERT BASE - 与 OpenAI 的 Transformer 大小相当，以便比较性能</li>
<li>BERT LARGE - 一个非常巨大的模型，它取得了最先进的结果</li>
</ul>
<p>2 种不同大小规模的 BERT 模型都有大量的 Encoder 层（论文里把这些层称为 Transformer Blocks）- BASE 版本由 12 层 Encoder，Large 版本有 20 层 Encoder。同时，这些 BERT 模型也有更大的前馈神经网络（分别有 768 个和 1024 个隐藏层单元）和更多的 attention heads（分别有 12 个和 16 个），超过了原始 Transformer 论文中的默认配置参数（原论文中有 6 个 Encoder 层， 512 个隐藏层单元和 8 个 attention heads）。</p>
<h3 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h3><p><img src="3-bert-input.png" alt="模型输入">图：模型输入</p>
<p>第一个输入的 token 是特殊的 [CLS]，它 的含义是分类（class的缩写）。</p>
<p>就像 Transformer 中普通的 Encoder 一样，BERT 将一串单词作为输入，这些单词在 Encoder 的栈中不断向上流动。每一层都会经过 Self Attention 层，并通过一个前馈神经网络，然后将结果传给下一个 Encoder。</p>
<p><img src="3-bert-encoder.webp" alt="BERT encoder">图：BERT encoder</p>
<p>在模型架构方面，到目前为止，和 Transformer 是相同的（除了模型大小，因为这是我们可以改变的参数）。我们会在下面看到，BERT 和 Transformer 在模型的输出上有一些不同。</p>
<h3 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h3><p>每个位置输出一个大小为 hidden_size（在 BERT Base 中是 768）的向量。对于上面提到的句子分类的例子，我们只关注第一个位置的输出（输入是 [CLS] 的那个位置）。</p>
<p><img src="3-bert-output.png" alt="BERT output">图：BERT output</p>
<p>这个输出的向量现在可以作为后面分类器的输入。论文里用单层神经网络作为分类器，取得了很好的效果。</p>
<p><img src="3-bert-clss.webp" alt="BERT 接分类器">图：BERT 接分类器</p>
<p>如果你有更多标签（例如你是一个电子邮件服务，需要将邮件标记为 “垃圾邮件”、“非垃圾邮件”、“社交”、“推广”），你只需要调整分类器的神经网络，增加输出的神经元个数，然后经过 softmax 即可。</p>
<h3 id="与卷积神经网络进行对比"><a href="#与卷积神经网络进行对比" class="headerlink" title="与卷积神经网络进行对比"></a>与卷积神经网络进行对比</h3><p>对于那些有计算机视觉背景的人来说，这个向量传递过程，会让人联想到 VGGNet 等网络的卷积部分，和网络最后的全连接分类部分之间的过程。</p>
<p><img src="3-cnn.png" alt="CNN">图：CNN</p>
<h3 id="词嵌入（Embedding）的新时代"><a href="#词嵌入（Embedding）的新时代" class="headerlink" title="词嵌入（Embedding）的新时代"></a>词嵌入（Embedding）的新时代</h3><p>上面提到的这些新发展带来了文本编码方式的新转变。到目前为止，词嵌入一直是 NLP 模型处理语言的主要表示方法。像 Word2Vec 和 Glove 这样的方法已经被广泛应用于此类任务。在我们讨论新的方法之前，让我们回顾一下它们是如何应用的。</p>
<h4 id="回顾词嵌入"><a href="#回顾词嵌入" class="headerlink" title="回顾词嵌入"></a>回顾词嵌入</h4><p>单词不能直接输入机器学习模型，而需要某种数值表示形式，以便模型能够在计算中使用。通过 Word2Vec，我们可以使用一个向量（一组数字）来恰当地表示单词，并捕捉单词的语义以及单词和单词之间的关系（例如，判断单词是否相似或者相反，或者像 “Stockholm” 和 “Sweden” 这样的一对词，与 “Cairo” 和 “Egypt”这一对词，是否有同样的关系）以及句法、语法关系（例如，”had” 和 “has” 之间的关系与 “was” 和 “is” 之间的关系相同）。</p>
<p>人们很快意识到，相比于在小规模数据集上和模型一起训练词嵌入，更好的一种做法是，在大规模文本数据上预训练好词嵌入，然后拿来使用。因此，我们可以下载由 Word2Vec 和 GloVe 预训练好的单词列表，及其词嵌入。下面是单词 “stick” 的 Glove 词嵌入向量的例子（词嵌入向量长度是 200）。</p>
<p><img src="3-wordvector.webp" alt="wrod vector">图： wrod vector</p>
<p>单词 “stick” 的 Glove 词嵌入 - 一个由200个浮点数组成的向量（四舍五入到小数点后两位）。</p>
<h4 id="语境问题"><a href="#语境问题" class="headerlink" title="语境问题"></a>语境问题</h4><p>如果我们使用 Glove 的词嵌入表示方法，那么不管上下文是什么，单词 “stick” 都只表示为同一个向量。一些研究人员指出，像 “stick” 这样的词有多种含义。为什么不能根据它使用的上下文来学习对应的词嵌入呢？这样既能捕捉单词的语义信息，又能捕捉上下文的语义信息。于是，语境化的词嵌入模型应运而生。<br><img src="3-elmo.webp" alt="ELMO">图：ELMO<br>语境化的词嵌入，可以根据单词在句子语境中的含义，赋予不同的词嵌入。你可以查看这个视频 RIP Robin Williams（<a href="https://zhuanlan.zhihu.com/RIP">https://zhuanlan.zhihu.com/RIP</a> Robin Williams）</p>
<p>ELMo 没有对每个单词使用固定的词嵌入，而是在为每个词分配词嵌入之前，查看整个句子，融合上下文信息。它使用在特定任务上经过训练的双向 LSTM 来创建这些词嵌入。</p>
<p>ELMo 在语境化的预训练这条道路上迈出了重要的一步。ELMo LSTM 会在一个大规模的数据集上进行训练，然后我们可以将它作为其他语言处理模型的一个部分，来处理自然语言任务。</p>
<p>那么 ELMo 的秘密是什么呢？</p>
<p>ELMo 通过训练，预测单词序列中的下一个词，从而获得了语言理解能力，这项任务被称为语言建模。要实现 ELMo 很方便，因为我们有大量文本数据，模型可以从这些数据中学习，而不需要额外的标签。</p>
<p><img src="3-elmo-pre.webp" alt="ELMO 训练">图： ELMO 训练</p>
<p>ELMo 预训练过程的其中一个步骤：以 “Let’s stick to” 作为输入，预测下一个最有可能的单词。这是一个语言建模任务。当我们在大规模数据集上训练时，模型开始学习语言的模式。例如，在 “hang” 这样的词之后，模型将会赋予 “out” 更高的概率（因为 “hang out” 是一个词组），而不是 “camera”。</p>
<p>在上图中，我们可以看到 ELMo 头部上方展示了 LSTM 的每一步的隐藏层状态向量。在这个预训练过程完成后，这些隐藏层状态在词嵌入过程中派上用场。</p>
<p><img src="3-elmo-pre1.png" alt="ELMO 训练 stick">图：ELMO 训练</p>
<p>ELMo 通过将隐藏层状态（以及初始化的词嵌入）以某种方式（向量拼接之后加权求和）结合在一起，实现了带有语境化的词嵌入。</p>
<p><img src="3-elmo-pre2.webp" alt="ELMO 训练 stick">图：ELMO 训练</p>
<p>ULM-FiT：NLP 领域的迁移学习<br>ULM-FiT 提出了一些方法来有效地利用模型在预训练期间学习到的东西 - 这些东西不仅仅是词嵌入，还有语境化的词嵌入。ULM-FiT 提出了一个语言模型和一套流程，可以有效地为各种任务微调这个语言模型。</p>
<p>现在，NLP 可能终于找到了好的方法，可以像计算机视觉那样进行迁移学习了。</p>
<h3 id="Transformer：超越-LSTM"><a href="#Transformer：超越-LSTM" class="headerlink" title="Transformer：超越 LSTM"></a>Transformer：超越 LSTM</h3><p>Transformer 论文和代码的发布，以及它在机器翻译等任务上取得的成果，开始让人们认为它是 LSTM 的替代品。这是因为 Transformer 可以比 LSTM 更好地处理长期依赖。</p>
<p>Transformer 的 Encoder-Decoder 结构使得它非常适合机器翻译。但你怎么才能用它来做文本分类呢？你怎么才能使用它来预训练一个语言模型，并能够在其他任务上进行微调（下游任务是指那些能够利用预训练模型的监督学习任务）？</p>
<h3 id="OpenAI-Transformer：预训练一个-Transformer-Decoder-来进行语言建模"><a href="#OpenAI-Transformer：预训练一个-Transformer-Decoder-来进行语言建模" class="headerlink" title="OpenAI Transformer：预训练一个 Transformer Decoder 来进行语言建模"></a>OpenAI Transformer：预训练一个 Transformer Decoder 来进行语言建模</h3><p>事实证明，我们不需要一个完整的 Transformer 来进行迁移学习和微调。我们只需要 Transformer 的 Decoder 就可以了。Decoder 是一个很好的选择，用它来做语言建模（预测下一个词）是很自然的，因为它可以<strong>屏蔽后来的词</strong> 。当你使用它进行逐词翻译时，这是个很有用的特性。</p>
<p><img src="3-openai.webp" alt="open ai模型">图： open ai模型</p>
<p>OpenAI Transformer 是由 Transformer 的 Decoder 堆叠而成的</p>
<p>这个模型包括 12 个 Decoder 层。因为在这种设计中没有 Encoder，<strong>这些 Decoder 层不会像普通的 Transformer 中的 Decoder 层那样有 Encoder-Decoder Attention 子层</strong>。不过，它仍然会有 Self Attention 层（这些层使用了 mask，因此不会看到句子后来的 token）。</p>
<p><img src="3-openai-next.webp" alt="open ai模型预测下一个词">图： open ai模型预测下一个词</p>
<p>上图表示：OpenAI Transformer 在 7000 本书的组成的数据集中预测下一个单词。</p>
<h3 id="下游任务的迁移学习"><a href="#下游任务的迁移学习" class="headerlink" title="下游任务的迁移学习"></a>下游任务的迁移学习</h3><p>现在，OpenAI Transformer 已经经过了预训练，它的网络层经过调整，可以很好地处理文本语言，我们可以开始使用它来处理下游任务。让我们先看下句子分类任务（把电子邮件分类为 ”垃圾邮件“ 或者 ”非垃圾邮件“）：</p>
<p><img src="3-openai-down.png" alt="open ai模型下游任务">图： open ai模型下游任务</p>
<p>下面这张图片来源于论文，展示了执行不同任务的模型结构和对应输入变换。这些都是非常很巧妙的做法。</p>
<p><img src="3-openai-method.webp" alt="open ai微调">图： open ai微调</p>
<h2 id="BERT：从-Decoder-到-Encoder"><a href="#BERT：从-Decoder-到-Encoder" class="headerlink" title="BERT：从 Decoder 到 Encoder"></a>BERT：从 Decoder 到 Encoder</h2><p>OpenAI Transformer 为我们提供了一个基于 Transformer 的可以微调的预训练网络。但是在把 LSTM 换成 Transformer 的过程中，有些东西丢失了。ELMo 的语言模型是双向的，但 OpenAI Transformer 只训练了一个前向的语言模型。我们是否可以构建一个基于 Transformer 的语言模型，它既向前看，又向后看（用技术术语来说 - 融合上文和下文的信息）。</p>
<p>？？？？decoder中没有前后向的attention吗？</p>
<h3 id="Masked-Language-Model（MLM-语言模型）"><a href="#Masked-Language-Model（MLM-语言模型）" class="headerlink" title="Masked Language Model（MLM 语言模型）"></a>Masked Language Model（MLM 语言模型）</h3><p>那么如何才能像 LSTM 那样，融合上文和下文的双向信息呢？</p>
<p>一种直观的想法是使用 Transformer 的 Encoder。但是 Encoder 的 Self Attention 层，每个 token 会把大部分注意力集中到自己身上，那么这样将容易预测到每个 token，模型学不到有用的信息。BERT 提出使用 mask，把需要预测的词屏蔽掉。</p>
<p>下面这段风趣的对话是博客原文的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BERT 说，“我们要用 Transformer 的 Encoder”。</span><br><span class="line"></span><br><span class="line">Ernie 说，”这没什么用，因为每个 token 都会在多层的双向上下文中看到自己“。</span><br><span class="line"></span><br><span class="line">BERT 自信地说，”我们会使用 mask“。</span><br></pre></td></tr></table></figure>
<p><img src="3-bert-mask.webp" alt="BERT mask">图： BERT mask</p>
<p>BERT 在语言建模任务中，巧妙地屏蔽了输入中 15% 的单词，并让模型预测这些屏蔽位置的单词。</p>
<p>找到合适的任务来训练一个 Transformer 的 Encoder 是一个复杂的问题，BERT 通过使用早期文献中的 “masked language model” 概念（在这里被称为完形填空）来解决这个问题。</p>
<p>除了屏蔽输入中 15% 的单词外， BERT 还混合使用了其他的一些技巧，来改进模型的微调方式。例如，有时它会随机地用一个词替换另一个词，然后让模型预测这个位置原来的实际单词。</p>
<p><img src="3-bert-mask.webp" alt="BERT mask">图： BERT mask</p>
<p>BERT 在语言建模任务中，巧妙地屏蔽了输入中 15% 的单词，并让模型预测这些屏蔽位置的单词。</p>
<p>找到合适的任务来训练一个 Transformer 的 Encoder 是一个复杂的问题，BERT 通过使用早期文献中的 “masked language model” 概念（在这里被称为完形填空）来解决这个问题。</p>
<p>除了屏蔽输入中 15% 的单词外， <strong>BERT 还混合使用了其他的一些技巧</strong>，<strong>来改进模型的微调方式</strong>。例如，有<strong>时它会随机地用一个词替换另一个词，然后让模型预测这个位置原来的实际单词</strong>。</p>
<h3 id="两个句子的任务"><a href="#两个句子的任务" class="headerlink" title="两个句子的任务"></a>两个句子的任务</h3><p>如果你回顾 OpenAI Transformer 在处理不同任务时所做的输入变换，你会注意到有些任务需要模型对两个句子的信息做一些处理（例如，判断它们是不是同一句话的不同解释。将一个维基百科条目作为输入，再将一个相关的问题作为另一个输入，模型判断是否可以回答这个问题）。</p>
<p>为了让 BERT 更好地处理多个句子之间的关系，预训练过程还包括一个额外的任务：给出两个句子（A 和 B），判断 B 是否是 A 后面的相邻句子。</p>
<p><img src="3-bert-2sent.webp" alt="2个句子任务">图： 2个句子任务</p>
<p>BERT 预训练的第 2 个任务是两个句子的分类任务。<strong>在上图中</strong>，<strong>tokenization 这一步被简化了</strong>，<strong>因为 BERT 实际上使用了 WordPieces 作为 token</strong>，<strong>而不是使用单词本身</strong>。<strong>在 WordPiece 中</strong>，<strong>有些词会被拆分成更小的部分</strong>。</p>
<h3 id="BERT-在不同任务上的应用"><a href="#BERT-在不同任务上的应用" class="headerlink" title="BERT 在不同任务上的应用"></a>BERT 在不同任务上的应用</h3><p>BERT 的论文展示了 BERT 在多种任务上的应用。</p>
<p><img src="3-bert-app.png" alt="BERT应用">图： BERT应用</p>
<h3 id="将-BERT-用于特征提取"><a href="#将-BERT-用于特征提取" class="headerlink" title="将 BERT 用于特征提取"></a>将 BERT 用于特征提取</h3><p>使用 BERT 并不是只有微调这一种方法。就像 ELMo 一样，你可以使用预训练的 BERT 来创建语境化的word embedding。然后你可以把这些word embedding用到你现有的模型中。论文里也提到，这种方法在命名实体识别任务中的效果，接近于微调 BERT 模型的效果。</p>
<p><img src="3-bert-feature.png" alt="BERT特征提取">图： BERT特征提取</p>
<p>那么哪种向量最适合作为上下文词嵌入？我认为这取决于任务。论文里验证了 6 种选择（与微调后的 96.4 分的模型相比）：</p>
<p><img src="3-bert-fea.webp" alt="BERT特征选择">图： BERT特征选择</p>
<p>？？？？</p>
<h3 id="如何使用-BERT"><a href="#如何使用-BERT" class="headerlink" title="如何使用 BERT"></a>如何使用 BERT</h3><p>将在篇章3中进行更为详细的讲解。</p>
<p>尝试 BERT 的最佳方式是通过托管在 Google Colab 上的 BERT FineTuning with Cloud TPUs。如果你之前从来没有使用过 Cloud TPU，那这也是一个很好的尝试开端，因为 BERT 代码可以运行在 TPU、CPU 和 GPU。</p>
<p>下一步是查看 BERT 仓库 中的代码：</p>
<ul>
<li>模型是在 modeling.py（class BertModel）中定义的，和普通的 Transformer encoder 完全相同。</li>
<li>run_classifier.py 是微调网络的一个示例。它还构建了监督模型分类层。如果你想构建自己的- 分类器，请查看这个文件中的 create_model() 方法。</li>
<li>可以下载一些预训练好的模型。这些模型包括 BERT Base、BERT Large，以及英语、中文和包括 102 种语言的多语言模型，这些模型都是在维基百科的数据上进行训练的。</li>
<li>BERT 不会将单词作为 token。相反，它关注的是 WordPiece。tokenization.py 就是 tokenizer，它会将你的单词转换为适合 BERT 的 wordPiece。</li>
</ul>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>主要由哈尔滨工业大学张贤同学翻译（经过原作者授权）撰写，由本项目同学组织和整理。感谢Jacob Devlin、Matt Gardner、Kenton Lee、Mark Neumann 和 <a href="https://twitter.com/mattthemathman">Matthew Peters</a> 为这篇文章的早期版本提供了反馈</p>
<h2 id="图解GPT"><a href="#图解GPT" class="headerlink" title="图解GPT"></a>图解GPT</h2><p>最著名的语言模型就是手机键盘，它可以根据你输入的内容，提示下一个单词。</p>
<p><img src="4-gpt-bert.webp" alt="gpt-bert">图：gpt-bert</p>
<p>我们可以将这些模块堆得多高呢？事实证明，这是区分不同的 GPT-2 的主要因素之一。</p>
<p><img src="4-gpt-his2.webp" alt="gpt区分">图：gpt区分</p>
<h3 id="与-BERT-的一个不同之处"><a href="#与-BERT-的一个不同之处" class="headerlink" title="与 BERT 的一个不同之处"></a>与 BERT 的一个不同之处</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">机器人第一定律：</span><br><span class="line"></span><br><span class="line">机器人不得伤害人类，也不能因不作为而使人类受到伤害。</span><br></pre></td></tr></table></figure>

<p>GPT-2 是使用 Transformer 的 Decoder 模块构建的。另一方面，BERT 是使用 Transformer 的 Encoder 模块构建的。我们将在下一节中研究这种差异。但它们之间的一个重要差异是，GPT-2 和传统的语言模型一样，一次输出一个  token。例如，让一个训练好的 GPT-2 背诵机器人第一定律：</p>
<p><img src="4-gpt2-output.webp" alt="gpt2 output">图： gpt2 output</p>
<p>这些模型的实际工作方式是，在产生每个 token 之后，将这个 token 添加到输入的序列中，形成一个新序列。然后这个新序列成为模型在下一个时间步的输入。这是一种叫“<strong>自回归</strong>（auto-regression）”的思想。这种做法可以使得 RNN 非常有效。</p>
<p><img src="4-gpt2-output2.webp" alt="gpt2 output">图： gpt2 output</p>
<p>GPT-2，和后来的一些模型如 TransformerXL 和 XLNet，本质上都是自回归的模型。但 BERT 不是自回归模型。这是一种权衡。去掉了自回归后，BERT 能够整合左右两边的上下文，从而获得更好的结果。XLNet 重新使用了 自回归，同时也找到一种方法能够结合两边的上下文。</p>
<h3 id="Transformer-模块的进化"><a href="#Transformer-模块的进化" class="headerlink" title="Transformer 模块的进化"></a>Transformer 模块的进化</h3><p>Transformer 原始论文(<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>) 介绍了两种模块：</p>
<p>Encoder 模块</p>
<p>首先是 Encoder 模块。</p>
<p><img src="4-encoder.webp" alt="encoder">图： encoder</p>
<p>原始的 Transformer 论文中的 Encoder 模块接受特定长度的输入（如 512 个 token）。如果一个输入序列比这个限制短，我们可以填充序列的其余部分。</p>
<p>Decoder 模块</p>
<p>其次是 Decoder。与 Encoder 相比，它在结构上有一个很小的差异：它有一个层，使得它可以关注来自 Encoder 特定的段。</p>
<p><img src="4-decoder.webp" alt="decoder">图： decoder</p>
<p>这里的 <strong>Self Attention 层的一个关键差异是，它会屏蔽未来的 token</strong>。具体来说，它不像 BERT 那样将单词改为mask，而是通过改变 Self Attention 的计算，阻止来自被计算位置右边的 token。</p>
<p>例如，我们想要计算位置 4，我们可以看到只允许处理以前和现在的 token。</p>
<p><img src="4-decoder1.webp" alt="decoder只能看到以前和现在的token">图： decoder只能看到以前和现在的token</p>
<p>很重要的一点是，（BERT 使用的）Self Attention 和 （GPT-2 使用的）masked Self Attention 有明确的区别。一个正常的 Self Attention 模块允许一个位置关注到它右边的部分。而 <strong>masked Self Attention</strong> 阻止了这种情况的发生：</p>
<p><img src="4-mask.png" alt="mask attention">图： mask attention</p>
<p>只有 Decoder 的模块</p>
<p>在 Transformer 原始论文发布之后，Generating Wikipedia by Summarizing Long Sequences(<a href="https://arxiv.org/pdf/1801.10198.pdf">https://arxiv.org/pdf/1801.10198.pdf</a>) 提出了另一种能够进行语言建模的 Transformer 模块的布局。这个模型丢弃了 Transformer 的 Encoder。因此，我们可以把这个模型称为 Transformer-Decoder。这种早期的基于 Transformer 的语言模型由 6 个 Decoder 模块组成。</p>
<p><img src="4-trans-decoder.webp" alt="transformer-decoder">图： transformer-decoder</p>
<p>这些 Decoder 模块都是相同的。我已经展开了第一个 Decoder，因此你可以看到它的 Self Attention 层是 masked 的。注意，现在这个模型可以处理多达 4000 个 token–是对原始论文中 512 个 token 的一个大升级。</p>
<p>这些模块和原始的 Decoder 模块非常类似，只是它们去掉了第二个 Self Attention 层。在 Character-Level Language Modeling with Deeper Self-Attention(<a href="https://arxiv.org/pdf/1808.04444.pdf">https://arxiv.org/pdf/1808.04444.pdf</a>) 中使用了类似的结构，来创建一次一个字母/字符的语言模型。</p>
<p>OpenAI 的 GPT-2 使用了这些 Decoder 模块。</p>
<h3 id="语言模型入门：了解-GPT2"><a href="#语言模型入门：了解-GPT2" class="headerlink" title="语言模型入门：了解 GPT2"></a>语言模型入门：了解 GPT2</h3><p>让我们拆解一个训练好的 GPT-2，看看它是如何工作的。</p>
<p><img src="4-gpt2-1.png" alt="拆解GPT2">图：拆解GPT2</p>
<p>GPT-2 能够处理 1024 个 token。每个 token 沿着自己的路径经过所有的 Decoder 模块</p>
<p>运行一个训练好的 GPT-2 模型的最简单的方法是让它自己生成文本（这在技术上称为 生成无条件样本）。或者，我们可以给它一个提示，让它谈论某个主题（即生成交互式条件样本）。在漫无目的情况下，我们可以简单地给它输入初始 token，并让它开始生成单词（训练好的模型使用 &lt;|endoftext|&gt; 作为初始的 token。我们称之为 &lt;s&gt;）。</p>
<p><img src="4-gpt2-start.webp" alt="拆解GPT2初始token">图：拆解GPT2初始token</p>
<p>模型只有一个输入的 token，因此只有一条活跃路径。token 在所有层中依次被处理，然后沿着该路径生成一个向量。这个向量可以根据模型的词汇表计算出一个分数（模型知道所有的 单词，在 GPT-2 中是 5000 个词）。在这个例子中，我们选择了概率最高的 the。但我们可以把事情搞混–你知道如果一直在键盘 app 中选择建议的单词，它有时候会陷入重复的循环中，唯一的出路就是点击第二个或者第三个建议的单词。同样的事情也会发生在这里，GPT-2 有一个 top-k 参数，我们可以使用这个参数，让模型考虑第一个词（top-k =1）之外的其他词。</p>
<p>下一步，我们把第一步的输出添加到我们的输入序列，然后让模型做下一个预测。</p>
<p><img src="4-gpt2-the.gif" alt="拆解GPT2">动态图：拆解GPT2</p>
<p>请注意，第二条路径是此计算中唯一活动的路径。GPT-2 的每一层都保留了它自己对第一个 token 的解释，而且会在处理第二个 token 时使用它（我们会在接下来关于 Self Attention 的章节中对此进行更详细的介绍）。GPT-2 不会根据第二个 token 重新计算第一个 token。</p>
<h3 id="深入理解-GPT2-的更多细节"><a href="#深入理解-GPT2-的更多细节" class="headerlink" title="深入理解 GPT2 的更多细节"></a>深入理解 GPT2 的更多细节</h3><p><strong>输入编码</strong></p>
<p>让我们更深入地了解模型。首先从输入开始。与之前我们讨论的其他 NLP 模型一样，GPT-2 在嵌入矩阵中查找输入的单词的对应的 embedding 向量–这是我们从训练好的模型中得到的组件之一。</p>
<p><img src="4-gpt-token.png" alt="token embedding">图：token embedding</p>
<p>每一行都是词的 embedding：这是一个数字列表，可以表示一个词并捕获一些含义。这个列表的大小在不同的 GPT-2 模型中是不同的。最小的模型使用的 embedding 大小是 768</p>
<p>因此在开始时，我们会在嵌入矩阵查找第一个 token  的 embedding。在把这个 embedding 传给模型的第一个模块之前，我们需要融入位置编码，这个位置编码能够指示单词在序列中的顺序。训练好的模型中，有一部分是一个矩阵，这个矩阵包括了 1024 个位置中每个位置的位置编码向量。</p>
<p><img src="4-gpt-pos.webp" alt="位置编码">图：位置编码</p>
<p>在这里，我们讨论了输入单词在传递到第一个 Transformer 模块之前，是如何被处理的。我们还知道，训练好的 GPT-2 包括两个权重矩阵。</p>
<p><img src="4-gpt-token-pos.png" alt="token+position">图： token+position</p>
<p>把一个单词输入到 Transformer 的第一个模块，意味着寻找这个单词的 embedding，并且添加第一个位置的位置编码向量</p>
<p><strong>在这些层中向上流动</strong></p>
<p>第一个模块现在可以处理 token，首先通过 Self Attention 层，然后通过神经网络层。一旦 Transformer 的第一个模块处理了 token，会得到一个结果向量，这个结果向量会被发送到堆栈的下一个模块处理。每个模块的处理过程都是相同的，不过每个模块都有自己的 Self Attention 和神经网络层。</p>
<p><img src="4-gpt-fllow.webp" alt="向上流动">图：向上流动</p>
<p><strong>回顾 Self-Attention</strong></p>
<p>语言严重依赖于上下文。例如，看看下面的第二定律：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">机器人第二定律</span><br><span class="line"></span><br><span class="line">机器人必须服从人给予**它**的命令，当**该命令**与**第一定律**冲突时例外。</span><br></pre></td></tr></table></figure>
<p>我在句子中加粗了 3 个部分，这些部分的词是用于指代其他的词。如果不结合它们所指的上下文，就无法理解或者处理这些词。当一个模型处理这个句子，它必须能够知道：</p>
<ul>
<li>它 指的是机器人</li>
<li>该命令 指的是这个定律的前面部分，也就是 人给予 它 的命令</li>
<li>第一定律 指的是机器人第一定律</li>
</ul>
<p>这就是 <strong>Self Attention</strong> 所做的事。它在处理某个词之前，将模型对这个词的相关词和关联词的理解融合起来（并输入到一个神经网络）。它通过对句子片段中每个词的相关性打分，并将这些词的表示向量加权求和。</p>
<p>举个例子，下图顶部模块中的 Self Attention 层在处理单词 <code>it</code> 的时候关注到<code> a robot</code>。它传递给神经网络的向量，是 3 个单词和它们各自分数相乘再相加的和。</p>
<p><img src="4-gpt-it.webp" alt="it的attention">图：it的attention</p>
<p><strong>Self-Attention 过程</strong></p>
<p>Self-Attention 沿着句子中每个 token 的路径进行处理，主要组成部分包括 3 个向量。</p>
<ul>
<li>Query：Query 向量是当前单词的表示，用于对其他所有单词（使用这些单词的 key 向量）进行评分。我们只关注当前正在处理的 token 的 query 向量。</li>
<li>Key：Key 向量就像句子中所有单词的标签。它们就是我们在搜索单词时所要匹配的。</li>
<li>Value：Value 向量是实际的单词表示，一旦我们对每个词的相关性进行了评分，我们需要对这些向量进行加权求和，从而表示当前的词。</li>
</ul>
<p><img src="4-gpt-query.webp" alt="query">图： query<br>一个粗略的类比是把它看作是在一个文件柜里面搜索，Query 向量是一个便签，上面写着你正在研究的主题，而 Key 向量就像是柜子里的文件夹的标签。当你将便签与标签匹配时，我们取出匹配的那些文件夹的内容，这些内容就是 Value 向量。但是你不仅仅是寻找一个 Value 向量，而是在一系列文件夹里寻找一系列 Value 向量。</p>
<p>将 Query 向量与每个文件夹的 Key 向量相乘，会为每个文件夹产生一个分数（从技术上来讲：就是点积后面跟着 softmax）。</p>
<p><img src="4-gpt-score.webp" alt="score">图： score</p>
<p>我们将每个 Value 向量乘以对应的分数，然后求和，得到 Self Attention 的输出。</p>
<p><img src="4-gpt-out.webp" alt="Self Attention 的输出">图：Self Attention 的输出</p>
<p>这些加权的 Value 向量会得到一个向量，它将 50% 的注意力放到单词 robot 上，将 30% 的注意力放到单词 a，将 19% 的注意力放到单词 it。在下文中，我们会更加深入 Self Attention，但现在，首先让我们继续在模型中往上走，直到模型的输出。</p>
<p><strong>模型输出</strong></p>
<p>当模型顶部的模块产生输出向量时（这个向量是经过 Self Attention 层和神经网络层得到的），模型会将这个向量乘以嵌入矩阵。</p>
<p><img src="4-gpt-out1.webp" alt="顶部的模块产生输出">图：顶部的模块产生输出</p>
<p>回忆一下，嵌入矩阵中的每一行都对应于模型词汇表中的一个词。这个相乘的结果，被解释为模型词汇表中每个词的分数。</p>
<p><img src="4-gpt-out3.webp" alt="token概率">图：token概率</p>
<p>我们可以选择最高分数的 token（top_k=1）。但如果模型可以同时考虑其他词，那么可以得到更好的结果。所以一个更好的策略是把分数作为单词的概率，从整个列表中选择一个单词（这样分数越高的单词，被选中的几率就越高）。一个折中的选择是把 top_k 设置为 40，让模型考虑得分最高的 40 个词。</p>
<p><img src="4-gpt-out4.webp" alt="top k选择输出">图：top k选择输出</p>
<p>这样，模型就完成了一次迭代，输出一个单词。模型会继续迭代，直到所有的上下文都已经生成（1024 个 token），或者直到输出了表示句子末尾的 token。</p>
<h3 id="GPT2-总结"><a href="#GPT2-总结" class="headerlink" title="GPT2 总结"></a>GPT2 总结</h3><p>现在我们基本知道了 GPT-2 是如何工作的。如果你想知道 Self Attention 层里面到底发生了什么，那么文章接下来的额外部分就是为你准备的，我添加这个额外的部分，来使用更多可视化解释 Self Attention，以便更加容易讲解后面的 Transformer 模型（TransformerXL 和 XLNet）。</p>
<p>我想在这里指出文中一些过于简化的说法：</p>
<ul>
<li>我在文中交替使用 token 和 词。但实际上，GPT-2 使用 Byte Pair Encoding 在词汇表中创建 token。这意味着 token 通常是词的一部分。</li>
<li>我们展示的例子是在推理模式下运行。这就是为什么它一次只处理一个 token。在训练时，模型将会针对更长的文本序列进行训练，并且同时处理多个 token。同样，在训练时，模型会处理更大的 batch size，而不是推理时使用的大小为 1 的 batch size。</li>
<li>为了更加方便地说明原理，我在本文的图片中一般会使用行向量。但有些向量实际上是列向量。在代码实现中，你需要注意这些向量的形式。</li>
<li>Transformer 使用了大量的层归一化（layer normalization），这一点是很重要的。我们在图解Transformer中已经提及到了一部分这点，但在这篇文章，我们会更加关注 Self Attention。</li>
<li>有时我需要更多的框来表示一个向量，例如下面这幅图：</li>
</ul>
<p><img src="4-gpt-sum.webp" alt="输入与输出维度">图：输入与输出维度</p>
<h3 id="可视化-Self-Attention"><a href="#可视化-Self-Attention" class="headerlink" title="可视化 Self-Attention"></a>可视化 Self-Attention</h3><p>在这篇文章的前面，我们使用了这张图片来展示，如何在一个层中使用 Self Attention，这个层正在处理单词 <code>it</code>。</p>
<p><img src="4-att-it.png" alt="it的attention">图：it的attention</p>
<p>在这一节，我们会详细介绍如何实现这一点。请注意，我们会讲解清楚每个单词都发生了什么。这就是为什么我们会展示大量的单个向量。而实际的代码实现，是通过巨大的矩阵相乘来完成的。但我想把重点放在词汇层面上。</p>
<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>让我们先看看原始的 Self Attention，它被用在 Encoder 模块中进行计算。让我们看看一个玩具 Transformer，它一次只能处理 4 个 token。</p>
<p>Self-Attention 主要通过 3 个步骤来实现：</p>
<ul>
<li>为每个路径创建 Query、Key、Value 矩阵。</li>
<li>对于每个输入的 token，使用它的 Query 向量为所有其他的 Key 向量进行打分。</li>
<li>将 Value 向量乘以它们对应的分数后求和。</li>
</ul>
<p><img src="4-att-3.webp" alt="3步">图：3步</p>
<p>(1) 创建 Query、Key 和 Value 向量</p>
<p>让我们关注第一条路径。我们会使用它的 Query 向量，并比较所有的 Key 向量。这会为每个 Key 向量产生一个分数。Self Attention 的第一步是为每个 token 的路径计算 3 个向量。</p>
<p><img src="4-att-31.webp" alt="第1步">图：第1步</p>
<p>(2) 计算分数</p>
<p>现在我们有了这些向量，我们只对步骤 2 使用 Query 向量和 Value 向量。因为我们关注的是第一个 token 的向量，我们将第一个 token 的 Query 向量和其他所有的 token 的 Key 向量相乘，得到 4 个 token 的分数。</p>
<p><img src="4-att-32.webp" alt="第2步">图：第2步</p>
<p>(3) 计算和</p>
<p>我们现在可以将这些分数和 Value 向量相乘。在我们将它们相加后，一个具有高分数的 Value 向量会占据结果向量的很大一部分。</p>
<p><img src="4-att-33.webp" alt="第3步">图：第3步</p>
<p>分数越低，Value 向量就越透明。这是为了说明，乘以一个小的数值会稀释 Value 向量。</p>
<p>如果我们对每个路径都执行相同的操作，我们会得到一个向量，可以表示每个 token，其中包含每个 token 合适的上下文信息。这些向量会输入到 Transformer 模块的下一个子层（前馈神经网络）。</p>
<p><img src="4-att-34.webp" alt="汇总">图：汇总</p>
<h3 id="图解-Masked-Self-attention"><a href="#图解-Masked-Self-attention" class="headerlink" title="图解 Masked Self_attention"></a>图解 Masked Self_attention</h3><p>现在，我们已经了解了 Transformer 的 Self Attention 步骤，现在让我们继续研究 masked Self Attention。Masked Self Attention 和 Self Attention 是相同的，除了第 2 个步骤。假设模型只有  2 个 token 作为输入，我们正在观察（处理）第二个 token。在这种情况下，最后 2 个 token 是被屏蔽（masked）的。所以模型会干扰评分的步骤。它基本上总是把未来的 token 评分为 0，因此模型不能看到未来的词：</p>
<p><img src="4-mask.webp" alt="masked self attention">图：masked self attention</p>
<p>这个屏蔽（masking）经常用一个矩阵来实现，称为 attention mask。想象一下有 4 个单词的序列（例如，机器人必须遵守命令）。在一个语言建模场景中，这个序列会分为 4 个步骤处理–每个步骤处理一个词（假设现在每个词是一个 token）。由于这些模型是以 batch size 的形式工作的，我们可以假设这个玩具模型的 batch size 为 4，它会将整个序列作（包括 4 个步骤）为一个 batch 处理。</p>
<p><img src="4-mask-matrix.webp" alt="masked 矩阵">图：masked 矩阵</p>
<p>在矩阵的形式中，我们把 Query 矩阵和 Key 矩阵相乘来计算分数。让我们将其可视化如下，不同的是，我们不使用单词，而是使用与格子中单词对应的 Query 矩阵（或者 Key 矩阵）。</p>
<p><img src="4-mask-q.webp" alt="Query矩阵">图：Query矩阵</p>
<p>在做完乘法之后，我们加上三角形的 attention mask。它将我们想要屏蔽的单元格设置为负无穷大或者一个非常大的负数（例如 GPT-2 中的 负十亿）：</p>
<p><img src="4-mask-s.webp" alt="加上attetnion的mask">图：加上attetnion的mask</p>
<p>然后对每一行应用 softmax，会产生实际的分数，我们会将这些分数用于 Self Attention。</p>
<p><img src="4-mask-soft.webp" alt="softmax">图：softmax</p>
<p>这个分数表的含义如下：</p>
<ul>
<li>当模型处理数据集中的第 1 个数据（第 1 行），其中只包含着一个单词 （robot），它将 100% 的注意力集中在这个单词上。</li>
<li>当模型处理数据集中的第 2 个数据（第 2 行），其中包含着单词（robot must）。当模型处理单词 must，它将 48% 的注意力集中在 robot，将 52% 的注意力集中在 must。</li>
<li>诸如此类，继续处理后面的单词。</li>
</ul>
<h3 id="GPT2-的-Self-Attention"><a href="#GPT2-的-Self-Attention" class="headerlink" title="GPT2 的 Self-Attention"></a>GPT2 的 Self-Attention</h3><p>让我们更详细地了解 GPT-2 的 masked attention。</p>
<p><em>评价模型：每次处理一个 token</em></p>
<p>我们可以让 GPT-2 像 mask Self Attention 一样工作。但是在评价模型时，当我们的模型在每次迭代后只添加一个新词，那么对于已经处理过的 token 来说，沿着之前的路径重新计算 Self Attention 是低效的。</p>
<p>在这种情况下，我们处理第一个 token（现在暂时忽略 &lt;s&gt;）。</p>
<p><img src="4-gpt2-self.png" alt="gpt2第一个token">图：gpt2第一个token</p>
<p>GPT-2 保存 token <code>a</code> 的 Key 向量和 Value 向量。每个 Self Attention 层都持有这个 token 对应的 Key 向量和 Value 向量：</p>
<p><img src="4-gpt2-a.png" alt="gpt2的词a">图：gpt2的词a</p>
<p>现在在下一个迭代，当模型处理单词 robot，它不需要生成 token a 的 Query、Value 以及 Key 向量。它只需要重新使用第一次迭代中保存的对应向量：</p>
<p><img src="4-gpt2-r.png" alt="gpt2的词robot">图：gpt2的词robot</p>
<p><code>(1) 创建 Query、Key 和 Value 矩阵</code></p>
<p>让我们假设模型正在处理单词 <code>it</code>。如果我们讨论最下面的模块（对于最下面的模块来说），这个 token 对应的输入就是 <code>it</code> 的 embedding 加上第 9 个位置的位置编码：</p>
<p><img src="4-gpt2-it.webp" alt="处理it">图：处理it</p>
<p>Transformer 中每个模块都有它自己的权重（在后文中会拆解展示）。我们首先遇到的权重矩阵是用于创建 Query、Key、和 Value 向量的。</p>
<p><img src="4-gpt2-it1.webp" alt="处理it">图：处理it</p>
<p>Self-Attention 将它的输入乘以权重矩阵（并添加一个 bias 向量，此处没有画出)</p>
<p>这个相乘会得到一个向量，这个向量基本上是 Query、Key 和 Value 向量的拼接。<br><img src="4-gpt2-it2.webp" alt="处理it">图：处理it</p>
<p>将输入向量与 attention 权重向量相乘（并加上一个 bias 向量）得到这个 token 的 Key、Value 和 Query 向量拆分为 attention heads。</p>
<p>在之前的例子中，我们只关注了 Self Attention，忽略了 multi-head 的部分。现在对这个概念做一些讲解是非常有帮助的。Self-attention 在 Q、K、V 向量的不同部分进行了多次计算。拆分 attention heads 只是把一个长向量变为矩阵。小的 GPT-2 有 12 个 attention heads，因此这将是变换后的矩阵的第一个维度：</p>
<p><img src="4-gpt2-it3.png" alt="处理it">图：处理it</p>
<p>在之前的例子中，我们研究了一个 attention head 的内部发生了什么。理解多个 attention-heads 的一种方法，是像下面这样（如果我们只可视化 12 个 attention heads 中的 3 个）：</p>
<p><img src="4-gpt2-it4.webp" alt="处理it">图：处理it</p>
<p><code>(2) 评分</code></p>
<p>我们现在可以继续进行评分，这里我们只关注一个 attention head（其他的 attention head 也是在进行类似的操作）。</p>
<p><img src="4-gpt2-it5.webp">图：处理it</p>
<p>现在，这个 token 可以根据其他所有 token 的 Key 向量进行评分（这些 Key 向量是在前面一个迭代中的第一个 attention head 计算得到的）：</p>
<p><img src="4-gpt2-it6.webp">图：</p>
<p><code>(3) 求和</code></p>
<p>正如我们之前所看的那样，我们现在将每个 Value 向量乘以对应的分数，然后加起来求和，得到第一个 attention head 的 Self Attention 结果：</p>
<p><img src="4-gpt2-it7.webp" alt="处理it">图：</p>
<p><code>合并 attention heads</code></p>
<p>我们处理各种注意力的方法是首先把它们连接成一个向量：</p>
<p><img src="4-gpt2-it8.webp" alt="处理it">图：处理it</p>
<p>但这个向量还没有准备好发送到下一个子层（向量的长度不对）。我们首先需要把这个隐层状态的巨大向量转换为同质的表示。</p>
<p><code>(4) 映射（投影）</code></p>
<p>我们将让模型学习如何将拼接好的 Self Attention 结果转换为前馈神经网络能够处理的形状。在这里，我们使用第二个巨大的权重矩阵，将 attention heads 的结果映射到 Self Attention 子层的输出向量：</p>
<p><img src="4-project.png" alt="映射">图：映射</p>
<p>通过这个，我们产生了一个向量，我们可以把这个向量传给下一层：</p>
<p><img src="4-vector.webp" alt="传给下一层">图：传给下一层</p>
<h3 id="GPT-2-全连接神经网络"><a href="#GPT-2-全连接神经网络" class="headerlink" title="GPT-2 全连接神经网络"></a>GPT-2 全连接神经网络</h3><p><code>第 1 层</code></p>
<p>全连接神经网络是用于处理 Self Attention 层的输出，这个输出的表示包含了合适的上下文。全连接神经网络由两层组成。第一层是模型大小的 4 倍（由于 GPT-2 small 是 768，因此这个网络会有3072个神经元）。为什么是四倍？这只是因为这是原始 Transformer 的大小（如果模型的维度是 512，那么全连接神经网络中第一个层的维度是 2048）。这似乎给了 Transformer 足够的表达能力，来处理目前的任务。</p>
<p><img src="4-full.gif" alt="全连接层">动态图：全连接层</p>
<p>没有展示 bias 向量</p>
<p><code>第 2 层. 把向量映射到模型的维度</code></p>
<p>第 2 层把第一层得到的结果映射回模型的维度（在 GPT-2 small 中是 768）。这个相乘的结果是 Transformer 对这个 token 的输出。</p>
<p><img src="4-full.webp" alt="全连接层">图：全连接层</p>
<p>没有展示 bias 向量</p>
<p>你完成了！</p>
<p>这就是我们讨论的 Transformer 的最详细的版本！现在，你几乎已经了解了 Transformer 语言模型内部发生了什么。总结一下，我们的输入会遇到下面这些权重矩阵：</p>
<p><img src="4-sum.png" alt="总结">图：</p>
<p>每个模块都有它自己的权重。另一方面，模型只有一个 token embedding 矩阵和一个位置编码矩阵。</p>
<p><img src="4-sum1.png" alt="总结">图：总结</p>
<p>如果你想查看模型的所有参数，我在这里对它们进行了统计：</p>
<p><img src="4-sum2.png" alt="总结">图：总结<br>由于某些原因，它们加起来是 124 M，而不是 117 M。我不确定这是为什么，但这个就是在发布的代码中展示的大小（如果我错了，请纠正我）。</p>
<h2 id="语言模型之外"><a href="#语言模型之外" class="headerlink" title="语言模型之外"></a>语言模型之外</h2><p>只有 Decoder 的 Transformer 在语言模型之外一直展现出不错的应用。它已经被成功应用在了许多应用中，我们可以用类似上面的可视化来描述这些成功应用。让我们看看这些应用，作为这篇文章的结尾。</p>
<h3 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h3><p>进行机器翻译时，Encoder 不是必须的。我们可以用只有 Decoder 的 Transformer 来解决同样的任务：</p>
<p><img src="4-trans.png" alt="翻译">图：翻译</p>
<h3 id="生成摘要"><a href="#生成摘要" class="headerlink" title="生成摘要"></a>生成摘要</h3><p>这是第一个只使用 Decoder 的 Transformer 来训练的任务。它被训练用于阅读一篇维基百科的文章（目录前面去掉了开头部分），然后生成摘要。文章的实际开头部分用作训练数据的标签：<br><img src="4-wiki.png" alt="摘要">图：</p>
<p>论文里针对维基百科的文章对模型进行了训练，因此这个模型能够总结文章，生成摘要：</p>
<p><img src="4-wiki1.webp" alt="摘要">图：摘要</p>
<h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>在 Sample Efficient Text Summarization Using a Single Pre-Trained Transformer(<a href="https://arxiv.org/abs/1905.08836">https://arxiv.org/abs/1905.08836</a>) 中，一个只有 Decoder 的 Transformer 首先在语言模型上进行预训练，然后微调进行生成摘要。结果表明，在数据量有限制时，它比预训练的 Encoder-Decoder Transformer 能够获得更好的结果。</p>
<p>GPT-2 的论文也展示了在语言模型进行预训练的生成摘要的结果。</p>
<h3 id="音乐生成"><a href="#音乐生成" class="headerlink" title="音乐生成"></a>音乐生成</h3><p>Music Transformer(<a href="https://magenta.tensorflow.org/music-transformer">https://magenta.tensorflow.org/music-transformer</a>) 论文使用了只有 Decoder 的 Transformer 来生成具有表现力的时序和动态性的音乐。音乐建模 就像语言建模一样，只需要让模型以无监督的方式学习音乐，然后让它采样输出（前面我们称这个为 漫步）。</p>
<p>你可能会好奇在这个场景中，音乐是如何表现的。请记住，语言建模可以把字符、单词、或者单词的一部分（token），表示为向量。在音乐表演中（让我们考虑一下钢琴），我们不仅要表示音符，还要表示速度–衡量钢琴键被按下的力度。</p>
<p><img src="4-music.webp" alt="音乐生成">图：音乐生成</p>
<p>一场表演就是一系列的 one-hot 向量。一个 midi 文件可以转换为下面这种格式。论文里使用了下面这种输入序列作为例子：</p>
<p><img src="4-music1.png" alt="音乐生成">图：音乐生成</p>
<p>这个输入系列的 one-hot 向量表示如下：</p>
<p><img src="4-music2.png" alt="音乐生成">图：音乐生成</p>
<p>我喜欢论文中的音乐 Transformer 展示的一个 Self Attention 的可视化。我在这基础之上添加了一些注释：</p>
<p><img src="4-music3.png" alt="音乐生成">图：音乐生成</p>
<p>这段音乐有一个反复出现的三角形轮廓。Query 矩阵位于后面的一个峰值，它注意到前面所有峰值的高音符，以知道音乐的开头。这幅图展示了一个 Query 向量（所有 attention 线的来源）和前面被关注的记忆（那些受到更大的softmax 概率的高亮音符）。attention 线的颜色对应不同的 attention heads，宽度对应于 softmax 概率的权重。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>现在，我们结束了 GPT-2 的旅程，以及对其父模型（只有 Decoder 的 Transformer）的探索。我希望你看完这篇文章后，能对 Self Attention 有一个更好的理解，也希望你能对 Transformer 内部发生的事情有更多的理解。</p>
<h2 id="致谢-1"><a href="#致谢-1" class="headerlink" title="致谢"></a>致谢</h2><p>主要由哈尔滨工业大学张贤同学翻译（经过原作者授权）撰写，由本项目同学组织和整理。</p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[NLP-transformer]]></title>
      <url>/2021/08/18/NLP-transformer/</url>
      <content type="html"><![CDATA[<ul>
<li>DataWhale开源学习资料:<a href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a><h2 id="图解Attention"><a href="#图解Attention" class="headerlink" title="图解Attention"></a>图解Attention</h2></li>
</ul>
<p>Seq2seq：有2篇开创性的论文：<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sutskever等2014年发表的Sequence to Sequence Learning<br>with Neural Networks</a>和<a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf">Cho等2014年发表的Learning Phrase Representations using RNN Encoder–Decoder<br>for Statistical Machine Translation</a>都对这些模型进行了解释。</p>
<p>编码器和解码器在Transformer出现之前一般采用的是循环神经网络。关于循环神经网络，建议阅读 <a href="https://www.youtube.com/watch?v=UNmqTiOnRfg">Luis Serrano写的一篇关于循环神经网络</a>的精彩介绍.</p>
<p>如果你觉得你准备好了学习注意力机制的代码实现，一定要看看基于 TensorFlow 的 神经机器翻译 (seq2seq) <a href="https://github.com/tensorflow/nmt">指南</a>。</p>
<h2 id="图解transformer"><a href="#图解transformer" class="headerlink" title="图解transformer"></a>图解transformer</h2><p>本文翻译自<a href="http://jalammar.github.io/illustrated-transformer">illustrated-transformer</a></p>
<p>添加了一些简单的代码，实现了一个基本的 Self Attention 以及 multi-head attention 的矩阵运算。</p>
<p>2017 年，Google 提出了 Transformer 模型，用 <strong>Self Attention</strong> 的结构，取代了以往 NLP 任务中的 <strong>RNN</strong> 网络结构</p>
<p>用Self Attention机制效果又好，而且还可以并行计算。<br>（这个模型的其中一个优点，就是使得模型训练过程能够并行计算。在 RNN 中，每一个 time step 的计算都依赖于上一个 time step 的输出，这就使得所有的 time step 必须串行化，无法并行计算。）</p>
<p>编码部分是多层的编码器(Encoder)组成（Transformer 的论文中使用了 6 层编码器，这里的层数 6 并不是固定的，你也可以根据实验效果来修改层数）。同理，解码部分也是由多层的解码器(Decoder)组成（论文里也使用了 6 层的解码器）。每层编码器在结构上都是一样的，但不同层编码器的权重参数是不同的。</p>
<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>每层编码器里面，主要由以下两部分组成：</p>
<ul>
<li>Self-Attention Layer</li>
<li>Feed Forward Neural Network（前馈神经网络，缩写为 FFNN）<br><img src="2-encoder.png" alt="encoder"></li>
</ul>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>同理，解码器也具有这两层，<strong>但是这两层中间还插入了一个 Encoder-Decoder Attention 层</strong>，这个层能帮助解码器聚焦于输入句子的相关部分（类似于 seq2seq 模型 中的 Attention）。<br><img src="2-decoder.webp" alt="decoder"></p>
<h3 id="Transformer细节"><a href="#Transformer细节" class="headerlink" title="Transformer细节"></a>Transformer细节</h3><h4 id="Transformer-的输入"><a href="#Transformer-的输入" class="headerlink" title="Transformer 的输入"></a>Transformer 的输入</h4><p>和通常的 NLP 任务一样，我们<strong>首先会使用词嵌入算法（embedding algorithm）</strong>，将每个词转换为一个词向量。实际中向量一般是 256 或者 512 维。</p>
<p>那么整个输入的句子是一个向量列表，其中有 3 个词向量。在实际中，每个句子的长度不一样，我们会取一个适当的值，作为向量列表的长度。<strong>如果一个句子达不到这个长度</strong>，<strong>那么就填充全为 0 的词向量</strong>；<strong>如果句子超出这个长度，则做截断</strong>。句子长度是一个超参数，通常是训练集中的句子的最大长度，你可以尝试不同长度的效果。</p>
<p>编码器（Encoder）接收的输入都是一个向量列表，输出也是大小同样的向量列表。</p>
<p><img src="2-x-encoder.png" alt="输入encoder"><br>图：输入encoder</p>
<p><img src="2-multi-encoder.webp" alt="一层传一层"><br>图：一层传一层</p>
<p>每一个人都经历过类似的无知的狼狈，其实每一个人都经历了自我怀疑之后才能如释重负。<br><strong>别被“Self-Attention”这么高大上的词给唬住了，乍一听好像每个人都应该对这个词熟悉一样</strong>。</p>
<p>当模型处理句子中的每个词时，Self Attention机制使得模型不仅能够关注这个位置的词，而且能够关注句子中其他位置的词，作为辅助线索，进而可以更好地编码当前位置的词。如果你熟悉 RNN，回忆一下：RNN 在处理一个词时，会考虑前面传过来的hidden state，而hidden state就包含了前面的词的信息。而 Transformer 使用Self Attention机制，会把其他单词的理解融入处理当前的单词。</p>
<h3 id="Self-Attention的细节"><a href="#Self-Attention的细节" class="headerlink" title="Self-Attention的细节"></a>Self-Attention的细节</h3><p>计算Query 向量，Key 向量，Value 向量</p>
<p>下面我们先看下如何使用向量来计算 Self Attention，然后再看下如何使用矩阵来实现 Self Attention。（矩阵运算的方式，使得 Self Attention 的计算能够并行化，这也是 Self Attention 最终的实现方式）。</p>
<p>计算 Self Attention 的第 1 步是：对输入编码器的每个词向量，都创建 3 个向量，分别是：Query 向量，Key 向量，Value 向量。这 3 个向量是词向量分别和 3 个矩阵相乘得到的，而这个矩阵是我们要学习的参数。</p>
<p>注意，这 3 个新得到的向量一般比原来的词向量的长度更小。假设这 3 个向量的长度是$d_{key}$，而原始的词向量或者最终输出的向量的长度是 512（<strong>这 3 个向量的长度，和最终输出的向量长度，是有倍数关系的</strong>）。关于 Multi-head Attention，后面会给出实际代码。这里为了简化，假设只有一个 head 的 Self-Attention。</p>
<p><img src="2-qkv.png" alt="Q,K,V">图：Q,K,V</p>
<p>这里可以锻炼一下github的提交，上下角标？？？？？？</p>
<p>上图中，有两个词向量：Thinking 的词向量 x1 和 Machines 的词向量 x2。以 x1 为例，X1 乘以 WQ 得到 q1，q1 就是 X1 对应的 Query 向量。同理，X1 乘以 WK 得到 k1，k1 是 X1 对应的 Key 向量；X1 乘以 WV 得到 v1，v1 是 X1 对应的 Value 向量。</p>
<p><strong>Query 向量，Key 向量，Value 向量是什么含义呢</strong>？</p>
<h4 id="计算-Attention-Score（注意力分数）"><a href="#计算-Attention-Score（注意力分数）" class="headerlink" title="计算 Attention Score（注意力分数）"></a>计算 Attention Score（注意力分数）</h4><p>第 2 步，是计算 Attention Score（注意力分数）。假设我们现在计算第一个词 Thinking 的 Attention Score（注意力分数），<strong>需要根据 Thinking 这个词，对句子中的其他每个词都计算一个分数</strong>。这些分数决定了我们在编码Thinking这个词时，需要对句子中其他位置的每个词放置多少的注意力。</p>
<p>这些分数，是通过计算 “Thinking” 对应的 Query 向量和其他位置的每个词的 Key 向量的点积，而得到的。如果我们计算句子中第一个位置单词的 Attention Score（注意力分数），那么第一个分数就是 q1 和 k1 的内积，第二个分数就是 q1 和 k2 的点积。</p>
<p><img src="2-think.png" alt="Thinking计算"><br>图：Thinking计算</p>
<p>第 3 步就是把每个分数除以 $\sqrt(d_{key})$ （$d_{key}$是 Key 向量的长度）。你也可以除以其他数，除以一个数是为了在反向传播时，求取梯度更加稳定。</p>
<p>第 4 步，接着把这些分数经过一个 Softmax 层，Softmax可以将分数归一化，这样使得分数都是正数并且加起来等于 1。</p>
<p><img src="2-think2.png" alt="Thinking计算"><br>图：Thinking计算</p>
<p>这些分数决定了在编码当前位置（这里的例子是第一个位置）的词时，对所有位置的词分别有多少的注意力。很明显，在上图的例子中，当前位置（这里的例子是第一个位置）的词会有最高的分数，但有时，关注到其他位置上相关的词也很有用。</p>
<p>第 5 步，得到每个位置的分数后，将每个分数分别与每个 Value 向量相乘。这种做法背后的直觉理解就是：对于分数高的位置，相乘后的值就越大，我们把更多的注意力放到了它们身上；对于分数低的位置，相乘后的值就越小，这些位置的词可能是相关性不大的，这样我们就忽略了这些位置的词。</p>
<p>第 6 步是把上一步得到的向量相加，就得到了 Self Attention 层在这个位置（这里的例子是第一个位置）的输出。</p>
<p><img src="2-sum.png" alt="Think计算"><br>图：Think计算</p>
<p>上面这张图，包含了 Self Attention 的全过程，最终得到的当前位置（这里的例子是第一个位置）的向量会输入到前馈神经网络。但这样每次只能计算一个位置的输出向量，在实际的代码实现中，Self Attention 的计算过程是使用矩阵来实现的，这样可以加速计算，一次就得到所有位置的输出向量。下面让我们来看，如何使用矩阵来计算所有位置的输出向量。</p>
<h4 id="使用矩阵计算Self-Attention"><a href="#使用矩阵计算Self-Attention" class="headerlink" title="使用矩阵计算Self-Attention"></a>使用矩阵计算Self-Attention</h4><p>第一步是计算 Query，Key，Value 的矩阵。首先，我们把所有词向量放到一个矩阵 X 中，然后分别和3 个权重矩阵$W^Q, W^K W^V$ 相乘，得到 Q，K，V 矩阵。</p>
<p><img src="2-qkv-multi.png">图：QKV矩阵乘法</p>
<p>?????? WQ\WK矩阵都是怎么来的</p>
<p>矩阵 X 中的每一行，表示句子中的每一个词的词向量，长度是 512。Q，K，V 矩阵中的每一行表示 Query 向量，Key 向量，Value 向量，向量长度是 64。</p>
<p>接着，由于我们使用了矩阵来计算，我们可以把上面的第 2 步到第 6 步压缩为一步，直接得到 Self Attention 的输出。</p>
<p><img src="2-attention-output.webp" alt="输出"><br>图：输出</p>
<h4 id="多头注意力机制（multi-head-attention）"><a href="#多头注意力机制（multi-head-attention）" class="headerlink" title="多头注意力机制（multi-head attention）"></a>多头注意力机制（multi-head attention）</h4><p>Transformer 的论文通过增加多头注意力机制（一组注意力称为一个 attention head），进一步完善了 Self Attention 层。这种机制从如下两个方面增强了 attention 层的能力：</p>
<ul>
<li>它扩展了模型关注不同位置的能力。</li>
<li>多头注意力机制赋予 attention 层多个“子表示空间”。</li>
</ul>
<p>接下来就有点麻烦了，因为前馈神经网络层接收的是 1 个矩阵（其中每行的向量表示一个词），而不是 8 个矩阵。所以我们需要一种方法，把 8 个矩阵整合为一个矩阵。</p>
<p>怎么才能做到呢？我们把矩阵拼接起来，然后和另一个权重矩阵$W^Q$相乘。</p>
<p><img src="2-to1.webp" alt="整合矩阵"><br>图：整合矩阵</p>
<ol>
<li>把 8 个矩阵 {Z0,Z1…,Z7} 拼接起来</li>
<li>把拼接后的矩阵和 WO 权重矩阵相乘</li>
<li>得到最终的矩阵 Z，这个矩阵包含了所有 attention heads（注意力头） 的信息。这个矩阵会输入到 FFNN (Feed Forward Neural Network)层。</li>
</ol>
<p>这就是多头注意力的全部内容。我知道，在上面的讲解中，出现了相当多的矩阵。下面我把所有的内容都放到一张图中，这样你可以总揽全局，在这张图中看到所有的内容。</p>
<p><img src="2-put-together.webp" alt="放在一起"><br>图：放在一起</p>
<p>既然我们已经谈到了多头注意力，现在让我们重新回顾之前的翻译例子，看下当我们编码单词it时，不同的 attention heads （注意力头）关注的是什么部分。</p>
<p><img src="2-it-attention.webp" alt="`it`的attention"><br>图：<code>it</code>的attention</p>
<p>当我们编码单词”it”时，其中一个 attention head （注意力头）最关注的是”the animal”，另外一个 attention head 关注的是”tired”。因此在某种意义上，”it”在模型中的表示，融合了”animal”和”word”的部分表达。</p>
<p>然而，当我们把所有 attention heads（注意力头） 都在图上画出来时，多头注意力又变得难以解释了。</p>
<p><img src="2-all-att.png" alt="所有注意力heads"><br>图：所有注意力heads</p>
<h2 id="代码实现矩阵计算-Attention"><a href="#代码实现矩阵计算-Attention" class="headerlink" title="代码实现矩阵计算 Attention"></a>代码实现矩阵计算 Attention</h2><p>需要注意的是：在前面的讲解中，我们的 K、Q、V 矩阵的序列长度都是一样的。但是在实际中，K、V 矩阵的序列长度是一样的，而 Q 矩阵的序列长度可以不一样。</p>
<p>这种情况发生在：在解码器部分的Encoder-Decoder Attention层中，Q 矩阵是来自解码器下层，而 K、V 矩阵则是来自编码器的输出。</p>
<p><img src="2-encoder-decoder.gif" alt="encoder-decoder动态图"><br>动态图：encoder-decoder动态图</p>
<p>在完成了编码（encoding）阶段之后，我们开始解码（decoding）阶段。解码（decoding ）阶段的每一个时间步都输出一个翻译后的单词（这里的例子是英语翻译）。</p>
<p>输出是：</p>
<ul>
<li>attn_output：形状是 (L,N,E)</li>
<li>attn_output_weights：形状是 (N,L,S)<br>代码示例如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">## nn.MultiheadAttention 输入第0维为length</span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br><span class="line">query &#x3D; torch.rand(12,64,300)</span><br><span class="line"># batch_size 为 64，有 10 个词，每个词的 Key 向量是 300 维</span><br><span class="line">key &#x3D; torch.rand(10,64,300)</span><br><span class="line"># batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维</span><br><span class="line">value&#x3D; torch.rand(10,64,300)</span><br><span class="line"></span><br><span class="line">embed_dim &#x3D; 299</span><br><span class="line">num_heads &#x3D; 1</span><br><span class="line"># 输出是 (attn_output, attn_output_weights)</span><br><span class="line">multihead_attn &#x3D; nn.MultiheadAttention(embed_dim, num_heads)</span><br><span class="line">attn_output &#x3D; multihead_attn(query, key, value)[0]</span><br><span class="line"># output: torch.Size([12, 64, 300])</span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的向量是 300 维</span><br><span class="line">print(attn_output.shape)</span><br></pre></td></tr></table></figure>

<h3 id="手动实现计算-Attention"><a href="#手动实现计算-Attention" class="headerlink" title="手动实现计算 Attention"></a>手动实现计算 Attention</h3><p>在 PyTorch 提供的 MultiheadAttention  中，第 1 维是句子长度，第 2 维是 batch size。这里我们的代码实现中，第 1 维是 batch size，第 2 维是句子长度。代码里也包括：如何用矩阵实现多组注意力的并行计算。代码中已经有详细注释和说明。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">class MultiheadAttention(nn.Module):</span><br><span class="line">    # n_heads：多头注意力的数量</span><br><span class="line">    # hid_dim：每个词输出的向量维度</span><br><span class="line">    def __init__(self, hid_dim, n_heads, dropout):</span><br><span class="line">        super(MultiheadAttention, self).__init__()</span><br><span class="line">        self.hid_dim &#x3D; hid_dim</span><br><span class="line">        self.n_heads &#x3D; n_heads</span><br><span class="line"></span><br><span class="line">        # 强制 hid_dim 必须整除 h</span><br><span class="line">        assert hid_dim % n_heads &#x3D;&#x3D; 0</span><br><span class="line">        # 定义 W_q 矩阵</span><br><span class="line">        self.w_q &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        # 定义 W_k 矩阵</span><br><span class="line">        self.w_k &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        # 定义 W_v 矩阵</span><br><span class="line">        self.w_v &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.do &#x3D; nn.Dropout(dropout)</span><br><span class="line">        # 缩放</span><br><span class="line">        self.scale &#x3D; torch.sqrt(torch.FloatTensor([hid_dim &#x2F;&#x2F; n_heads]))</span><br><span class="line"></span><br><span class="line">    def forward(self, query, key, value, mask&#x3D;None):</span><br><span class="line">        # K: [64,10,300], batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br><span class="line">        # V: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span><br><span class="line">        # Q: [64,12,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span><br><span class="line">        # bsz： batch size</span><br><span class="line">        bsz &#x3D; query.shape[0]</span><br><span class="line">        Q &#x3D; self.w_q(query)</span><br><span class="line">        K &#x3D; self.w_k(key)</span><br><span class="line">        V &#x3D; self.w_v(value)</span><br><span class="line">        # 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵</span><br><span class="line">        # 最后一维就是是用 self.hid_dim &#x2F;&#x2F; self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300&#x2F;6&#x3D;50</span><br><span class="line">        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度</span><br><span class="line">        # K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]</span><br><span class="line">        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算</span><br><span class="line">        Q &#x3D; Q.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        K &#x3D; K.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        V &#x3D; V.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line"></span><br><span class="line">        # 第 1 步：Q 乘以 K的转置，除以scale</span><br><span class="line">        # [64,6,12,50] * [64,6,50,10] &#x3D; [64,6,12,10]</span><br><span class="line">        # attention：[64,6,12,10]</span><br><span class="line">        attention &#x3D; torch.matmul(Q, K.permute(0, 1, 3, 2)) &#x2F; self.scale</span><br><span class="line"></span><br><span class="line">        # 把 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10</span><br><span class="line">        if mask isnotNone:</span><br><span class="line">            attention &#x3D; attention.masked_fill(mask &#x3D;&#x3D; 0, -1e10)</span><br><span class="line"></span><br><span class="line">        # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。</span><br><span class="line">        # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax</span><br><span class="line">        # attention: [64,6,12,10]</span><br><span class="line">        attention &#x3D; self.do(torch.softmax(attention, dim&#x3D;-1))</span><br><span class="line"></span><br><span class="line">        # 第三步，attention结果与V相乘，得到多头注意力的结果</span><br><span class="line">        # [64,6,12,10] * [64,6,10,50] &#x3D; [64,6,12,50]</span><br><span class="line">        # x: [64,6,12,50]</span><br><span class="line">        x &#x3D; torch.matmul(attention, V)</span><br><span class="line"></span><br><span class="line">        # 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果</span><br><span class="line">        # x: [64,6,12,50] 转置-&gt; [64,12,6,50]</span><br><span class="line">        x &#x3D; x.permute(0, 2, 1, 3).contiguous()</span><br><span class="line">        # 这里的矩阵转换就是：把多组注意力的结果拼接起来</span><br><span class="line">        # 最终结果就是 [64,12,300]</span><br><span class="line">        # x: [64,12,6,50] -&gt; [64,12,300]</span><br><span class="line">        x &#x3D; x.view(bsz, -1, self.n_heads * (self.hid_dim &#x2F;&#x2F; self.n_heads))</span><br><span class="line">        x &#x3D; self.fc(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br><span class="line">query &#x3D; torch.rand(64, 12, 300)</span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维</span><br><span class="line">key &#x3D; torch.rand(64, 10, 300)</span><br><span class="line"># batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维</span><br><span class="line">value &#x3D; torch.rand(64, 10, 300)</span><br><span class="line">attention &#x3D; MultiheadAttention(hid_dim&#x3D;300, n_heads&#x3D;6, dropout&#x3D;0.1)</span><br><span class="line">output &#x3D; attention(query, key, value)</span><br><span class="line">## output: torch.Size([64, 12, 300])</span><br><span class="line">print(output.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="关键代码"><a href="#关键代码" class="headerlink" title="关键代码"></a>关键代码</h3><p>其中用矩阵实现多头注意力的关键代码如下所示， K、Q、V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵</span><br><span class="line">        # 最后一维就是是用 self.hid_dim &#x2F;&#x2F; self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300&#x2F;6&#x3D;50</span><br><span class="line">        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 个词，50 表示每组注意力的词的向量长度</span><br><span class="line">        # K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]</span><br><span class="line">        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算</span><br><span class="line">        Q &#x3D; Q.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        K &#x3D; K.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        V &#x3D; V.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">经过 attention 计算得到 x 的形状是 &#96;[64,12,6,50]&#96;，64 表示 batch size，6 表示有 6组注意力，10 表示有 10 个词，50 表示每组注意力的词的向量长度。把这个矩阵转换为 &#96;[64,12,300]&#96;的矩阵，就是相当于把多组注意力的结果拼接起来。</span><br><span class="line">e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e ee</span><br><span class="line"></span><br><span class="line">这里的矩阵转换就是：把多组注意力的结果拼接起来，最终结果就是 [64,12,300]，x: [64,12,6,50] -&gt; [64,12,300]</span><br><span class="line">x &#x3D; x.view(bsz, -1, self.n_heads * (self.hid_dim &#x2F;&#x2F; self.n_heads))</span><br></pre></td></tr></table></figure>

<h2 id="使用位置编码来表示序列的顺序"><a href="#使用位置编码来表示序列的顺序" class="headerlink" title="使用位置编码来表示序列的顺序"></a>使用位置编码来表示序列的顺序</h2><p>到目前为止，我们阐述的模型中缺失了一个东西，那就是表示序列中单词顺序的方法。</p>
<p>为了解决这个问题，Transformer 模型对每个输入的向量都添加了一个向量。这些向量遵循模型学习到的特定模式，有助于确定每个单词的位置，或者句子中不同单词之间的距离。这种做法背后的直觉是：将这些表示位置的向量添加到词向量中，得到了新的向量，这些新向量映射到 Q/K/V，然后计算点积得到 attention 时，可以提供有意义的信息。</p>
<p><img src="2-position.png" alt="位置编码"><br>图：位置编码</p>
<p>为了让模型了解单词的顺序，我们添加了带有位置编码的向量–这些向量的值遵循特定的模式。 如果我们假设词向量的维度是 4，那么带有位置编码的向量可能如下所示：</p>
<p><img src="2-position2.png" alt="位置编码"><br>图：位置编码</p>
<p>位置编码的可视化分析：</p>
<p>在下图中，每一行表示一个带有位置编码的向量。所以，第一行对应于序列中第一个单词的位置编码向量。每一行都包含 512 个值，每个值的范围在 -1 和 1 之间。我对这些向量进行了涂色可视化，你可以从中看到向量遵循的模式。</p>
<p><img src="2-position3.png" alt="位置编码图示"><br>图：位置编码图示</p>
<p>这是一个真实的例子，包含了 20 个词，每个词向量的维度是 512。你可以看到，它看起来像从中间一分为二。这是因为左半部分的值是由<strong>sine</strong>函数产生的，而右半部分的值是由<strong>cosine</strong>函数产生的，然后将他们拼接起来，得到每个位置编码向量。</p>
<p>你可以在get_timing_signal_1d()上查看生成位置编码的代码。这种方法来自于<code>Tranformer2Transformer</code> 的实现。</p>
<p>而论文中的方法和上面图中的稍有不同，它不是直接拼接两个向量，而是将两个向量交织在一起。如下图所示。</p>
<p><img src="2-positin4.png" alt="位置编码交织"><br>图：位置编码交织</p>
<p>此为生成位置编码的公式，在 Transformer 论文的 3.5 节中有详细说明。</p>
<p>这不是唯一一种生成位置编码的方法。但这种方法的优点是：可以扩展到未知的序列长度。例如：当我们的模型需要翻译一个句子，而这个句子的长度大于训练集中所有句子的长度，这时，这种位置编码的方法也可以生成一样长的位置编码向量。</p>
<h3 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h3><p>在我们继续讲解之前，编码器结构中有一个需要注意的细节是：编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization）。</p>
<p><img src="2-resnet.png" alt="残差连接"><br>图：残差连接</p>
<p>将 Self-Attention 层的层标准化（layer-normalization）和向量都进行可视化，如下所示：</p>
<p><img src="2-lyn.png" alt="标准化"><br>图：标准化</p>
<p>在解码器的子层里面也有层标准化（layer-normalization）。假设一个 Transformer 是由 2 层编码器和两层解码器组成的，如下图所示。</p>
<p><img src="2-2layer.png" alt="2层示意图"><br>图：2层示意图</p>
<h3 id="Decoder（解码器）"><a href="#Decoder（解码器）" class="headerlink" title="Decoder（解码器）"></a>Decoder（解码器）</h3><p>现在我们已经介绍了解码器中的大部分概念，我们也基本知道了解码器的原理。现在让我们来看下， 编码器和解码器是如何协同工作的。</p>
<p>上面说了，编码器一般有多层，第一个编码器的输入是一个序列，最后一个编码器输出是一组注意力向量 K 和 V。这些注意力向量将会输入到每个解码器的Encoder-Decoder Attention层，这有助于解码器把注意力集中中输入序列的合适位置。</p>
<p>在完成了编码（encoding）阶段之后，我们开始解码（decoding）阶段。解码（decoding ）阶段的每一个时间步都输出一个翻译后的单词（这里的例子是英语翻译）。</p>
<p>接下来会重复这个过程，直到输出一个结束符，Transformer 就完成了所有的输出。每一步的输出都会在下一个时间步输入到下面的第一个解码器。Decoder 就像 Encoder 那样，从下往上一层一层地输出结果。正对如编码器的输入所做的处理，我们把解码器的输入向量，也加上位置编码向量，来指示每个词的位置。</p>
<p><img src="2-decoder.gif" alt="decoder动态图"><br>动态图：decoder动态图</p>
<p>解码器中的 Self Attention 层，和编码器中的 Self Attention 层不太一样：<strong>在解码器里，Self Attention 层只允许关注到输出序列中早于当前位置之前的单词</strong>。具体做法是：在 Self Attention 分数经过 Softmax 层之前，屏蔽当前位置之后的那些位置。</p>
<p>Encoder-Decoder Attention层的原理和多头注意力（multiheaded Self Attention）机制类似，<strong>不同之处</strong>是：Encoder-Decoder Attention层是使用前一层的输出来构造 Query 矩阵，而 Key 矩阵和 Value 矩阵来自于解码器最终的输出。</p>
<h3 id="最后的线性层和-Softmax-层"><a href="#最后的线性层和-Softmax-层" class="headerlink" title="最后的线性层和 Softmax 层"></a>最后的线性层和 Softmax 层</h3><p>Decoder 最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是由 Softmax 层后面的线性层来完成的。</p>
<p><strong>线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更长的向量，这个向量称为 logits 向量</strong>。</p>
<p><strong>Softmax 层会把这些分数转换为概率（把所有的分数转换为正数，并且加起来等于 1）。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词</strong>。</p>
<p><img src="2-linear.png" alt="线性层"><br>图：线性层</p>
<p>在上图中，最下面的向量，就是编码器的输出，这个向量输入到线性层和 Softmax 层，最终得到输出的词。</p>
<h3 id="Transformer-的训练过程"><a href="#Transformer-的训练过程" class="headerlink" title="Transformer 的训练过程"></a>Transformer 的训练过程</h3><p>现在我们已经了解了 Transformer 的前向传播过程，下面讲讲 Transformer 的训练过程，这也是非常有用的知识。</p>
<p>在训练过程中，模型会经过上面讲的所有前向传播的步骤。但是，当我们在一个标注好的数据集上训练这个模型的时候，我们可以对比模型的输出和真实的标签。</p>
<p>为了可视化这个对比，让我们假设输出词汇表只包含 6 个单词（“a”, “am”, “i”, “thanks”, “student”, and “<eos>”（“<eos>”表示句子末尾））。</p>
<p><img src="2-6words.webp" alt="6个词"><br>图：6个词</p>
<p>我们模型的输出词汇表，是在训练之前的数据预处理阶段构造的。当我们确定了输出词汇表，我们可以用向量来表示词汇表中的每个单词。这个表示方法也称为  one-hot encoding。例如，我们可以把单词 “am” 用下面的向量来表示：</p>
<p><img src="2-am.webp" alt="am向量"><br>图：am向量</p>
<p>介绍了训练过程，我们接着讨论模型的损失函数，这我们在训练时需要优化的目标，通过优化这个目标来得到一个训练好的、非常精确的模型。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>用一个简单的例子来说明训练过程，比如：把“merci”翻译为“thanks”。</p>
<p>这意味着我们希望模型最终输出的概率分布，会指向单词 ”thanks“（在“thanks”这个词的概率最高）。但模型还没训练好，它输出的概率分布可能和我们希望的概率分布相差甚远。</p>
<p><img src="2-loss.webp" alt="概率分布"><br>图：概率分布</p>
<p>由于模型的参数都是随机初始化的。模型在每个词输出的概率都是随机的。我们可以把这个概率和正确的输出概率做对比，然后使用反向传播来调整模型的权重，使得输出的概率分布更加接近正确(pq)输出。</p>
<p>那我们要怎么比较两个概率分布呢？我们可以简单地用一个概率分布减去另一个概率分布。关于更多细节，你可以查看<strong>交叉熵</strong>(cross-entropy)]和<strong>KL 散度</strong>(Kullback–Leibler divergence)的相关概念。</p>
<p>但上面的例子是经过简化的，因为我们的句子只有一个单词。在实际中，我们使用的句子不只有一个单词。例如–输入是：“je suis étudiant” ，输出是：“i am a student”。这意味着，我们的模型需要输出多个概率分布，满足如下条件：</p>
<ul>
<li>每个概率分布都是一个向量，长度是 vocab_size（我们的例子中，向量长度是 6，但实际中更可能是 30000 或者 50000）</li>
<li>第一个概率分布中，最高概率对应的单词是 “i”</li>
<li>第二个概率分布中，最高概率对应的单词是 “am”</li>
<li>以此类推，直到第 5 个概率分布中，最高概率对应的单词是 “&lt;eos&gt;(pq)”，表示没有下一个单词了</li>
</ul>
<p><img src="2-target.png" alt="概率分布"><br>图：概率分布</p>
<p>我们用例子中的句子训练模型，希望产生图中所示的概率分布<br>我们的模型在一个足够大的数据集上，经过足够长时间的训练后，希望输出的概率分布如下图所示：</p>
<p><img src="2-trained.webp" alt="训练后概率分布"><br>图：训练后概率分布</p>
<p>希望经过训练，模型会输出我们希望的正确翻译。当然，如果你要翻译的句子是训练集中的一部分，那输出的结果并不能说明什么。我们希望的是模型在没见过的句子上也能够准确翻译。需要注意的是：概率分布向量中，每个位置都会有一点概率，即使这个位置不是输出对应的单词–这是 Softmax 中一个很有用的特性，有助于帮助训练过程。</p>
<p>现在，由于模型每个时间步只产生一个输出，我们可以认为：模型是从概率分布中选择概率最大的词，并且丢弃其他词。这种方法叫做<strong>贪婪解码</strong>（greedy decoding）。</p>
<p>另一种方法是每个时间步保留两个最高概率的输出词，然后在下一个时间步，重复执行这个过程：假设第一个位置概率最高的两个输出的词是”I“和”a“，这两个词都保留，然后根据第一个词计算第二个位置的词的概率分布，再取出 2 个概率最高的词，对于第二个位置和第三个位置，我们也重复这个过程。这种方法称为<strong>集束搜索</strong>(beam search)，在我们的例子中，beam_size 的值是 2（含义是：在所有时间步，我们保留两个最高概率），top_beams 的值也是 2（表示我们最终会返回两个翻译的结果）。beam_size 和 top_beams 都是你可以在实验中尝试的超参数。</p>
<h3 id="更进一步理解"><a href="#更进一步理解" class="headerlink" title="更进一步理解"></a>更进一步理解</h3><p>我希望上面讲的内容，可以帮助你理解 Transformer 中的主要概念。如果你想更深一步地理解，我建议你可以参考下面这些：</p>
<ul>
<li>阅读 Transformer 的论文：<br>《Attention Is All You Need》<br>链接地址：<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
<li>阅读Transformer 的博客文章：<br>《Transformer: A Novel Neural Network Architecture for Language Understanding》<br>链接地址：<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a><br>阅读《Tensor2Tensor announcement》</li>
<li>链接地址：<a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html">https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html</a></li>
<li>观看视频 【Łukasz Kaiser’s talk】来理解模型和其中的细节<br>链接地址：<a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">https://www.youtube.com/watch?v=rBCqOTEfxvg</a><br>运行这份代码：【Jupyter Notebook provided as part of the Tensor2Tensor repo】</li>
<li>链接地址：<a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb%E3%80%82">https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb。</a></li>
<li>查看这个项目：【Tensor2Tensor repo】<br>链接地址：<a href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a></li>
</ul>
<h3 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h3><p>主要由哈尔滨工业大学张贤同学翻译撰写，由本项目同学组织和整理。最后，期待您的阅读反馈和star哦，谢谢。</p>
<h3 id="转载并编辑"><a href="#转载并编辑" class="headerlink" title="转载并编辑"></a>转载并编辑</h3><p>童茗 整理笔记</p>
<p>编码器是并行的，但是解码器和RNN一样不能并行。</p>
]]></content>
      
        
        <tags>
            
            <tag> DataWhale-NLP-Transformer </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title></title>
      <url>/2021/08/18/Task02%20%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<h1 id="Task02-消息传递图神经网络"><a href="#Task02-消息传递图神经网络" class="headerlink" title="Task02 消息传递图神经网络"></a>Task02 消息传递图神经网络</h1><h2 id="笔记部分"><a href="#笔记部分" class="headerlink" title="笔记部分"></a>笔记部分</h2><ul>
<li><p>消息传递范式</p>
<ul>
<li><p>用$\mathbf{x}^{(k-1)}<em>i\in\mathbb{R}^F$表示$(k-1)$层中节点$i$的节点特征，$\mathbf{e}</em>{j,i} \in \mathbb{R}^D$ 表示从节点$j$到节点$i$的边的特征，消息传递图神经网络可以描述为</p>
</li>
<li><p>$$<br>\mathbf{x}_i^{(k)} = \gamma^{(k)} \left( \mathbf{x}<em>i^{(k-1)}, \square</em>{j \in \mathcal{N}(i)} , \phi^{(k)}\left(\mathbf{x}_i^{(k-1)}, \mathbf{x}<em>j^{(k-1)},\mathbf{e}</em>{j,i}\right) \right),<br>$$</p>
<p><img src="file://C:\Users\qinan\Desktop\team-study\team-learning-nlp\GNN\Markdown%E7%89%88%E6%9C%AC\images\image-20210516110407207.png?lastModify=1624111214" alt="节点嵌入（Node Embedding）"></p>
<p>图片展示了<strong>基于消息传递范式的生成节点表征的过程</strong>：</p>
<ol>
<li>在图的最右侧，B节点的邻接节点（A,C）的信息传递给了B，经过信息变换得到了B的嵌入，C、D节点同。</li>
<li>在图的中右侧，A节点的邻接节点（B,C,D）的之前得到的节点嵌入传递给了节点A；在图的中左侧，聚合得到的信息经过信息变换得到了A节点新的嵌入。</li>
<li>重复多次，我们可以得到每一个节点的经过多次信息变换的嵌入。这样的经过多次信息聚合与变换的节点嵌入就可以作为节点的表征，可以用于节点的分类。</li>
</ol>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>继承<code>MessagePassing</code>类的<code>GCNConv</code></p>
<ul>
<li>GCNConv的数学定义为</li>
</ul>
<p>$$<br>\mathbf{x}<em>i^{(k)} = \sum</em>{j \in \mathcal{N}(i) \cup { i }} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right),<br>$$</p>
<ul>
<li><p>其中，相邻节点的特征首先通过权重矩阵$\mathbf{\Theta}$进行转换，然后按端点的度进行归一化处理，最后进行加总。</p>
</li>
<li><p>步骤细分：</p>
</li>
</ul>
<ol>
<li>向邻接矩阵添加自环边。</li>
<li>线性转换节点特征矩阵。</li>
<li>计算归一化系数。</li>
<li>归一化$j$中的节点特征。</li>
<li>将相邻节点特征相加（”求和 “聚合）。</li>
</ol>
</li>
</ul>
<ul>
<li><p>作业</p>
<ul>
<li><p>MessagePassing的运行流程：</p>
<ul>
<li>通过线性变换以及利用归一化系数将源节点往目标节点传递特征</li>
<li>通过多种方式（max、average、sum）对特征进行聚合</li>
<li>将聚合后的信息再次进行转换</li>
</ul>
</li>
<li><p>继承<code>MessagePassing</code>类的规范,并请继承<code>MessagePassing</code>类来自定义几个的图神经网络类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> MessagePassing</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyGNN</span>(<span class="params">MessagePassing</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \mathbf&#123;x&#125;^&#123;\prime&#125;_i = \mathbf&#123;x&#125;_i \cdot \mathbf&#123;\Theta&#125;_1 +</span></span><br><span class="line"><span class="string">        \sum_&#123;j \in \mathcal&#123;N&#125;(i)&#125; e_&#123;j,i&#125; \cdot</span></span><br><span class="line"><span class="string">        (\mathbf&#123;\Theta&#125;_2 \mathbf&#123;x&#125;_i - \mathbf&#123;\Theta&#125;_3 \mathbf&#123;x&#125;_j)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, device</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyGNN, self).__init__(aggr=<span class="string">&#x27;add&#x27;</span>)</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line"></span><br><span class="line">        self.lin1 = torch.nn.Linear(in_channels, out_channels).to(device)</span><br><span class="line">        self.lin2 = torch.nn.Linear(in_channels, out_channels).to(device)</span><br><span class="line">        self.lin3 = torch.nn.Linear(in_channels, out_channels).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        a = self.lin1(x)</span><br><span class="line">        b = self.lin2(x)</span><br><span class="line">        out = self.propagate(edge_index, a=a, b=b)</span><br><span class="line">        <span class="keyword">return</span> self.lin3(x) + out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">message</span>(<span class="params">self, a_i, b_j</span>):</span></span><br><span class="line">        out = a_i - b_j</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#123;&#125;(&#123;&#125;, &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(self.__class__.__name__, self.in_channels,</span><br><span class="line">                                   self.out_channels)</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dataset = Planetoid(root=<span class="string">&#x27;dataset/Cora&#x27;</span>, name=<span class="string">&#x27;Cora&#x27;</span>)</span><br><span class="line">model = MyGNN(in_channels=dataset.num_features, out_channels=dataset.num_classes, device=device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line">data = dataset[<span class="number">0</span>].to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out = model(data.x, data.edge_index).to(device)</span><br><span class="line">    pred = out.argmax(dim=<span class="number">1</span>)</span><br><span class="line">    accuracy = <span class="built_in">int</span>((pred[data.test_mask] == data.y[data.test_mask]).<span class="built_in">sum</span>()) / data.test_mask.<span class="built_in">sum</span>()</span><br><span class="line">    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Train Epoch: &#123;:3&#125; Accuracy: &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(epoch, accuracy.item() * <span class="number">100.0</span>))</span><br><span class="line">        </span><br><span class="line">参考自@天国之影 http://relph.gitee.io/my-team-learning/<span class="comment">#/gnn_learning26/task02</span></span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>/2021/08/18/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title></title>
      <url>/2021/08/18/AI%E4%BA%A4%E5%8F%89/</url>
      <content type="html"></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[知识图谱笔记]]></title>
      <url>/2021/04/16/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h1><ul>
<li><a href="https://github.com/km1994/NLP-Interview-Notes/tree/main/NLPinterview/KG">https://github.com/km1994/NLP-Interview-Notes/tree/main/NLPinterview/KG</a></li>
</ul>
<ol>
<li>什么是schema？<br> 即是给出特定的数据格式，如实体的属性只有年龄 姓名，那么加入此知识图谱的实体就必须符合这个要求，加入如“性别”就不行。</li>
<li>信息抽取的难点在哪里？<br> 处理非结构化数据，抽取出结构化数据（实体和关系）</li>
<li>构建KG所涉及到的技术<ul>
<li>NER（实体命名识别）<ul>
<li>目标：从文本中提取出实体并对每个实体做“分类/打标签”<br>如“Virgil’s BBQ”提取出并标记上“Restaurant”</li>
<li>目前技术已经比较成熟</li>
</ul>
</li>
<li>关系抽取（Relation Extaction）</li>
<li>实体对齐（Entity Resolution **）<ul>
<li>如NYC和new york city指得同一个东西需要对齐</li>
</ul>
</li>
<li>指代消解（Coreference Resolution）<br>“it”指代的哪个名词</li>
</ul>
</li>
<li>KG的存储和查询<ul>
<li>知识图谱并不一定用图数据库（PDB），数据库只是一种存储形式，特定场景可以用不同数据库（如不用扩展的可以用关系型数据库）</li>
<li>eg： 查 Bob’s friend’s friend’s friend<ul>
<li>若使用关系型数据库，就要一直做joint，查询时间指数级增长</li>
<li>但是若使用图数据库，查询时间只会线性增长</li>
</ul>
</li>
<li>PDB图数据库的好处：<ul>
<li>表达更接近于自然语言</li>
<li>易扩展</li>
<li>性能好</li>
</ul>
</li>
</ul>
</li>
</ol>
]]></content>
      
        
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[hexo调试日志]]></title>
      <url>/2021/03/24/hexo%E8%B0%83%E8%AF%95%E6%97%A5%E5%BF%97/</url>
      <content type="html"><![CDATA[<ol>
<li>hexo d出错 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Error: Spawn failed at ChildProcess.&lt;anonymous&gt; (E:\blog\node_modules\hexo-</span><br><span class="line">util\lib\spawn.js:51:21) at ChildProcess.emit (events.js:210:5) at ChildProcess.cp.emit (E:\blog\node_modules\cross-</span><br><span class="line">spawn\lib\enoent.js:34:29) at Process.ChildProcess._handle.onexit (internal&#x2F;child_process.js:272:12)</span><br></pre></td></tr></table></figure>
<ul>
<li>解决办法：</li>
</ul>
<ol>
<li>删除.deploy_git文件</li>
<li>依次执行<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dexo clean</span><br><span class="line">dexo g</span><br><span class="line">dexo d</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
]]></content>
      
        
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[调试日志]]></title>
      <url>/2021/03/24/python%E8%B0%83%E8%AF%95%E6%97%A5%E5%BF%97/</url>
      <content type="html"><![CDATA[<hr>
<ol>
<li><p>如果函数def中没有return，只有print，如果再在引用时使用print，那么最后会有none输出。</p>
</li>
<li><p>print不换行加  end=“”</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;....&quot;, end&#x3D;&quot;&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>计算字符串中各字母出现的次数：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def get_frequency_dict(sequence):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Returns a dictionary where the keys are elements of the sequence</span><br><span class="line">    and the values are integer counts, for the number of times that</span><br><span class="line">    an element is repeated in the sequence.</span><br><span class="line"></span><br><span class="line">    sequence: string or list</span><br><span class="line">    return: dictionary</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # freqs: dictionary (element_type -&gt; int)</span><br><span class="line">    freq &#x3D; &#123;&#125;</span><br><span class="line">    for x in sequence:</span><br><span class="line">        freq[x] &#x3D; freq.get(x,0) + 1</span><br><span class="line">    return freq</span><br></pre></td></tr></table></figure></li>
<li><p><strong>dict.get(‘a’,0) and dict[‘a’]</strong>:usual way to access a value is hand[‘a’], where ‘a’<br>​is the key we want to find. However,this only works if the key is in the dictionary; otherwise, we get a KeyError ​. To avoid this, we can instead use the function call hand.get(‘a’,0)</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get(key, [default])</span><br><span class="line">Return the value for key if key is in the dictionary, else default. If default is not given, it defaults to None, so that this method never raises a KeyError.</span><br></pre></td></tr></table></figure></li>
<li><p>大小写转换</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">str &#x3D; &quot;www.runoob.com&quot;</span><br><span class="line">print(str.upper())          # 把所有字符中的小写字母转换成大写字母</span><br><span class="line">print(str.lower())          # 把所有字符中的大写字母转换成小写字母</span><br></pre></td></tr></table></figure></li>
<li><p>在字典遍历过程中修改字典元素，报错  <strong>得知遍历时不能修改字典元素</strong></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for k in func_dict.keys():</span><br><span class="line">    if func_dict[k] is np.nan:</span><br><span class="line">        del func_dict[k]</span><br><span class="line">        continue</span><br><span class="line">RuntimeError: dictionary changed size during iteration</span><br></pre></td></tr></table></figure>
<p> <strong>解决办法：将遍历条件改为列表</strong></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for k in list(func_dict.keys()):</span><br><span class="line">if func_dict[k] is np.nan:</span><br><span class="line">    del func_dict[k]</span><br><span class="line">    continue</span><br></pre></td></tr></table></figure></li>
<li><p>dict.copy()才是复制，而等于只是引用而已,字符串复制直接可以用等号</p>
</li>
<li><p>How can I add new keys to a dictionary?</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d &#x3D; &#123;&#39;key&#39;: &#39;value&#39;&#125;</span><br><span class="line">print(d)  # &#123;&#39;key&#39;: &#39;value&#39;&#125;</span><br><span class="line"></span><br><span class="line">d[&#39;mynewkey&#39;] &#x3D; &#39;mynewvalue&#39;</span><br><span class="line"></span><br><span class="line">print(d)  # &#123;&#39;key&#39;: &#39;value&#39;, &#39;mynewkey&#39;: &#39;mynewvalue&#39;&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>input() can only has one parameter</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nilai_tinggi &#x3D; int(input(&quot;Enter the height of children number &quot;, str(A)))</span><br><span class="line">TypeError: raw_input() takes from 1 to 2 positional arguments but 3 were given</span><br></pre></td></tr></table></figure>
<p> 解决方法：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nilai_tinggi &#x3D; int(input(&quot;Enter the height of children number %d&quot; %A))</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      
        
        <tags>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hello, World Article!]]></title>
      <url>/2021/03/24/Hello-World-Article/</url>
      <content type="html"><![CDATA[<h1 id="hello-word"><a href="#hello-word" class="headerlink" title="hello word!"></a>hello word!</h1>]]></content>
      
        
    </entry>
    
  
  
</search>
