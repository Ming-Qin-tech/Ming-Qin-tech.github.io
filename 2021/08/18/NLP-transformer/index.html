<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="DataWhale开源学习资料:https:&#x2F;&#x2F;github.com&#x2F;datawhalechina&#x2F;learn-nlp-with-transformers图解Attention  Seq2seq：有2篇开创性的论文：Sutskever等2014年发表的Sequence to Sequence Learningwith Neural Networks和Cho等2014年发表的Learning Ph">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP-transformer">
<meta property="og:url" content="http://example.com/2021/08/18/NLP-transformer/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="DataWhale开源学习资料:https:&#x2F;&#x2F;github.com&#x2F;datawhalechina&#x2F;learn-nlp-with-transformers图解Attention  Seq2seq：有2篇开创性的论文：Sutskever等2014年发表的Sequence to Sequence Learningwith Neural Networks和Cho等2014年发表的Learning Ph">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-encoder.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-decoder.webp">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-x-encoder.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-multi-encoder.webp">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-qkv.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-think.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-think2.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-sum.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-qkv-multi.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-attention-output.webp">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-to1.webp">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-put-together.webp">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-it-attention.webp">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-all-att.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-encoder-decoder.gif">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-position.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-position2.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-position3.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-positin4.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-resnet.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-lyn.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-2layer.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-decoder.gif">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-linear.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-6words.webp">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-am.webp">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-loss.webp">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-target.png">
<meta property="og:image" content="http://example.com/2021/08/18/NLP-transformer/2-trained.webp">
<meta property="article:published_time" content="2021-08-18T13:38:34.000Z">
<meta property="article:modified_time" content="2021-08-18T13:55:43.673Z">
<meta property="article:author" content="Ming Qin">
<meta property="article:tag" content="DataWhale-NLP-Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/08/18/NLP-transformer/2-encoder.png">

<link rel="canonical" href="http://example.com/2021/08/18/NLP-transformer/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>NLP-transformer | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/18/NLP-transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ming Qin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP-transformer
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-08-18 21:38:34 / Modified: 21:55:43" itemprop="dateCreated datePublished" datetime="2021-08-18T21:38:34+08:00">2021-08-18</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <ul>
<li>DataWhale开源学习资料:<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers">https://github.com/datawhalechina/learn-nlp-with-transformers</a><h2 id="图解Attention"><a href="#图解Attention" class="headerlink" title="图解Attention"></a>图解Attention</h2></li>
</ul>
<p>Seq2seq：有2篇开创性的论文：<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sutskever等2014年发表的Sequence to Sequence Learning<br>with Neural Networks</a>和<a target="_blank" rel="noopener" href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf">Cho等2014年发表的Learning Phrase Representations using RNN Encoder–Decoder<br>for Statistical Machine Translation</a>都对这些模型进行了解释。</p>
<p>编码器和解码器在Transformer出现之前一般采用的是循环神经网络。关于循环神经网络，建议阅读 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=UNmqTiOnRfg">Luis Serrano写的一篇关于循环神经网络</a>的精彩介绍.</p>
<p>如果你觉得你准备好了学习注意力机制的代码实现，一定要看看基于 TensorFlow 的 神经机器翻译 (seq2seq) <a target="_blank" rel="noopener" href="https://github.com/tensorflow/nmt">指南</a>。</p>
<h2 id="图解transformer"><a href="#图解transformer" class="headerlink" title="图解transformer"></a>图解transformer</h2><p>本文翻译自<a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-transformer">illustrated-transformer</a></p>
<p>添加了一些简单的代码，实现了一个基本的 Self Attention 以及 multi-head attention 的矩阵运算。</p>
<p>2017 年，Google 提出了 Transformer 模型，用 <strong>Self Attention</strong> 的结构，取代了以往 NLP 任务中的 <strong>RNN</strong> 网络结构</p>
<p>用Self Attention机制效果又好，而且还可以并行计算。<br>（这个模型的其中一个优点，就是使得模型训练过程能够并行计算。在 RNN 中，每一个 time step 的计算都依赖于上一个 time step 的输出，这就使得所有的 time step 必须串行化，无法并行计算。）</p>
<p>编码部分是多层的编码器(Encoder)组成（Transformer 的论文中使用了 6 层编码器，这里的层数 6 并不是固定的，你也可以根据实验效果来修改层数）。同理，解码部分也是由多层的解码器(Decoder)组成（论文里也使用了 6 层的解码器）。每层编码器在结构上都是一样的，但不同层编码器的权重参数是不同的。</p>
<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>每层编码器里面，主要由以下两部分组成：</p>
<ul>
<li>Self-Attention Layer</li>
<li>Feed Forward Neural Network（前馈神经网络，缩写为 FFNN）<br><img src="2-encoder.png" alt="encoder"></li>
</ul>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>同理，解码器也具有这两层，<strong>但是这两层中间还插入了一个 Encoder-Decoder Attention 层</strong>，这个层能帮助解码器聚焦于输入句子的相关部分（类似于 seq2seq 模型 中的 Attention）。<br><img src="2-decoder.webp" alt="decoder"></p>
<h3 id="Transformer细节"><a href="#Transformer细节" class="headerlink" title="Transformer细节"></a>Transformer细节</h3><h4 id="Transformer-的输入"><a href="#Transformer-的输入" class="headerlink" title="Transformer 的输入"></a>Transformer 的输入</h4><p>和通常的 NLP 任务一样，我们<strong>首先会使用词嵌入算法（embedding algorithm）</strong>，将每个词转换为一个词向量。实际中向量一般是 256 或者 512 维。</p>
<p>那么整个输入的句子是一个向量列表，其中有 3 个词向量。在实际中，每个句子的长度不一样，我们会取一个适当的值，作为向量列表的长度。<strong>如果一个句子达不到这个长度</strong>，<strong>那么就填充全为 0 的词向量</strong>；<strong>如果句子超出这个长度，则做截断</strong>。句子长度是一个超参数，通常是训练集中的句子的最大长度，你可以尝试不同长度的效果。</p>
<p>编码器（Encoder）接收的输入都是一个向量列表，输出也是大小同样的向量列表。</p>
<p><img src="2-x-encoder.png" alt="输入encoder"><br>图：输入encoder</p>
<p><img src="2-multi-encoder.webp" alt="一层传一层"><br>图：一层传一层</p>
<p>每一个人都经历过类似的无知的狼狈，其实每一个人都经历了自我怀疑之后才能如释重负。<br><strong>别被“Self-Attention”这么高大上的词给唬住了，乍一听好像每个人都应该对这个词熟悉一样</strong>。</p>
<p>当模型处理句子中的每个词时，Self Attention机制使得模型不仅能够关注这个位置的词，而且能够关注句子中其他位置的词，作为辅助线索，进而可以更好地编码当前位置的词。如果你熟悉 RNN，回忆一下：RNN 在处理一个词时，会考虑前面传过来的hidden state，而hidden state就包含了前面的词的信息。而 Transformer 使用Self Attention机制，会把其他单词的理解融入处理当前的单词。</p>
<h3 id="Self-Attention的细节"><a href="#Self-Attention的细节" class="headerlink" title="Self-Attention的细节"></a>Self-Attention的细节</h3><p>计算Query 向量，Key 向量，Value 向量</p>
<p>下面我们先看下如何使用向量来计算 Self Attention，然后再看下如何使用矩阵来实现 Self Attention。（矩阵运算的方式，使得 Self Attention 的计算能够并行化，这也是 Self Attention 最终的实现方式）。</p>
<p>计算 Self Attention 的第 1 步是：对输入编码器的每个词向量，都创建 3 个向量，分别是：Query 向量，Key 向量，Value 向量。这 3 个向量是词向量分别和 3 个矩阵相乘得到的，而这个矩阵是我们要学习的参数。</p>
<p>注意，这 3 个新得到的向量一般比原来的词向量的长度更小。假设这 3 个向量的长度是$d_{key}$，而原始的词向量或者最终输出的向量的长度是 512（<strong>这 3 个向量的长度，和最终输出的向量长度，是有倍数关系的</strong>）。关于 Multi-head Attention，后面会给出实际代码。这里为了简化，假设只有一个 head 的 Self-Attention。</p>
<p><img src="2-qkv.png" alt="Q,K,V">图：Q,K,V</p>
<p>这里可以锻炼一下github的提交，上下角标？？？？？？</p>
<p>上图中，有两个词向量：Thinking 的词向量 x1 和 Machines 的词向量 x2。以 x1 为例，X1 乘以 WQ 得到 q1，q1 就是 X1 对应的 Query 向量。同理，X1 乘以 WK 得到 k1，k1 是 X1 对应的 Key 向量；X1 乘以 WV 得到 v1，v1 是 X1 对应的 Value 向量。</p>
<p><strong>Query 向量，Key 向量，Value 向量是什么含义呢</strong>？</p>
<h4 id="计算-Attention-Score（注意力分数）"><a href="#计算-Attention-Score（注意力分数）" class="headerlink" title="计算 Attention Score（注意力分数）"></a>计算 Attention Score（注意力分数）</h4><p>第 2 步，是计算 Attention Score（注意力分数）。假设我们现在计算第一个词 Thinking 的 Attention Score（注意力分数），<strong>需要根据 Thinking 这个词，对句子中的其他每个词都计算一个分数</strong>。这些分数决定了我们在编码Thinking这个词时，需要对句子中其他位置的每个词放置多少的注意力。</p>
<p>这些分数，是通过计算 “Thinking” 对应的 Query 向量和其他位置的每个词的 Key 向量的点积，而得到的。如果我们计算句子中第一个位置单词的 Attention Score（注意力分数），那么第一个分数就是 q1 和 k1 的内积，第二个分数就是 q1 和 k2 的点积。</p>
<p><img src="2-think.png" alt="Thinking计算"><br>图：Thinking计算</p>
<p>第 3 步就是把每个分数除以 $\sqrt(d_{key})$ （$d_{key}$是 Key 向量的长度）。你也可以除以其他数，除以一个数是为了在反向传播时，求取梯度更加稳定。</p>
<p>第 4 步，接着把这些分数经过一个 Softmax 层，Softmax可以将分数归一化，这样使得分数都是正数并且加起来等于 1。</p>
<p><img src="2-think2.png" alt="Thinking计算"><br>图：Thinking计算</p>
<p>这些分数决定了在编码当前位置（这里的例子是第一个位置）的词时，对所有位置的词分别有多少的注意力。很明显，在上图的例子中，当前位置（这里的例子是第一个位置）的词会有最高的分数，但有时，关注到其他位置上相关的词也很有用。</p>
<p>第 5 步，得到每个位置的分数后，将每个分数分别与每个 Value 向量相乘。这种做法背后的直觉理解就是：对于分数高的位置，相乘后的值就越大，我们把更多的注意力放到了它们身上；对于分数低的位置，相乘后的值就越小，这些位置的词可能是相关性不大的，这样我们就忽略了这些位置的词。</p>
<p>第 6 步是把上一步得到的向量相加，就得到了 Self Attention 层在这个位置（这里的例子是第一个位置）的输出。</p>
<p><img src="2-sum.png" alt="Think计算"><br>图：Think计算</p>
<p>上面这张图，包含了 Self Attention 的全过程，最终得到的当前位置（这里的例子是第一个位置）的向量会输入到前馈神经网络。但这样每次只能计算一个位置的输出向量，在实际的代码实现中，Self Attention 的计算过程是使用矩阵来实现的，这样可以加速计算，一次就得到所有位置的输出向量。下面让我们来看，如何使用矩阵来计算所有位置的输出向量。</p>
<h4 id="使用矩阵计算Self-Attention"><a href="#使用矩阵计算Self-Attention" class="headerlink" title="使用矩阵计算Self-Attention"></a>使用矩阵计算Self-Attention</h4><p>第一步是计算 Query，Key，Value 的矩阵。首先，我们把所有词向量放到一个矩阵 X 中，然后分别和3 个权重矩阵$W^Q, W^K W^V$ 相乘，得到 Q，K，V 矩阵。</p>
<p><img src="2-qkv-multi.png">图：QKV矩阵乘法</p>
<p>?????? WQ\WK矩阵都是怎么来的</p>
<p>矩阵 X 中的每一行，表示句子中的每一个词的词向量，长度是 512。Q，K，V 矩阵中的每一行表示 Query 向量，Key 向量，Value 向量，向量长度是 64。</p>
<p>接着，由于我们使用了矩阵来计算，我们可以把上面的第 2 步到第 6 步压缩为一步，直接得到 Self Attention 的输出。</p>
<p><img src="2-attention-output.webp" alt="输出"><br>图：输出</p>
<h4 id="多头注意力机制（multi-head-attention）"><a href="#多头注意力机制（multi-head-attention）" class="headerlink" title="多头注意力机制（multi-head attention）"></a>多头注意力机制（multi-head attention）</h4><p>Transformer 的论文通过增加多头注意力机制（一组注意力称为一个 attention head），进一步完善了 Self Attention 层。这种机制从如下两个方面增强了 attention 层的能力：</p>
<ul>
<li>它扩展了模型关注不同位置的能力。</li>
<li>多头注意力机制赋予 attention 层多个“子表示空间”。</li>
</ul>
<p>接下来就有点麻烦了，因为前馈神经网络层接收的是 1 个矩阵（其中每行的向量表示一个词），而不是 8 个矩阵。所以我们需要一种方法，把 8 个矩阵整合为一个矩阵。</p>
<p>怎么才能做到呢？我们把矩阵拼接起来，然后和另一个权重矩阵$W^Q$相乘。</p>
<p><img src="2-to1.webp" alt="整合矩阵"><br>图：整合矩阵</p>
<ol>
<li>把 8 个矩阵 {Z0,Z1…,Z7} 拼接起来</li>
<li>把拼接后的矩阵和 WO 权重矩阵相乘</li>
<li>得到最终的矩阵 Z，这个矩阵包含了所有 attention heads（注意力头） 的信息。这个矩阵会输入到 FFNN (Feed Forward Neural Network)层。</li>
</ol>
<p>这就是多头注意力的全部内容。我知道，在上面的讲解中，出现了相当多的矩阵。下面我把所有的内容都放到一张图中，这样你可以总揽全局，在这张图中看到所有的内容。</p>
<p><img src="2-put-together.webp" alt="放在一起"><br>图：放在一起</p>
<p>既然我们已经谈到了多头注意力，现在让我们重新回顾之前的翻译例子，看下当我们编码单词it时，不同的 attention heads （注意力头）关注的是什么部分。</p>
<p><img src="2-it-attention.webp" alt="`it`的attention"><br>图：<code>it</code>的attention</p>
<p>当我们编码单词”it”时，其中一个 attention head （注意力头）最关注的是”the animal”，另外一个 attention head 关注的是”tired”。因此在某种意义上，”it”在模型中的表示，融合了”animal”和”word”的部分表达。</p>
<p>然而，当我们把所有 attention heads（注意力头） 都在图上画出来时，多头注意力又变得难以解释了。</p>
<p><img src="2-all-att.png" alt="所有注意力heads"><br>图：所有注意力heads</p>
<h2 id="代码实现矩阵计算-Attention"><a href="#代码实现矩阵计算-Attention" class="headerlink" title="代码实现矩阵计算 Attention"></a>代码实现矩阵计算 Attention</h2><p>需要注意的是：在前面的讲解中，我们的 K、Q、V 矩阵的序列长度都是一样的。但是在实际中，K、V 矩阵的序列长度是一样的，而 Q 矩阵的序列长度可以不一样。</p>
<p>这种情况发生在：在解码器部分的Encoder-Decoder Attention层中，Q 矩阵是来自解码器下层，而 K、V 矩阵则是来自编码器的输出。</p>
<p><img src="2-encoder-decoder.gif" alt="encoder-decoder动态图"><br>动态图：encoder-decoder动态图</p>
<p>在完成了编码（encoding）阶段之后，我们开始解码（decoding）阶段。解码（decoding ）阶段的每一个时间步都输出一个翻译后的单词（这里的例子是英语翻译）。</p>
<p>输出是：</p>
<ul>
<li>attn_output：形状是 (L,N,E)</li>
<li>attn_output_weights：形状是 (N,L,S)<br>代码示例如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">## nn.MultiheadAttention 输入第0维为length</span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br><span class="line">query &#x3D; torch.rand(12,64,300)</span><br><span class="line"># batch_size 为 64，有 10 个词，每个词的 Key 向量是 300 维</span><br><span class="line">key &#x3D; torch.rand(10,64,300)</span><br><span class="line"># batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维</span><br><span class="line">value&#x3D; torch.rand(10,64,300)</span><br><span class="line"></span><br><span class="line">embed_dim &#x3D; 299</span><br><span class="line">num_heads &#x3D; 1</span><br><span class="line"># 输出是 (attn_output, attn_output_weights)</span><br><span class="line">multihead_attn &#x3D; nn.MultiheadAttention(embed_dim, num_heads)</span><br><span class="line">attn_output &#x3D; multihead_attn(query, key, value)[0]</span><br><span class="line"># output: torch.Size([12, 64, 300])</span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的向量是 300 维</span><br><span class="line">print(attn_output.shape)</span><br></pre></td></tr></table></figure>

<h3 id="手动实现计算-Attention"><a href="#手动实现计算-Attention" class="headerlink" title="手动实现计算 Attention"></a>手动实现计算 Attention</h3><p>在 PyTorch 提供的 MultiheadAttention  中，第 1 维是句子长度，第 2 维是 batch size。这里我们的代码实现中，第 1 维是 batch size，第 2 维是句子长度。代码里也包括：如何用矩阵实现多组注意力的并行计算。代码中已经有详细注释和说明。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">class MultiheadAttention(nn.Module):</span><br><span class="line">    # n_heads：多头注意力的数量</span><br><span class="line">    # hid_dim：每个词输出的向量维度</span><br><span class="line">    def __init__(self, hid_dim, n_heads, dropout):</span><br><span class="line">        super(MultiheadAttention, self).__init__()</span><br><span class="line">        self.hid_dim &#x3D; hid_dim</span><br><span class="line">        self.n_heads &#x3D; n_heads</span><br><span class="line"></span><br><span class="line">        # 强制 hid_dim 必须整除 h</span><br><span class="line">        assert hid_dim % n_heads &#x3D;&#x3D; 0</span><br><span class="line">        # 定义 W_q 矩阵</span><br><span class="line">        self.w_q &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        # 定义 W_k 矩阵</span><br><span class="line">        self.w_k &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        # 定义 W_v 矩阵</span><br><span class="line">        self.w_v &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc &#x3D; nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.do &#x3D; nn.Dropout(dropout)</span><br><span class="line">        # 缩放</span><br><span class="line">        self.scale &#x3D; torch.sqrt(torch.FloatTensor([hid_dim &#x2F;&#x2F; n_heads]))</span><br><span class="line"></span><br><span class="line">    def forward(self, query, key, value, mask&#x3D;None):</span><br><span class="line">        # K: [64,10,300], batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br><span class="line">        # V: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span><br><span class="line">        # Q: [64,12,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span><br><span class="line">        # bsz： batch size</span><br><span class="line">        bsz &#x3D; query.shape[0]</span><br><span class="line">        Q &#x3D; self.w_q(query)</span><br><span class="line">        K &#x3D; self.w_k(key)</span><br><span class="line">        V &#x3D; self.w_v(value)</span><br><span class="line">        # 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵</span><br><span class="line">        # 最后一维就是是用 self.hid_dim &#x2F;&#x2F; self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300&#x2F;6&#x3D;50</span><br><span class="line">        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度</span><br><span class="line">        # K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]</span><br><span class="line">        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算</span><br><span class="line">        Q &#x3D; Q.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        K &#x3D; K.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        V &#x3D; V.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line"></span><br><span class="line">        # 第 1 步：Q 乘以 K的转置，除以scale</span><br><span class="line">        # [64,6,12,50] * [64,6,50,10] &#x3D; [64,6,12,10]</span><br><span class="line">        # attention：[64,6,12,10]</span><br><span class="line">        attention &#x3D; torch.matmul(Q, K.permute(0, 1, 3, 2)) &#x2F; self.scale</span><br><span class="line"></span><br><span class="line">        # 把 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10</span><br><span class="line">        if mask isnotNone:</span><br><span class="line">            attention &#x3D; attention.masked_fill(mask &#x3D;&#x3D; 0, -1e10)</span><br><span class="line"></span><br><span class="line">        # 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。</span><br><span class="line">        # 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax</span><br><span class="line">        # attention: [64,6,12,10]</span><br><span class="line">        attention &#x3D; self.do(torch.softmax(attention, dim&#x3D;-1))</span><br><span class="line"></span><br><span class="line">        # 第三步，attention结果与V相乘，得到多头注意力的结果</span><br><span class="line">        # [64,6,12,10] * [64,6,10,50] &#x3D; [64,6,12,50]</span><br><span class="line">        # x: [64,6,12,50]</span><br><span class="line">        x &#x3D; torch.matmul(attention, V)</span><br><span class="line"></span><br><span class="line">        # 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果</span><br><span class="line">        # x: [64,6,12,50] 转置-&gt; [64,12,6,50]</span><br><span class="line">        x &#x3D; x.permute(0, 2, 1, 3).contiguous()</span><br><span class="line">        # 这里的矩阵转换就是：把多组注意力的结果拼接起来</span><br><span class="line">        # 最终结果就是 [64,12,300]</span><br><span class="line">        # x: [64,12,6,50] -&gt; [64,12,300]</span><br><span class="line">        x &#x3D; x.view(bsz, -1, self.n_heads * (self.hid_dim &#x2F;&#x2F; self.n_heads))</span><br><span class="line">        x &#x3D; self.fc(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br><span class="line">query &#x3D; torch.rand(64, 12, 300)</span><br><span class="line"># batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维</span><br><span class="line">key &#x3D; torch.rand(64, 10, 300)</span><br><span class="line"># batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维</span><br><span class="line">value &#x3D; torch.rand(64, 10, 300)</span><br><span class="line">attention &#x3D; MultiheadAttention(hid_dim&#x3D;300, n_heads&#x3D;6, dropout&#x3D;0.1)</span><br><span class="line">output &#x3D; attention(query, key, value)</span><br><span class="line">## output: torch.Size([64, 12, 300])</span><br><span class="line">print(output.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="关键代码"><a href="#关键代码" class="headerlink" title="关键代码"></a>关键代码</h3><p>其中用矩阵实现多头注意力的关键代码如下所示， K、Q、V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵</span><br><span class="line">        # 最后一维就是是用 self.hid_dim &#x2F;&#x2F; self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300&#x2F;6&#x3D;50</span><br><span class="line">        # 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 个词，50 表示每组注意力的词的向量长度</span><br><span class="line">        # K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br><span class="line">        # Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]</span><br><span class="line">        # 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算</span><br><span class="line">        Q &#x3D; Q.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        K &#x3D; K.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">        V &#x3D; V.view(bsz, -1, self.n_heads, self.hid_dim &#x2F;&#x2F;</span><br><span class="line">                   self.n_heads).permute(0, 2, 1, 3)</span><br><span class="line">经过 attention 计算得到 x 的形状是 &#96;[64,12,6,50]&#96;，64 表示 batch size，6 表示有 6组注意力，10 表示有 10 个词，50 表示每组注意力的词的向量长度。把这个矩阵转换为 &#96;[64,12,300]&#96;的矩阵，就是相当于把多组注意力的结果拼接起来。</span><br><span class="line">e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e ee</span><br><span class="line"></span><br><span class="line">这里的矩阵转换就是：把多组注意力的结果拼接起来，最终结果就是 [64,12,300]，x: [64,12,6,50] -&gt; [64,12,300]</span><br><span class="line">x &#x3D; x.view(bsz, -1, self.n_heads * (self.hid_dim &#x2F;&#x2F; self.n_heads))</span><br></pre></td></tr></table></figure>

<h2 id="使用位置编码来表示序列的顺序"><a href="#使用位置编码来表示序列的顺序" class="headerlink" title="使用位置编码来表示序列的顺序"></a>使用位置编码来表示序列的顺序</h2><p>到目前为止，我们阐述的模型中缺失了一个东西，那就是表示序列中单词顺序的方法。</p>
<p>为了解决这个问题，Transformer 模型对每个输入的向量都添加了一个向量。这些向量遵循模型学习到的特定模式，有助于确定每个单词的位置，或者句子中不同单词之间的距离。这种做法背后的直觉是：将这些表示位置的向量添加到词向量中，得到了新的向量，这些新向量映射到 Q/K/V，然后计算点积得到 attention 时，可以提供有意义的信息。</p>
<p><img src="2-position.png" alt="位置编码"><br>图：位置编码</p>
<p>为了让模型了解单词的顺序，我们添加了带有位置编码的向量–这些向量的值遵循特定的模式。 如果我们假设词向量的维度是 4，那么带有位置编码的向量可能如下所示：</p>
<p><img src="2-position2.png" alt="位置编码"><br>图：位置编码</p>
<p>位置编码的可视化分析：</p>
<p>在下图中，每一行表示一个带有位置编码的向量。所以，第一行对应于序列中第一个单词的位置编码向量。每一行都包含 512 个值，每个值的范围在 -1 和 1 之间。我对这些向量进行了涂色可视化，你可以从中看到向量遵循的模式。</p>
<p><img src="2-position3.png" alt="位置编码图示"><br>图：位置编码图示</p>
<p>这是一个真实的例子，包含了 20 个词，每个词向量的维度是 512。你可以看到，它看起来像从中间一分为二。这是因为左半部分的值是由<strong>sine</strong>函数产生的，而右半部分的值是由<strong>cosine</strong>函数产生的，然后将他们拼接起来，得到每个位置编码向量。</p>
<p>你可以在get_timing_signal_1d()上查看生成位置编码的代码。这种方法来自于<code>Tranformer2Transformer</code> 的实现。</p>
<p>而论文中的方法和上面图中的稍有不同，它不是直接拼接两个向量，而是将两个向量交织在一起。如下图所示。</p>
<p><img src="2-positin4.png" alt="位置编码交织"><br>图：位置编码交织</p>
<p>此为生成位置编码的公式，在 Transformer 论文的 3.5 节中有详细说明。</p>
<p>这不是唯一一种生成位置编码的方法。但这种方法的优点是：可以扩展到未知的序列长度。例如：当我们的模型需要翻译一个句子，而这个句子的长度大于训练集中所有句子的长度，这时，这种位置编码的方法也可以生成一样长的位置编码向量。</p>
<h3 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h3><p>在我们继续讲解之前，编码器结构中有一个需要注意的细节是：编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization）。</p>
<p><img src="2-resnet.png" alt="残差连接"><br>图：残差连接</p>
<p>将 Self-Attention 层的层标准化（layer-normalization）和向量都进行可视化，如下所示：</p>
<p><img src="2-lyn.png" alt="标准化"><br>图：标准化</p>
<p>在解码器的子层里面也有层标准化（layer-normalization）。假设一个 Transformer 是由 2 层编码器和两层解码器组成的，如下图所示。</p>
<p><img src="2-2layer.png" alt="2层示意图"><br>图：2层示意图</p>
<h3 id="Decoder（解码器）"><a href="#Decoder（解码器）" class="headerlink" title="Decoder（解码器）"></a>Decoder（解码器）</h3><p>现在我们已经介绍了解码器中的大部分概念，我们也基本知道了解码器的原理。现在让我们来看下， 编码器和解码器是如何协同工作的。</p>
<p>上面说了，编码器一般有多层，第一个编码器的输入是一个序列，最后一个编码器输出是一组注意力向量 K 和 V。这些注意力向量将会输入到每个解码器的Encoder-Decoder Attention层，这有助于解码器把注意力集中中输入序列的合适位置。</p>
<p>在完成了编码（encoding）阶段之后，我们开始解码（decoding）阶段。解码（decoding ）阶段的每一个时间步都输出一个翻译后的单词（这里的例子是英语翻译）。</p>
<p>接下来会重复这个过程，直到输出一个结束符，Transformer 就完成了所有的输出。每一步的输出都会在下一个时间步输入到下面的第一个解码器。Decoder 就像 Encoder 那样，从下往上一层一层地输出结果。正对如编码器的输入所做的处理，我们把解码器的输入向量，也加上位置编码向量，来指示每个词的位置。</p>
<p><img src="2-decoder.gif" alt="decoder动态图"><br>动态图：decoder动态图</p>
<p>解码器中的 Self Attention 层，和编码器中的 Self Attention 层不太一样：<strong>在解码器里，Self Attention 层只允许关注到输出序列中早于当前位置之前的单词</strong>。具体做法是：在 Self Attention 分数经过 Softmax 层之前，屏蔽当前位置之后的那些位置。</p>
<p>Encoder-Decoder Attention层的原理和多头注意力（multiheaded Self Attention）机制类似，<strong>不同之处</strong>是：Encoder-Decoder Attention层是使用前一层的输出来构造 Query 矩阵，而 Key 矩阵和 Value 矩阵来自于解码器最终的输出。</p>
<h3 id="最后的线性层和-Softmax-层"><a href="#最后的线性层和-Softmax-层" class="headerlink" title="最后的线性层和 Softmax 层"></a>最后的线性层和 Softmax 层</h3><p>Decoder 最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是由 Softmax 层后面的线性层来完成的。</p>
<p><strong>线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更长的向量，这个向量称为 logits 向量</strong>。</p>
<p><strong>Softmax 层会把这些分数转换为概率（把所有的分数转换为正数，并且加起来等于 1）。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词</strong>。</p>
<p><img src="2-linear.png" alt="线性层"><br>图：线性层</p>
<p>在上图中，最下面的向量，就是编码器的输出，这个向量输入到线性层和 Softmax 层，最终得到输出的词。</p>
<h3 id="Transformer-的训练过程"><a href="#Transformer-的训练过程" class="headerlink" title="Transformer 的训练过程"></a>Transformer 的训练过程</h3><p>现在我们已经了解了 Transformer 的前向传播过程，下面讲讲 Transformer 的训练过程，这也是非常有用的知识。</p>
<p>在训练过程中，模型会经过上面讲的所有前向传播的步骤。但是，当我们在一个标注好的数据集上训练这个模型的时候，我们可以对比模型的输出和真实的标签。</p>
<p>为了可视化这个对比，让我们假设输出词汇表只包含 6 个单词（“a”, “am”, “i”, “thanks”, “student”, and “<eos>”（“<eos>”表示句子末尾））。</p>
<p><img src="2-6words.webp" alt="6个词"><br>图：6个词</p>
<p>我们模型的输出词汇表，是在训练之前的数据预处理阶段构造的。当我们确定了输出词汇表，我们可以用向量来表示词汇表中的每个单词。这个表示方法也称为  one-hot encoding。例如，我们可以把单词 “am” 用下面的向量来表示：</p>
<p><img src="2-am.webp" alt="am向量"><br>图：am向量</p>
<p>介绍了训练过程，我们接着讨论模型的损失函数，这我们在训练时需要优化的目标，通过优化这个目标来得到一个训练好的、非常精确的模型。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>用一个简单的例子来说明训练过程，比如：把“merci”翻译为“thanks”。</p>
<p>这意味着我们希望模型最终输出的概率分布，会指向单词 ”thanks“（在“thanks”这个词的概率最高）。但模型还没训练好，它输出的概率分布可能和我们希望的概率分布相差甚远。</p>
<p><img src="2-loss.webp" alt="概率分布"><br>图：概率分布</p>
<p>由于模型的参数都是随机初始化的。模型在每个词输出的概率都是随机的。我们可以把这个概率和正确的输出概率做对比，然后使用反向传播来调整模型的权重，使得输出的概率分布更加接近正确(pq)输出。</p>
<p>那我们要怎么比较两个概率分布呢？我们可以简单地用一个概率分布减去另一个概率分布。关于更多细节，你可以查看<strong>交叉熵</strong>(cross-entropy)]和<strong>KL 散度</strong>(Kullback–Leibler divergence)的相关概念。</p>
<p>但上面的例子是经过简化的，因为我们的句子只有一个单词。在实际中，我们使用的句子不只有一个单词。例如–输入是：“je suis étudiant” ，输出是：“i am a student”。这意味着，我们的模型需要输出多个概率分布，满足如下条件：</p>
<ul>
<li>每个概率分布都是一个向量，长度是 vocab_size（我们的例子中，向量长度是 6，但实际中更可能是 30000 或者 50000）</li>
<li>第一个概率分布中，最高概率对应的单词是 “i”</li>
<li>第二个概率分布中，最高概率对应的单词是 “am”</li>
<li>以此类推，直到第 5 个概率分布中，最高概率对应的单词是 “&lt;eos&gt;(pq)”，表示没有下一个单词了</li>
</ul>
<p><img src="2-target.png" alt="概率分布"><br>图：概率分布</p>
<p>我们用例子中的句子训练模型，希望产生图中所示的概率分布<br>我们的模型在一个足够大的数据集上，经过足够长时间的训练后，希望输出的概率分布如下图所示：</p>
<p><img src="2-trained.webp" alt="训练后概率分布"><br>图：训练后概率分布</p>
<p>希望经过训练，模型会输出我们希望的正确翻译。当然，如果你要翻译的句子是训练集中的一部分，那输出的结果并不能说明什么。我们希望的是模型在没见过的句子上也能够准确翻译。需要注意的是：概率分布向量中，每个位置都会有一点概率，即使这个位置不是输出对应的单词–这是 Softmax 中一个很有用的特性，有助于帮助训练过程。</p>
<p>现在，由于模型每个时间步只产生一个输出，我们可以认为：模型是从概率分布中选择概率最大的词，并且丢弃其他词。这种方法叫做<strong>贪婪解码</strong>（greedy decoding）。</p>
<p>另一种方法是每个时间步保留两个最高概率的输出词，然后在下一个时间步，重复执行这个过程：假设第一个位置概率最高的两个输出的词是”I“和”a“，这两个词都保留，然后根据第一个词计算第二个位置的词的概率分布，再取出 2 个概率最高的词，对于第二个位置和第三个位置，我们也重复这个过程。这种方法称为<strong>集束搜索</strong>(beam search)，在我们的例子中，beam_size 的值是 2（含义是：在所有时间步，我们保留两个最高概率），top_beams 的值也是 2（表示我们最终会返回两个翻译的结果）。beam_size 和 top_beams 都是你可以在实验中尝试的超参数。</p>
<h3 id="更进一步理解"><a href="#更进一步理解" class="headerlink" title="更进一步理解"></a>更进一步理解</h3><p>我希望上面讲的内容，可以帮助你理解 Transformer 中的主要概念。如果你想更深一步地理解，我建议你可以参考下面这些：</p>
<ul>
<li>阅读 Transformer 的论文：<br>《Attention Is All You Need》<br>链接地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
<li>阅读Transformer 的博客文章：<br>《Transformer: A Novel Neural Network Architecture for Language Understanding》<br>链接地址：<a target="_blank" rel="noopener" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a><br>阅读《Tensor2Tensor announcement》</li>
<li>链接地址：<a target="_blank" rel="noopener" href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html">https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html</a></li>
<li>观看视频 【Łukasz Kaiser’s talk】来理解模型和其中的细节<br>链接地址：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=rBCqOTEfxvg">https://www.youtube.com/watch?v=rBCqOTEfxvg</a><br>运行这份代码：【Jupyter Notebook provided as part of the Tensor2Tensor repo】</li>
<li>链接地址：<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb%E3%80%82">https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb。</a></li>
<li>查看这个项目：【Tensor2Tensor repo】<br>链接地址：<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a></li>
</ul>
<h3 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h3><p>主要由哈尔滨工业大学张贤同学翻译撰写，由本项目同学组织和整理。最后，期待您的阅读反馈和star哦，谢谢。</p>
<h3 id="转载并编辑"><a href="#转载并编辑" class="headerlink" title="转载并编辑"></a>转载并编辑</h3><p>童茗 整理笔记</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/DataWhale-NLP-Transformer/" rel="tag"># DataWhale-NLP-Transformer</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/08/18/Task02%20%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="prev" title="">
      <i class="fa fa-chevron-left"></i> 
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E8%A7%A3Attention"><span class="nav-number">1.</span> <span class="nav-text">图解Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E8%A7%A3transformer"><span class="nav-number">2.</span> <span class="nav-text">图解transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">2.1.</span> <span class="nav-text">编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-number">2.2.</span> <span class="nav-text">解码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformer%E7%BB%86%E8%8A%82"><span class="nav-number">2.3.</span> <span class="nav-text">Transformer细节</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Transformer-%E7%9A%84%E8%BE%93%E5%85%A5"><span class="nav-number">2.3.1.</span> <span class="nav-text">Transformer 的输入</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-Attention%E7%9A%84%E7%BB%86%E8%8A%82"><span class="nav-number">2.4.</span> <span class="nav-text">Self-Attention的细节</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97-Attention-Score%EF%BC%88%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0%EF%BC%89"><span class="nav-number">2.4.1.</span> <span class="nav-text">计算 Attention Score（注意力分数）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97Self-Attention"><span class="nav-number">2.4.2.</span> <span class="nav-text">使用矩阵计算Self-Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88multi-head-attention%EF%BC%89"><span class="nav-number">2.4.3.</span> <span class="nav-text">多头注意力机制（multi-head attention）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97-Attention"><span class="nav-number">3.</span> <span class="nav-text">代码实现矩阵计算 Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0%E8%AE%A1%E7%AE%97-Attention"><span class="nav-number">3.1.</span> <span class="nav-text">手动实现计算 Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E4%BB%A3%E7%A0%81"><span class="nav-number">3.2.</span> <span class="nav-text">关键代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E6%9D%A5%E8%A1%A8%E7%A4%BA%E5%BA%8F%E5%88%97%E7%9A%84%E9%A1%BA%E5%BA%8F"><span class="nav-number">4.</span> <span class="nav-text">使用位置编码来表示序列的顺序</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5"><span class="nav-number">4.1.</span> <span class="nav-text">残差连接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder%EF%BC%88%E8%A7%A3%E7%A0%81%E5%99%A8%EF%BC%89"><span class="nav-number">4.2.</span> <span class="nav-text">Decoder（解码器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%90%8E%E7%9A%84%E7%BA%BF%E6%80%A7%E5%B1%82%E5%92%8C-Softmax-%E5%B1%82"><span class="nav-number">4.3.</span> <span class="nav-text">最后的线性层和 Softmax 层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformer-%E7%9A%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">4.4.</span> <span class="nav-text">Transformer 的训练过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">4.5.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%90%86%E8%A7%A3"><span class="nav-number">4.6.</span> <span class="nav-text">更进一步理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%B4%E8%B0%A2"><span class="nav-number">4.7.</span> <span class="nav-text">致谢</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%AC%E8%BD%BD%E5%B9%B6%E7%BC%96%E8%BE%91"><span class="nav-number">4.8.</span> <span class="nav-text">转载并编辑</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ming Qin</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ming Qin</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
